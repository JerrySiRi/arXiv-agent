{"id": "2510.14235", "categories": ["cs.NE", "C.1.3; I.2.6; I.2.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.14235", "abs": "https://arxiv.org/abs/2510.14235", "authors": ["Kama Svoboda", "Tosiron Adegbija"], "title": "Spiking Neural Network Architecture Search: A Survey", "comment": "16 pages, 9 figures, submitted to IEEE Computational Intelligence\n  Magazine", "summary": "This survey paper presents a comprehensive examination of Spiking Neural\nNetwork (SNN) architecture search (SNNaS) from a unique hardware/software\nco-design perspective. SNNs, inspired by biological neurons, have emerged as a\npromising approach to neuromorphic computing. They offer significant advantages\nin terms of power efficiency and real-time resource-constrained processing,\nmaking them ideal for edge computing and IoT applications. However, designing\noptimal SNN architectures poses significant challenges, due to their inherent\ncomplexity (e.g., with respect to training) and the interplay between hardware\nconstraints and SNN models. We begin by providing an overview of SNNs,\nemphasizing their operational principles and key distinctions from traditional\nartificial neural networks (ANNs). We then provide a brief overview of the\nstate of the art in NAS for ANNs, highlighting the challenges of directly\napplying these approaches to SNNs. We then survey the state-of-the-art in\nSNN-specific NAS approaches. Finally, we conclude with insights into future\nresearch directions for SNN research, emphasizing the potential of\nhardware/software co-design in unlocking the full capabilities of SNNs. This\nsurvey aims to serve as a valuable resource for researchers and practitioners\nin the field, offering a holistic view of SNNaS and underscoring the importance\nof a co-design approach to harness the true potential of neuromorphic\ncomputing."}
{"id": "2510.14361", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2510.14361", "abs": "https://arxiv.org/abs/2510.14361", "authors": ["Pawel Pawlowski"], "title": "T-BAT semantics and its logics", "comment": null, "summary": "\\textbf{T-BAT} logic is a formal system designed to express the notion of\ninformal provability. This type of provability is closely related to\nmathematical practice and is quite often contrasted with formal provability,\nunderstood as a formal derivation in an appropriate formal system.\n\\textbf{T-BAT} is a non-deterministic four-valued logic. The logical values in\n\\textbf{T-BAT} semantics convey not only the information whether a given\nformula is true but also about its provability status.\n  The primary aim of our paper is to study the proposed four-valued\nnon-deterministic semantics. We look into the intricacies of the interactions\nbetween various weakenings and strengthenings of the semantics with axioms that\nthey induce. We prove the completeness of all the logics that are definable in\nthis semantics by transforming truth values into specific expressions\nformulated within the object language of the semantics. Additionally, we\nutilize Kripke semantics to examine these axioms from a modal perspective by\nproviding a frame condition that they induce. The secondary aim of this paper\nis to provide an intuitive axiomatization of \\textbf{T-BAT} logic."}
{"id": "2510.13903", "categories": ["cs.MA", "cs.AI", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.13903", "abs": "https://arxiv.org/abs/2510.13903", "authors": ["Michael Rizvi-Martel", "Satwik Bhattamishra", "Neil Rathi", "Guillaume Rabusseau", "Michael Hahn"], "title": "Benefits and Limitations of Communication in Multi-Agent Reasoning", "comment": "34 pages, 14 figures", "summary": "Chain-of-thought prompting has popularized step-by-step reasoning in large\nlanguage models, yet model performance still degrades as problem complexity and\ncontext length grow. By decomposing difficult tasks with long contexts into\nshorter, manageable ones, recent multi-agent paradigms offer a promising\nnear-term solution to this problem. However, the fundamental capacities of such\nsystems are poorly understood. In this work, we propose a theoretical framework\nto analyze the expressivity of multi-agent systems. We apply our framework to\nthree algorithmic families: state tracking, recall, and $k$-hop reasoning. We\nderive bounds on (i) the number of agents required to solve the task exactly,\n(ii) the quantity and structure of inter-agent communication, and (iii) the\nachievable speedups as problem size and context scale. Our results identify\nregimes where communication is provably beneficial, delineate tradeoffs between\nagent count and bandwidth, and expose intrinsic limitations when either\nresource is constrained. We complement our theoretical analysis with a set of\nexperiments on pretrained LLMs using controlled synthetic benchmarks. Empirical\noutcomes confirm the tradeoffs between key quantities predicted by our theory.\nCollectively, our analysis offers principled guidance for designing scalable\nmulti-agent reasoning systems."}
{"id": "2510.14550", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2510.14550", "abs": "https://arxiv.org/abs/2510.14550", "authors": ["S Hitarth", "Alessio Mansutti", "Guruprerana Shabadi"], "title": "Optimization Modulo Integer Linear-Exponential Programs", "comment": "Extended version of a SODA 2026 paper", "summary": "This paper presents the first study of the complexity of the optimization\nproblem for integer linear-exponential programs which extend classical integer\nlinear programs with the exponential function $x \\mapsto 2^x$ and the remainder\nfunction ${(x,y) \\mapsto (x \\bmod 2^y)}$. The problem of deciding if such a\nprogram has a solution was recently shown to be NP-complete in [Chistikov et\nal., ICALP'24]. The optimization problem instead asks for a solution that\nmaximizes (or minimizes) a linear-exponential objective function, subject to\nthe constraints of an integer linear-exponential program. We establish the\nfollowing results:\n  1. If an optimal solution exists, then one of them can be succinctly\nrepresented as an integer linear-exponential straight-line program (ILESLP): an\narithmetic circuit whose gates always output an integer value (by construction)\nand implement the operations of addition, exponentiation, and multiplication by\nrational numbers.\n  2. There is an algorithm that runs in polynomial time, given access to an\ninteger factoring oracle, which determines whether an ILESLP encodes a solution\nto an integer linear-exponential program. This algorithm can also be used to\ncompare the values taken by the objective function on two given solutions.\n  Building on these results, we place the optimization problem for integer\nlinear-exponential programs within an extension of the optimization class\n$\\text{NPO}$ that lies within $\\text{FNP}^{\\text{NP}}$. In essence, this\nextension forgoes determining the optimal solution via binary search."}
{"id": "2510.13982", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13982", "abs": "https://arxiv.org/abs/2510.13982", "authors": ["Jinkun Chen", "Sher Badshah", "Xuemin Yu", "Sijia Han", "Jiechao Gao"], "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations", "comment": null, "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. \\textbf{We call on the community to move beyond static paradigms\nand help shape the next generation of adaptive, socially-aware multi-agent\nsimulations.}"}
{"id": "2510.14619", "categories": ["cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2510.14619", "abs": "https://arxiv.org/abs/2510.14619", "authors": ["Sara Ayhan"], "title": "Problems and Consequences of Bilateral Notions of (Meta-)Derivability", "comment": null, "summary": "A bilateralist take on proof-theoretic semantics can be understood as\ndemanding of a proof system to display not only rules giving the connectives'\nprovability conditions but also their refutability conditions. On such a view,\nthen, a system with two derivability relations is obtained, which can be quite\nnaturally expressed in a proof system of natural deduction but which faces\nobstacles in a sequent calculus representation. Since in a sequent calculus\nthere are two derivability relations inherent, one expressed by the sequent\nsign and one by the horizontal lines holding between sequents, in a truly\nbilateral calculus both need to be dualized. While dualizing the sequent sign\nis rather straightforwardly corresponding to dualizing the horizontal lines in\nnatural deduction, dualizing the horizontal lines in sequent calculus, uncovers\nproblems that, as will be argued in this paper, shed light on deeper conceptual\nissues concerning an imbalance between the notions of proof vs. refutation. The\nroots of this problem will be further analyzed and possible solutions on how to\nretain a bilaterally desired balance in our system are presented."}
{"id": "2510.14008", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14008", "abs": "https://arxiv.org/abs/2510.14008", "authors": ["Jinwei Hu", "Yi Dong", "Shuang Ao", "Zhuoyun Li", "Boxuan Wang", "Lokesh Singh", "Guangliang Cheng", "Sarvapali D. Ramchurn", "Xiaowei Huang"], "title": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local Alignment", "comment": "Under Review", "summary": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in\ndistributed reasoning, collaboration, and task generalization but also\nintroduce additional risks due to unguaranteed agreement, cascading\nuncertainty, and adversarial vulnerabilities. We argue that ensuring\nresponsible behavior in such systems requires a paradigm shift: from local,\nsuperficial agent-level alignment to global, systemic agreement. We\nconceptualize responsibility not as a static constraint but as a lifecycle-wide\nproperty encompassing agreement, uncertainty, and security, each requiring the\ncomplementary integration of subjective human-centered values and objective\nverifiability. Furthermore, a dual-perspective governance framework that\ncombines interdisciplinary design with human-AI collaborative oversight is\nessential for tracing and ensuring responsibility throughout the lifecycle of\nLLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as\nunified, dynamic socio-technical systems that demand principled mechanisms to\nsupport each dimension of responsibility and enable ethically aligned,\nverifiably coherent, and resilient behavior for sustained, system-wide\nagreement."}
{"id": "2510.14749", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2510.14749", "abs": "https://arxiv.org/abs/2510.14749", "authors": ["Kenji Saotome", "Koji Nakazawa"], "title": "Admissibility of Substitution Rule in Cyclic-Proof Systems", "comment": "20 pages, 4 figures(Including the derivation trees inserted within\n  the main text, there are 8 JPEG files)", "summary": "This paper investigates the admissibility of the substitution rule in\ncyclic-proof systems. The substitution rule complicates theoretical case\nanalysis and increases computational cost in proof search since every sequent\ncan be a conclusion of an instance of the substitution rule; hence,\nadmissibility is desirable on both fronts. While admissibility is often shown\nby local proof transformations in non-cyclic systems, such transformations may\ndisrupt cyclic structure and do not readily apply. Prior remarks suggested that\nthe substitution rule is likely nonadmissible in the cyclic-proof system\nCLKID^omega for first-order logic with inductive predicates. In this paper, we\nprove admissibility in CLKID^omega, assuming the presence of the cut rule. Our\napproach unfolds a cyclic proof into an infinitary form, lifts the substitution\nrules, and places back edges to construct a cyclic proof without the\nsubstitution rule. If we restrict substitutions to exclude function symbols,\nthe result extends to a broader class of systems, including cut-free\nCLKID^omega and cyclic-proof systems for the separation logic."}
{"id": "2510.14401", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14401", "abs": "https://arxiv.org/abs/2510.14401", "authors": ["Prateek Gupta", "Qiankun Zhong", "Hiromu Yakura", "Thomas Eisenmann", "Iyad Rahwan"], "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems", "comment": null, "summary": "A growing body of multi-agent studies with Large Language Models (LLMs)\nexplores how norms and cooperation emerge in mixed-motive scenarios, where\npursuing individual gain can undermine the collective good. While prior work\nhas explored these dynamics in both richly contextualized simulations and\nsimplified game-theoretic environments, most LLM systems featuring common-pool\nresource (CPR) games provide agents with explicit reward functions directly\ntied to their actions. In contrast, human cooperation often emerges without\nfull visibility into payoffs and population, relying instead on heuristics,\ncommunication, and punishment. We introduce a CPR simulation framework that\nremoves explicit reward signals and embeds cultural-evolutionary mechanisms:\nsocial learning (adopting strategies and beliefs from successful peers) and\nnorm-based punishment, grounded in Ostrom's principles of resource governance.\nAgents also individually learn from the consequences of harvesting, monitoring,\nand punishing via environmental feedback, enabling norms to emerge\nendogenously. We establish the validity of our simulation by reproducing key\nfindings from existing studies on human behavior. Building on this, we examine\nnorm evolution across a $2\\times2$ grid of environmental and social\ninitialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and\nbenchmark how agentic societies comprised of different LLMs perform under these\nconditions. Our results reveal systematic model differences in sustaining\ncooperation and norm formation, positioning the framework as a rigorous testbed\nfor studying emergent norms in mixed-motive LLM societies. Such analysis can\ninform the design of AI systems deployed in social and organizational contexts,\nwhere alignment with cooperative norms is critical for stability, fairness, and\neffective governance of AI-mediated environments."}
{"id": "2510.13903", "categories": ["cs.MA", "cs.AI", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.13903", "abs": "https://arxiv.org/abs/2510.13903", "authors": ["Michael Rizvi-Martel", "Satwik Bhattamishra", "Neil Rathi", "Guillaume Rabusseau", "Michael Hahn"], "title": "Benefits and Limitations of Communication in Multi-Agent Reasoning", "comment": "34 pages, 14 figures", "summary": "Chain-of-thought prompting has popularized step-by-step reasoning in large\nlanguage models, yet model performance still degrades as problem complexity and\ncontext length grow. By decomposing difficult tasks with long contexts into\nshorter, manageable ones, recent multi-agent paradigms offer a promising\nnear-term solution to this problem. However, the fundamental capacities of such\nsystems are poorly understood. In this work, we propose a theoretical framework\nto analyze the expressivity of multi-agent systems. We apply our framework to\nthree algorithmic families: state tracking, recall, and $k$-hop reasoning. We\nderive bounds on (i) the number of agents required to solve the task exactly,\n(ii) the quantity and structure of inter-agent communication, and (iii) the\nachievable speedups as problem size and context scale. Our results identify\nregimes where communication is provably beneficial, delineate tradeoffs between\nagent count and bandwidth, and expose intrinsic limitations when either\nresource is constrained. We complement our theoretical analysis with a set of\nexperiments on pretrained LLMs using controlled synthetic benchmarks. Empirical\noutcomes confirm the tradeoffs between key quantities predicted by our theory.\nCollectively, our analysis offers principled guidance for designing scalable\nmulti-agent reasoning systems."}
{"id": "2510.13982", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13982", "abs": "https://arxiv.org/abs/2510.13982", "authors": ["Jinkun Chen", "Sher Badshah", "Xuemin Yu", "Sijia Han", "Jiechao Gao"], "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations", "comment": null, "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. \\textbf{We call on the community to move beyond static paradigms\nand help shape the next generation of adaptive, socially-aware multi-agent\nsimulations.}"}
{"id": "2510.14401", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14401", "abs": "https://arxiv.org/abs/2510.14401", "authors": ["Prateek Gupta", "Qiankun Zhong", "Hiromu Yakura", "Thomas Eisenmann", "Iyad Rahwan"], "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems", "comment": null, "summary": "A growing body of multi-agent studies with Large Language Models (LLMs)\nexplores how norms and cooperation emerge in mixed-motive scenarios, where\npursuing individual gain can undermine the collective good. While prior work\nhas explored these dynamics in both richly contextualized simulations and\nsimplified game-theoretic environments, most LLM systems featuring common-pool\nresource (CPR) games provide agents with explicit reward functions directly\ntied to their actions. In contrast, human cooperation often emerges without\nfull visibility into payoffs and population, relying instead on heuristics,\ncommunication, and punishment. We introduce a CPR simulation framework that\nremoves explicit reward signals and embeds cultural-evolutionary mechanisms:\nsocial learning (adopting strategies and beliefs from successful peers) and\nnorm-based punishment, grounded in Ostrom's principles of resource governance.\nAgents also individually learn from the consequences of harvesting, monitoring,\nand punishing via environmental feedback, enabling norms to emerge\nendogenously. We establish the validity of our simulation by reproducing key\nfindings from existing studies on human behavior. Building on this, we examine\nnorm evolution across a $2\\times2$ grid of environmental and social\ninitialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and\nbenchmark how agentic societies comprised of different LLMs perform under these\nconditions. Our results reveal systematic model differences in sustaining\ncooperation and norm formation, positioning the framework as a rigorous testbed\nfor studying emergent norms in mixed-motive LLM societies. Such analysis can\ninform the design of AI systems deployed in social and organizational contexts,\nwhere alignment with cooperative norms is critical for stability, fairness, and\neffective governance of AI-mediated environments."}
