<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同规模的视觉语言模型（500M和2.2B参数）在盲人和低视力用户可访问性描述质量上的表现，引入了专门的可访问性评估框架，并在智能手机上测试了不同精度变体的实际性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成高质量视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对依赖详细、上下文感知描述的盲人和低视力用户群体。

Method: 评估了SmolVLM2的500M和2.2B参数变体在AVCaps（户外）和Charades（室内）数据集上的表现。引入了多上下文BLV框架和导航辅助框架两个专门的可访问性评估框架，并系统评估了四种提示设计策略，在智能手机上测试了FP32和INT8精度变体。

Result: 通过专门设计的可访问性评估框架，系统比较了不同规模模型在空间定向、社交互动、动作事件、环境氛围等关键可访问性维度的表现，并评估了在资源受限移动设备上的实际部署性能。

Conclusion: 研究为开发适合盲人和低视力用户实际需求的轻量级视觉语言模型提供了重要指导，强调了模型规模与可访问性描述质量之间的平衡，以及移动设备部署的可行性。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自一致性采样（SCS）方法，解决多模态大语言模型在强化学习中正确猜测但错误推理轨迹获得相同奖励的问题，通过在视觉扰动和轨迹重采样中计算一致性得分来优化策略更新。


<details>
  <summary>Details</summary>
Motivation: 多模态推理基准测试中，强化学习面临一个重要但常被忽视的问题：即使推理链错误但最终猜对选项的轨迹，与真正正确推理的轨迹获得相同奖励，这影响了学习效果。

Method: SCS方法对每个问题引入小视觉扰动，重复截断和重采样初始轨迹，通过轨迹间一致性计算可微一致性得分，在策略更新时降低不可靠轨迹的权重。

Result: 在Qwen2.5-VL-7B-Instruct模型上，SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率提升高达7.7个百分点，计算开销可忽略。

Conclusion: SCS为多模态大语言模型的结果奖励强化学习提供了一个简单通用的解决方案，在多个模型上都取得了显著提升。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文提出了一种编码器增强的因果解码器模型架构，该架构在训练效率上优于因果变换器，并在有限硬件条件下实现更高压缩率。研究展示了如何基于每词元估计熵，并证明接近训练数据熵的模型比过度优化的模型具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于因果（下一个词元预测）的大型语言模型是最有效的语言压缩算法，但使用这些模型准确估计语言熵在计算上不可行。需要开发更高效的架构来估计语言熵并改善模型泛化能力。

Method: 引入编码器增强的因果解码器模型架构，通过改进训练效率实现更高压缩率。开发基于每词元的熵估计方法，证明模型训练接近但不超出估计熵值时能获得更好泛化性能。

Result: 新架构在有限硬件条件下比因果变换器实现更高压缩率。实证表明，接近估计每词元熵的因果模型比不考虑熵的模型具有显著更好的泛化能力。

Conclusion: 语言预测受语言内在信息熵约束，存在准确率上限和压缩下限。通过编码器增强架构和熵感知训练，可以在有限计算资源下实现更好压缩和泛化性能，为高效语言建模提供新方向。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [4] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 提出了Socratic Self-Refine (SSR)框架，通过将模型响应分解为可验证的子问题-子答案对，实现细粒度评估和精确精炼，在多个推理基准测试中优于现有自我精炼方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗略的自我验证和自我校正，限制了在复杂任务上的有效性，需要更精细的评估和精炼方法。

Method: SSR将模型响应分解为可验证的(子问题,子答案)对，通过受控重解和自我一致性检查进行步骤级置信度估计，精确定位不可靠步骤并迭代精炼。

Result: 在五个推理基准测试和三个LLM上的实证结果显示，SSR始终优于最先进的迭代自我精炼基线方法。

Conclusion: SSR不仅提升了性能，还为评估和理解LLM内部推理过程提供了原则性的黑盒方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [5] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开源的30亿参数语言模型家族，在完全开放的数据和代码库上训练，在AMD MI300X GPU上开发。尽管预训练token数量较少，但在完全开源模型中达到最先进水平，并发布支持128K上下文长度的Instella-Long和专注数学推理的Instella-Math两个变体。


<details>
  <summary>Details</summary>
Motivation: 当前高性能语言模型大多为闭源或部分开源，限制了透明度和可复现性，需要开发完全开源且性能优异的替代方案。

Method: 使用AMD Instinct MI300X GPU进行大规模预训练、通用指令微调和人类偏好对齐。开发了三个变体：基础模型、支持128K上下文的Instella-Long，以及通过监督微调和强化学习在数学任务上增强的Instella-Math。

Result: Instella在完全开源模型中达到最先进水平，与同规模领先开源权重模型竞争。Instella-Long支持128K上下文，Instella-Math在数学推理任务上表现优异。

Conclusion: Instella为社区提供了透明、高性能且多功能的开源语言建模替代方案，推动了开放和可复现语言建模研究的发展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [6] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出生成对抗蒸馏（GAD）方法，用于黑盒知识蒸馏，通过将学生LLM作为生成器、训练判别器区分学生与教师模型的响应，实现稳定的在线蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒蒸馏仅从教师模型的文本输出学习，无法访问内部logits或参数，存在训练不稳定和效果有限的问题。

Method: GAD将学生LLM作为生成器，训练判别器区分学生与教师模型的响应，形成极小极大博弈。判别器作为在线奖励模型，与学生共同进化，提供稳定自适应反馈。

Result: 实验表明GAD持续优于常用的序列级知识蒸馏。Qwen2.5-14B-Instruct学生模型在LMSYS-Chat自动评估中达到与教师GPT-5-Chat相当的水平。

Conclusion: GAD为黑盒LLM蒸馏提供了一个有前景且有效的范式，实现了在线黑盒蒸馏的稳定训练。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [7] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: 本文提出了ParoQuant方法，一种权重后训练量化技术，通过成对旋转量化和通道缩放来减少大语言模型中的异常值影响，在推理任务上比AWQ平均提升2.4%准确率，且运行时开销低于10%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重和激活中存在异常值，导致量化误差大和精度严重下降，特别是在推理任务中误差会在长思维链中累积。现有PTQ方法要么无法充分抑制异常值，要么在推理时引入显著开销。

Method: ParoQuant结合硬件高效的独立Givens旋转和通道级缩放，均衡通道间幅度并缩小每个量化组内的动态范围。同时协同设计推理内核以充分利用GPU并行性，保持旋转和缩放操作在运行时轻量级。

Result: 在推理任务上比AWQ平均提升2.4%准确率，运行时开销低于10%。

Conclusion: ParoQuant为大语言模型推理任务提供了更高效和准确的部署方案。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种验证仿真中发现的自动驾驶故障场景在真实世界中可重现性的方法，通过定义场景匹配的正式规范，开发了高效的查询算法来在真实数据集中定位匹配场景。


<details>
  <summary>Details</summary>
Motivation: 解决仿真测试中发现的故障场景是否能在真实世界中重现的问题，克服仿真与现实之间的传感器数据差异导致的sim-to-real差距。

Method: 使用Scenic概率编程语言表示抽象场景，提出正式的时序传感器数据匹配定义，开发查询算法在标记数据集中高效识别匹配指定场景的数据子集。

Result: 实验表明，该算法在查询场景时比最先进的商业视觉大语言模型更准确且快几个数量级，能够随着查询时序数据时长而扩展。

Conclusion: 该方法为验证仿真故障场景在真实世界中的可重现性提供了有效解决方案，显著提升了仿真测试结果的可信度。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
