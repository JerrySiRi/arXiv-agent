<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 21]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict是一个几何感知的视觉-语言-动作框架，通过预测性运动学和几何先验增强连续动作策略，解决了传统VLA模型在需要精确3D推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型虽然在机器人操作中表现出良好的泛化能力，但主要是反应式的且以2D为中心，在需要精确3D推理的任务中可靠性不足。

Method: GeoPredict包含两个预测模块：轨迹级模块编码运动历史并预测机器人手臂的多步3D关键点轨迹；预测性3D高斯几何模块沿未来关键点轨迹预测工作空间几何形状。这些模块仅作为训练时监督，推理时只需轻量级查询令牌。

Result: 在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线模型，特别是在几何密集和空间要求高的场景中。

Conclusion: GeoPredict通过整合预测性几何和运动学先验，显著提升了VLA模型在需要3D推理的机器人操作任务中的性能，为几何感知的机器人学习提供了有效框架。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8变体的车牌识别系统，采用YOLOv8 Nano进行车牌检测（精度0.964，mAP50 0.918），YOLOv8 Small进行字符识别（精度0.92，mAP50 0.91），并引入基于x轴位置的字符排序方法，构建了一个兼顾计算效率和准确率的优化流水线，适用于边缘设备的智能交通系统部署。


<details>
  <summary>Details</summary>
Motivation: 在智能交通系统和车辆监控领域，高效的车牌检测与识别技术至关重要。传统方法在多样化环境中难以保持实时准确性，特别是在边缘设备部署时面临挑战。本研究旨在开发一个既能保持高精度又具有计算效率的车牌识别系统，为智能城市基础设施建设提供技术支撑。

Method: 本研究采用YOLOv8的不同变体构建两阶段车牌识别系统：1）使用YOLOv8 Nano进行车牌检测；2）使用YOLOv8 Small进行字符识别。引入基于x轴位置的字符排序方法，将检测到的字符按水平位置排列。使用两个独立的数据集进行训练和评估，最终构建了一个优化的处理流水线。

Result: YOLOv8 Nano在车牌检测任务上达到0.964的精度和0.918的mAP50；YOLOv8 Small在字符识别任务上达到0.92的精度和0.91的mAP50。提出的字符排序方法能有效按x轴位置排列检测到的字符。优化后的流水线在保持计算效率的同时确保了高准确性。

Conclusion: 本研究成功开发了一个基于YOLOv8变体的高效车牌识别系统，通过YOLOv8 Nano进行车牌检测和YOLOv8 Small进行字符识别的组合方案，在计算效率和准确性之间取得了良好平衡。该系统为智能交通系统在边缘设备上的实际部署提供了坚实基础，是迈向更智能、更高效城市基础设施的重要一步。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [3] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 本文提出了一种紧凑的图像到文本架构，仅使用单个正面胸片图像生成放射报告。通过冻结的DINOv3视觉编码器和GPT-2解码器结合分层解剖注意力机制，在无需额外可训练参数的情况下显著提升了病理检测性能，为资源受限环境提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的放射报告生成系统（如MAIRA-2和MedPaLM-M）依赖大规模多模态训练、临床元数据和多视图图像，资源需求高且难以普及。本文旨在开发一种仅需单个正面胸片图像的紧凑架构，降低计算资源需求，提高在常规临床环境中的可访问性。

Method: 采用冻结的DINOv3视觉Transformer编码器提取图像特征，结合GPT-2解码器生成文本。创新性地引入分层解剖注意力机制，通过肺部和心脏分割掩码结合高斯平滑，引导注意力聚焦于临床相关区域，无需增加可训练参数。

Result: 在MIMIC-CXR数据集上评估显示显著性能提升：5种关键病理的CheXpert Macro-F1提高168%（0.083→0.238），Micro-F1提高146%（0.137→0.337）；14种观察指标的总体性能提升86%（0.170→0.316）；RadGraph F1结构一致性提高9.7%。

Conclusion: 尽管模型规模小且仅依赖图像输入，但通过解码器层面的解剖引导机制显著改善了空间定位能力和临床相关区域的连贯性。该方法为资源受限环境提供了高效的放射报告生成方案，证明了解剖先验知识在提升模型性能中的重要性。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [4] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch是首个野外环境下的第一人称全手触觉数据集，包含5.1小时同步的视频-触觉-姿态数据，以及2900个带详细文本标注的剪辑片段。该数据集填补了视觉感知与物理交互之间的空白，为多模态感知和具身学习提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 人类手部是我们与物理世界交互的主要界面，但第一人称视角的感知很少能准确知道何时、何地以及以多大力度发生接触。目前缺乏鲁棒的穿戴式触觉传感器，也没有野外环境下的第一人称视频与全手触觉对齐的数据集。为了弥合视觉感知与物理交互之间的鸿沟，研究者开发了OpenTouch数据集。

Method: 研究者收集了5.1小时的同步视频-触觉-姿态数据，包含2900个经过精心筛选的剪辑片段，每个片段都有详细的文本标注。基于OpenTouch数据集，他们引入了检索和分类基准测试，探索触觉信号如何为感知和行动提供基础。

Result: 研究表明，触觉信号为抓握理解提供了紧凑而强大的线索，增强了跨模态对齐能力，并且可以从野外视频查询中可靠地检索出来。触觉信息能够有效提升对物理交互的理解和表征能力。

Conclusion: 通过发布这个带标注的视觉-触觉-姿态数据集和基准测试，OpenTouch旨在推动多模态第一人称感知、具身学习以及接触丰富的机器人操作技术的发展。该数据集为研究触觉在物理交互中的作用提供了重要资源。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [5] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文揭示了文本到图像（T2I）模型评估中的基准漂移问题：静态基准评估器无法跟上新模型能力的发展，导致评估结果与人类判断严重偏离。以GenEval为例，其与人类判断的绝对误差高达17.7%，表明该基准已饱和。作者提出新基准GenEval 2和评估方法Soft-TIFA，通过改进视觉基元覆盖和组合性来提高挑战性，并减少未来漂移风险。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型评估面临自动化挑战：评估模型需要正确评分，测试提示需要挑战当前T2I模型但不能挑战评估器。这些约束可能导致基准随时间漂移，静态基准评估器无法跟上新模型能力发展。作者发现GenEval这一流行基准已出现严重漂移问题，需要新的解决方案。

Method: 作者提出新基准GenEval 2，改进视觉基元概念覆盖和组合性程度，使其对当前模型更具挑战性。同时提出Soft-TIFA评估方法，结合视觉基元判断，相比VQAScore等整体评估器，与人类判断更一致且未来漂移风险更低。

Result: 研究表明GenEval基准已出现显著漂移，与人类判断的绝对误差高达17.7%，表明该基准已饱和。通过大规模人类研究验证了这一发现。GenEval 2基准对当前模型更具挑战性，Soft-TIFA评估方法与人类判断更一致，且未来漂移风险更低。

Conclusion: 基准漂移是T2I模型评估中的重要问题，GenEval已严重偏离人类判断。GenEval 2和Soft-TIFA提供了改进方案，但避免基准漂移远非保证。这项工作强调了T2I及相关自动化模型评估基准需要持续审计和改进的重要性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [6] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan提出了一种区域对齐规划框架，通过"规划-执行"范式解决指令-视觉复杂性（IV-Complexity）下的图像编辑问题。核心创新包括：视觉语言规划器进行逐步推理和区域定位，扩散编辑器使用免训练的注意力区域注入机制实现并行多区域编辑，以及基于GRPO强化学习提升规划能力。在IV-Edit基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑模型在处理指令-视觉复杂性（IV-Complexity）时存在局限，即当复杂指令遇到杂乱或模糊场景时，模型难以准确执行编辑任务。需要解决精细区域定位和多区域并行编辑的挑战。

Method: RePlan采用"规划-执行"框架：1）视觉语言规划器分解指令并进行逐步推理，将指令显式定位到目标区域；2）扩散编辑器使用免训练的注意力区域注入机制，实现精确的并行多区域编辑，无需迭代修复；3）使用GRPO强化学习在1K纯指令数据上训练，提升推理保真度和格式可靠性。

Result: 在IV-Complex设置下，RePlan在IV-Edit基准测试中持续优于使用更大数据集训练的强基线模型，显著提升了区域定位精度和整体编辑保真度。强化学习训练带来了推理保真度和格式可靠性的实质性提升。

Conclusion: RePlan通过区域对齐规划和免训练注意力注入机制，有效解决了指令-视觉复杂性下的图像编辑挑战。该框架在精细区域定位和知识密集型编辑任务上表现出色，为复杂场景下的可控图像编辑提供了新思路。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [7] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3是一种无需训练的增强型视频分割框架，针对手术场景中SAM3的局限性进行改进，通过相关性感知内存过滤、分段插值方案和特征重识别模块，显著提升了内窥镜视频中手术器械的分割精度，在零样本设置下相比原始SAM3取得了7-16%的绝对性能提升。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中手术器械的精确分割对于计算机辅助干预至关重要，但由于频繁遮挡、快速运动、镜面伪影和器械长期重新进入等挑战，现有方法效果有限。虽然SAM3提供了强大的时空视频对象分割框架，但其在手术场景中受到非选择性内存更新、固定内存容量和遮挡后弱身份恢复的限制。

Method: ReMeDI-SAM3是SAM3的无训练内存增强扩展，包含三个核心组件：1) 相关性感知内存过滤，配备专门的遮挡感知内存存储遮挡前帧；2) 分段插值方案扩展有效内存容量；3) 基于特征的重识别模块，通过时间投票实现可靠的遮挡后身份消歧。这些组件共同减轻错误累积并实现可靠的遮挡恢复。

Result: 在EndoVis17和EndoVis18数据集上的零样本评估显示，相比原始SAM3分别获得了约7%和16%的绝对mcIoU提升，甚至超越了先前需要训练的方法。

Conclusion: ReMeDI-SAM3通过创新的内存管理策略有效解决了手术视频分割中的关键挑战，在无需额外训练的情况下显著提升了分割性能，为计算机辅助手术提供了更可靠的工具。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [8] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 本文提出M-PhyGs方法，从视频中估计复杂自然物体（以花朵为代表）的材料组成和物理参数，解决了现有方法假设单一材料、预学习动力学或简单拓扑的局限性。通过级联3D和2D损失以及时间小批量处理，实现了高效的多材料物理参数估计。


<details>
  <summary>Details</summary>
Motivation: 现实世界物体通常具有复杂的材料组成和几何形状，而现有方法假设单一材料、预学习动力学或简单拓扑，无法准确估计多材料复杂自然物体的物理材料参数。本文特别关注花朵作为代表性常见物体，需要从自然环境中捕获的视频中估计其材料组成和物理参数。

Method: 提出Multi-material Physical Gaussians (M-PhyGs)方法，从短视频中联合分割物体为相似材料区域并恢复其连续力学参数，同时考虑重力影响。采用新引入的级联3D和2D损失以及时间小批量处理来提高效率。

Result: 在Phlowers数据集（人与花朵交互的新数据集）上的实验结果表明，M-PhyGs及其组件在准确性和有效性方面表现优异，验证了该方法在多材料物理参数估计这一挑战性任务上的能力。

Conclusion: M-PhyGs能够从自然环境中捕获的视频中准确估计复杂多材料自然物体的材料组成和物理参数，为理解现实世界物体的物理特性提供了有效方法，特别是在花朵这类代表性物体上展示了良好的性能。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [9] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut提出了一种从视频大语言模型(VLLMs)中提取世界知识的表示方法，用于视频推荐任务。该方法通过可提示查询从原始帧中提取语义基础、知识感知的token，采用跨层知识融合MoE选择适当的抽象级别，实现了无需手工标注、基于原始帧的视频推荐，在标准基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型(VLLMs)在下游任务如视频推荐中面临挑战：仅解码生成导致顺序推理延迟高、典型接口不支持多视频输入、语言输出约束丢弃了对下游视觉任务重要的细粒度视觉细节。这些限制源于缺乏一种既能保留像素级细节又能利用世界知识的表示方法。

Method: LinkedOut通过可提示查询和可选辅助模态从原始帧中提取语义基础、知识感知的token。引入跨层知识融合MoE，从丰富的VLLM特征中选择适当的抽象级别。该方法支持多视频历史记录，移除了语言瓶颈，实现了快速推理。

Result: LinkedOut在标准基准测试中达到最先进结果，是首个无需手工标注、基于原始帧的VLLM视频推荐方法。可解释性研究和消融实验证实了层多样性和分层融合的益处，为充分利用VLLM世界知识先验和视觉推理提供了实用路径。

Conclusion: LinkedOut提供了一种从VLLMs中提取世界知识的表示方法，解决了当前VLLMs在视频推荐中的部署挑战。该方法支持多视频输入、快速推理，并保留了视觉细节，为下游视觉任务充分利用VLLM的世界知识先验和视觉推理能力开辟了实用路径。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [10] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait提出了一种端到端的视频扩散Transformer，能够合成保持身份一致性的无限长度肖像视频，同时实现高达6倍的推理加速。通过身份无关的面部表情特征提取、归一化面部表情块、动态滑动窗口加权融合以及基于高阶潜在导数的多步跳跃去噪，解决了现有方法在长肖像动画中身份一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的长肖像动画加速方法难以确保身份一致性。现有方法在生成无限长度视频时，面部特征容易漂移，导致身份信息不稳定，同时推理速度较慢。需要一种既能保持身份一致性又能实现高效推理的长肖像动画生成方法。

Method: FlashPortrait采用端到端视频扩散Transformer架构。首先使用现成提取器获取身份无关的面部表情特征，然后通过归一化面部表情块将面部特征与扩散潜变量对齐。推理时采用动态滑动窗口加权融合确保平滑过渡。基于特定时间步的潜变量变化率和扩散层间导数幅度比，利用当前时间步的高阶潜变量导数直接预测未来时间步的潜变量，跳过多个去噪步骤实现6倍加速。

Result: 在基准测试上的实验表明，FlashPortrait在定性和定量评估中都表现出有效性。能够合成保持身份一致性的无限长度肖像视频，同时实现高达6倍的推理速度加速，解决了现有方法在长肖像动画中身份一致性不足的问题。

Conclusion: FlashPortrait通过创新的归一化面部特征对齐、动态滑动窗口机制和高阶导数加速策略，成功实现了身份一致的长肖像动画生成与高效推理的平衡，为长序列肖像视频合成提供了有效的解决方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [11] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA是一个基于指令的视频编辑框架，通过VLM引导编码和奖励优化解决现有方法泛化能力不足的问题。主要创新包括：1）使用VLM编码器将文本指令、视频首帧和参考图像转换为视觉基础指令表示；2）提出Edit-GRPO后训练阶段，通过相对奖励直接优化模型；3）构建合成数据管道生成多样化高质量训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频编辑方法通常只在简单编辑操作的配对数据上训练，这从根本上限制了它们对多样化复杂真实世界指令的泛化能力。为了解决这一泛化差距，需要开发能够处理复杂指令的通用视频编辑框架。

Method: VIVA框架包含三个核心组件：1）VLM引导编码器，将文本指令、视频首帧和可选参考图像编码为视觉基础指令表示；2）Edit-GRPO后训练，将组相对策略优化应用于视频编辑领域，使用相对奖励直接优化模型；3）合成数据管道，生成多样化高质量的视频-指令配对数据用于训练。

Result: 大量实验表明，VIVA在指令遵循、泛化能力和编辑质量方面均优于现有最先进方法。该框架能够处理复杂多样的真实世界编辑指令，同时保持内容保真度和时间连贯性。

Conclusion: VIVA通过结合VLM引导编码和奖励优化，成功解决了基于指令的视频编辑中的泛化问题。该框架为处理复杂真实世界编辑指令提供了可扩展的解决方案，在多个关键指标上超越了现有方法。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [12] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 本文提出了EgoMAN数据集和模型，用于解决现有3D手部轨迹预测研究中运动与语义监督解耦、推理与动作弱关联的问题。通过大规模第一人称视角数据集和推理到运动的框架，实现了语义、空间和运动推理的整合。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究存在两个主要问题：1）数据集将运动与语义监督解耦；2）模型仅弱关联推理与动作。这限制了模型在实际交互场景中的准确性和实用性。

Method: 提出EgoMAN数据集（219K 6DoF轨迹和3M结构化QA对）和EgoMAN模型，采用推理到运动框架，通过轨迹标记接口连接视觉语言推理和运动生成，通过渐进式训练对齐推理与运动动态。

Result: 该方法能够生成准确且阶段感知的轨迹，在真实世界场景中展现出良好的泛化能力，解决了现有方法中推理与动作弱关联的问题。

Conclusion: 通过整合语义推理与运动生成，EgoMAN框架为3D手部轨迹预测提供了更准确、实用的解决方案，能够适应真实世界交互场景的需求。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [13] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文提出了首个多视角变化检测基准SceneDiff Benchmark，包含350个视频对和数千个变化物体实例标注，并提出了无需训练的SceneDiff方法，通过3D对齐、物体区域提取和特征比较来检测多视角下的物体变化，在多个基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 识别场景在不同时间捕获的图像或视频中物体的添加、移除或移动变化，对于机器人整理、施工进度监控等应用很重要。主要挑战在于不同视角可能导致物体被错误地识别为发生变化。

Method: SceneDiff方法是一种无需训练的多视角物体变化检测方法，利用预训练的3D、分割和图像编码模型。首先将捕获的场景在3D空间中对齐，然后提取物体区域，最后比较空间和语义区域特征来检测变化。

Result: 在多视角和双视角基准测试中，SceneDiff方法显著优于现有方法，分别实现了94%和37.4%的相对AP提升。

Conclusion: 本文提出了首个多视角变化检测基准SceneDiff Benchmark和无需训练的SceneDiff方法，通过3D对齐和特征比较有效解决了多视角变化检测问题，在多个基准上取得了显著性能提升，为相关应用提供了有力工具。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [14] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok是一种新型离散图像分词器，通过多步迭代机制实现精确重建。它采用自强制引导视觉重建和去偏拟合训练策略，解决了多步过程中的训练-推理不一致问题，在仅64个token的高压缩率下实现了最先进的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前离散分词器虽然在自回归范式中具有天然优势，但在重建质量上仍落后于连续分词器，这限制了其在多模态系统中的采用。为了解决离散分词器性能不足的问题，需要开发一种能够实现精确图像重建的高质量离散分词器。

Method: SFTok是一种离散分词器，采用多步迭代机制进行精确重建。核心创新包括：1）自强制引导视觉重建机制；2）去偏拟合训练策略。这些方法解决了多步过程中的训练-推理不一致问题，显著提升了图像重建质量。

Result: 在每张图像仅64个token的高压缩率下，SFTok在ImageNet上实现了最先进的重建质量（rFID = 1.21）。在类到图像生成任务中表现出色（gFID = 2.29），显著优于现有方法。

Conclusion: SFTok通过创新的多步迭代机制和训练策略，成功解决了离散分词器的性能瓶颈，在高压缩率下实现了卓越的图像重建质量，为多模态系统中的离散图像表示提供了有力解决方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [15] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V是一个简单有效的指令驱动视频编辑框架，通过创新的数据构建策略（利用现有专家模型、单帧监督、伪视频对、密集标注剪辑和过渡监督）、简化的模型设计（序列拼接+轻量LoRA微调）以及统一的空间时间控制机制，实现了灵活的视频编辑，在质量和一致性上超越了现有系统。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑技术发展迅速，但视频编辑仍面临一致性、控制和泛化能力等挑战。研究者旨在探索数据、架构和控制的设计空间，开发一个简单有效的指令驱动视频编辑框架。

Method: EasyV2V框架包含三个核心创新：1）数据方面，通过组合现有专家模型、单帧监督提升、伪视频对构建、密集标注剪辑挖掘和过渡监督来创建多样化视频编辑对；2）模型方面，发现预训练文本到视频模型已具备编辑能力，采用简单的序列拼接条件和轻量LoRA微调；3）控制方面，通过单一掩码机制统一时空控制，并支持可选参考图像。

Result: EasyV2V在视频编辑任务上取得了最先进的结果，超越了同期研究和商业系统。该框架支持多种灵活输入组合（视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本），实现了高质量、一致性的视频编辑效果。

Conclusion: EasyV2V通过系统性地探索数据、架构和控制的设计空间，提出了一个简单而有效的指令驱动视频编辑框架。该工作展示了通过精心设计的数据构建策略和简化的模型微调方法，可以显著提升视频编辑的质量和灵活性，为视频编辑领域提供了新的解决方案。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [16] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了首个大规模统一立体视频转换数据集UniStereo，并基于此开发了StereoPilot模型，该模型通过前馈方式直接合成目标视图，无需显式深度图或迭代扩散采样，在视觉质量和计算效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 立体显示设备快速增长，但高质量立体视频内容制作成本高、复杂度大。传统的"深度-扭曲-修复"多阶段转换流程存在误差传播、深度模糊以及并行与汇聚立体格式不一致等问题，限制了单目到立体视频转换的质量。

Method: 提出StereoPilot模型，采用前馈架构直接合成目标视图，避免依赖显式深度图。模型包含可学习的域切换器和循环一致性损失，能自适应不同立体格式并提升一致性。基于UniStereo数据集进行训练，这是首个覆盖两种立体格式的统一数据集。

Result: 实验表明，StereoPilot在视觉保真度和计算效率方面显著优于现有最先进方法。模型能够无缝适应不同立体格式，并实现更好的视图一致性。

Conclusion: StereoPilot通过统一数据集和前馈直接合成方法，有效解决了传统立体视频转换中的关键问题，为高质量单目到立体视频转换提供了高效解决方案。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [17] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM是一个自动化框架，通过强化学习训练MLLM作为审计器，主动发现和修复多模态大语言模型的失败模式。该方法生成具有挑战性的问题和反事实图像来最大化目标模型间的分歧，从而揭示模型弱点并作为无标注数据用于模型改进。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型评估方法缺乏可解释性，且往往无法充分揭示模型间的显著能力差距。现有方法不足以全面发现和诊断模型的失败模式，需要一种更主动、可解释的评估框架来识别模型弱点并指导改进。

Method: AuditDM通过强化学习微调一个MLLM作为审计器，使其生成能够最大化目标模型间分歧的挑战性问题和反事实图像。训练后的审计器能够发现多样化的、可解释的失败案例，这些案例揭示了模型弱点，并可作为无标注数据用于模型修复。

Result: 在Gemma-3和PaliGemma-2等先进模型上应用AuditDM，发现了超过20种不同的失败类型。基于这些发现进行微调后，所有模型在16个基准测试中均获得一致改进，并使一个3B参数的模型超越了其28B参数的对应版本。

Conclusion: 随着数据扩展达到收益递减阶段，有针对性的模型审计为模型诊断和改进提供了有效途径。AuditDM框架能够主动发现模型失败模式，生成可解释的弱点示例，并利用这些发现显著提升模型性能，为MLLM评估和改进提供了新范式。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [18] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: AdaTooler-V是一个多模态大语言模型，通过自适应工具使用机制解决现有开源模型盲目调用视觉工具的问题。核心创新包括AT-GRPO强化学习算法和两个训练数据集，在12个基准测试中表现出色，7B版本在V*基准上达到89.8%准确率，超越GPT-4o和Gemini 1.5 Pro。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型存在盲目工具使用问题，即使不需要视觉工具也会调用，这增加了推理开销并降低了模型性能。为了解决这个问题，需要开发能够自适应判断何时真正需要视觉工具的多模态模型。

Method: 提出AdaTooler-V模型，采用AT-GRPO强化学习算法，根据每个样本的工具效益评分自适应调整奖励尺度，鼓励模型仅在工具能真正提升性能时调用。构建两个训练数据集：AdaTooler-V-CoT-100k用于监督微调冷启动，AdaTooler-V-300k用于强化学习训练，涵盖单图像、多图像和视频数据。

Result: 在12个基准测试中表现出强大的推理能力，在多样化视觉推理任务中超越现有方法。AdaTooler-V-7B在V*高分辨率基准上达到89.8%准确率，超越了商业专有模型GPT-4o和Gemini 1.5 Pro。

Conclusion: AdaTooler-V通过自适应工具使用机制有效解决了多模态大语言模型中盲目工具调用的问题，在保持高性能的同时显著降低了推理开销。该方法为多模态推理系统提供了更高效的工具使用策略，所有代码、模型和数据均已开源。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [19] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本文提出NEPA（Next-Embedding Predictive Autoregression），一种从学习表示转向学习模型的视觉自监督方法。受语言生成预训练启发，该方法让模型直接预测未来patch嵌入而非重建像素，无需对比损失或任务特定头，在ImageNet-1K上ViT-B/L分别达到83.8%/85.3% top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成预训练成功的启发，探索同样的原则能否产生强大的视觉自监督学习器。传统方法训练模型输出特征供下游使用，而本文希望训练模型直接生成嵌入来执行预测任务，实现从学习表示到学习模型的转变。

Method: 提出NEPA（Next-Embedding Predictive Autoregression），使用因果掩码和停止梯度，让模型学习基于过去patch嵌入预测未来patch嵌入。采用简单Transformer在ImageNet-1k上预训练，仅使用下一个嵌入预测作为学习目标，无需像素重建、离散标记、对比损失或任务特定头。

Result: NEPA在ImageNet-1K上微调后，ViT-B和ViT-L骨干分别达到83.8%和85.3%的top-1准确率。在ADE20K语义分割任务上也能有效迁移，证明该方法具有强大的跨任务泛化能力。

Conclusion: 从嵌入进行生成预训练为视觉自监督学习提供了一个简单、可扩展且可能模态无关的替代方案。该方法保留了架构简单性和可扩展性，无需额外的设计复杂性，为视觉表示学习开辟了新方向。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [20] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种生成式重聚焦方法，通过两步流程实现单图像重聚焦：先使用DeblurNet从各种输入恢复全焦图像，再用BokehNet生成可控散景。主要创新在于半监督训练方法，结合合成配对数据和未配对的真实散景图像，利用EXIF元数据捕捉真实光学特性。


<details>
  <summary>Details</summary>
Motivation: 景深控制在摄影中至关重要，但获得完美焦点通常需要多次尝试或特殊设备。单图像重聚焦仍然困难，涉及恢复清晰内容和创建逼真散景。现有方法存在显著缺陷：需要全焦输入、依赖模拟器生成的合成数据、对光圈控制有限。

Method: 提出生成式重聚焦的两步流程：1) DeblurNet从各种输入恢复全焦图像；2) BokehNet生成可控散景。核心创新是半监督训练方法，结合合成配对数据和未配对的真实散景图像，利用EXIF元数据捕捉真实光学特性，超越模拟器能力。

Result: 实验表明，该方法在散焦去模糊、散景合成和重聚焦基准测试中达到最佳性能。生成式重聚焦还支持文本引导调整和自定义光圈形状。

Conclusion: 提出的生成式重聚焦方法通过半监督训练有效解决了单图像重聚焦的挑战，结合合成数据和真实光学特性，实现了高质量的去模糊、散景合成和灵活的重聚焦控制。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [21] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个支持多模态输入（文本、轨迹、参考图像）的可提示世界事件生成框架，能够生成包含多智能体交互、物体进出、参考引导外观和反直觉事件的连贯可控视频，实现了从被动预测到交互式用户塑造模拟的进步。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：纯文本方法缺乏对运动、时序和可见性的精确控制，而现有的轨迹控制图像到视频方法缺乏语义意图和视觉基础。需要一种能够结合运动轨迹、语义意图和视觉基础的多模态方法，以生成更丰富、可控的世界事件模拟。

Method: WorldCanvas框架结合三种输入模态：1）轨迹编码运动、时序和可见性；2）自然语言表达语义意图；3）参考图像提供物体身份的视觉基础。通过多模态融合实现可控事件生成，支持多智能体交互、物体进出、参考引导外观和反直觉事件。

Result: 生成的视频不仅具有时间连贯性，还表现出涌现一致性，能够在物体暂时消失时保持物体身份和场景一致性。框架能够生成复杂的世界事件，包括多智能体交互、物体进出、参考引导外观和反直觉事件。

Conclusion: WorldCanvas通过多模态输入实现了丰富、用户导向的世界事件模拟，将世界模型从被动预测器推进到交互式、用户可塑造的模拟器，为可控视频生成和世界建模提供了新方向。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出Grammar Forced Translation (GraFT)框架，通过限制每一步有效输出标记来简化自然语言到时序逻辑的翻译任务，相比现有方法在端到端翻译准确率上平均提升5.49%，域外翻译准确率提升14.06%。


<details>
  <summary>Details</summary>
Motivation: 现有方法将自然语言到时序逻辑的翻译任务分解为原子命题提升和翻译两个阶段，但面临准确提升困难、共指关系存在以及有限数据学习等问题，需要更有效的解决方案。

Method: GraFT框架通过限制每一步有效输出标记来降低任务复杂度，利用每个问题的独特属性减少解空间，将完整词汇表限制为少量有效标记，并提供理论证明说明这种解空间缩减能带来更高效的学习。

Result: 在CW、GLTL和Navi基准测试中，GraFT相比最先进的翻译方法，端到端翻译准确率平均提升5.49%，域外翻译准确率平均提升14.06%。

Conclusion: GraFT框架通过解空间缩减策略有效解决了自然语言到时序逻辑翻译中的关键挑战，在准确率和泛化能力方面均显著优于现有方法。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [23] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种信息论方法，量化语音中仅由韵律（而非文本）传递的信息量，并识别这些信息的具体内容。研究发现，对于讽刺和情感表达，音频通道（即韵律）传递的信息量比纯文本通道高出一个数量级以上。


<details>
  <summary>Details</summary>
Motivation: 韵律（语音的旋律）传递着文本或文字无法捕捉的关键信息。然而，目前缺乏系统的方法来量化韵律单独传递的信息量，以及这些信息具体涉及哪些语义维度。

Method: 采用信息论方法，利用大型语音和语言模型估计话语特定语义维度（如情感）与不同通信通道（如音频或文本）之间的互信息。通过互信息量化音频和文本在讽刺、情感和疑问性等特征上的信息传递能力。

Result: 对于讽刺和情感特征，音频通道（即韵律通道）传递的信息量比纯文本通道高出一个数量级以上，特别是在缺乏当前句子之外的长期上下文时。对于疑问性特征，韵律提供的额外信息相对较少。

Conclusion: 韵律在传达讽刺和情感方面比文本提供更多信息，而疑问性则更依赖文本。研究提出将这种方法扩展到更多语义维度、通信通道和语言，以全面理解韵律在人类交流中的作用。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [24] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache：一种新颖的层级缓存框架，通过基于输入序列语义相似性重用中间激活来加速Transformer推理，实现高达3.1倍加速且精度损失<0.5%


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型虽然性能优异，但其高推理延迟对实时和大规模部署构成挑战。现有的token级键值缓存机制在范围和适用性上有限，需要更通用的加速方案。

Method: LLMCache是一个模型无关的层级缓存框架，支持任意Transformer层的缓存。采用轻量级指纹机制匹配语义相似的输入，并提出自适应淘汰策略管理缓存陈旧性，适用于编码器和解码器架构。

Result: 在BERT和GPT-2模型上，针对SQuAD、WikiText-103和OpenBookQA数据集，LLMCache实现了高达3.1倍的推理时间加速，同时精度损失小于0.5%。

Conclusion: LLMCache是一种实用且通用的解决方案，能够有效优化Transformer在实际应用中的推理效率，为实时和大规模部署提供了可行的技术路径。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [25] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 本文提出了首个针对多模态奖励模型的综合基准MMRB2，涵盖图像生成、编辑、交错生成和推理四大任务，包含4000个专家标注的偏好对。研究发现当前最佳模型准确率仅75-80%，远低于人类的90%以上，揭示了多模态奖励模型发展的关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对于训练大语言模型至关重要，但在处理交错图像和文本序列的全能模型（omni models）中仍缺乏系统研究。现有基准主要关注文本或单一模态，缺少针对多模态理解和交错生成任务的综合评估框架。

Method: 提出了Multimodal RewardBench 2 (MMRB2)基准，包含文本到图像、图像编辑、交错生成和多模态推理四大任务，每个任务提供1000个专家标注的偏好对。采用实际但具有挑战性的提示、最先进模型响应以及通过集成过滤策略筛选的高质量偏好对。

Result: Gemini 3 Pro达到75-80%准确率，GPT-5和Gemini 2.5 Pro为66-75%，GPT-4o仅59%。最佳开源模型Qwen3-VL-32B与Gemini 2.5 Flash相当（64%）。人类专家准确率超过90%，显示多模态奖励模型仍有巨大提升空间。MMRB2性能与下游任务成功强相关。

Conclusion: MMRB2是首个全面的多模态奖励模型基准，揭示了当前模型在复杂多模态任务上的显著局限性。研究为改进奖励模型提供了关键方向，强调了需要更高质量的训练数据和更先进的评估方法。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [26] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 该论文研究了当Transformer在变量符号含义不固定的序列上进行算术训练时，会发展出符号推理机制而非几何嵌入。模型在符号到代数群元素的映射随序列变化的任务中仍能达到接近完美的准确率，并能泛化到未见过的代数群。研究发现模型学习到三种机制：交换复制、单位元识别和基于闭包的消去。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现Transformer在固定符号含义的算术任务中会发展出反映代数结构的几何嵌入，但本研究探索当符号含义不固定、随序列变化时，Transformer会发展出何种推理机制。

Method: 设计新任务：符号到特定代数群元素的映射随序列变化。创建有针对性的数据分布来进行因果测试，分析模型学习到的机制。通过实验隔离出三种机制：交换复制、单位元识别和基于闭包的消去。

Result: Transformer在符号含义不固定的任务中达到接近完美的准确率，并能泛化到未见过的代数群。模型学习到三种一致的机制：1）交换复制：专用头复制答案；2）单位元识别：区分包含单位元的事实；3）闭包消去：跟踪群成员资格以约束有效答案。

Conclusion: 与固定符号设置中的几何表示互补，当训练模型在符号含义不固定的变量上进行上下文推理时，模型会发展出符号推理机制。这表明Transformer能够学习抽象推理模式，而不仅仅是依赖固定的符号-含义映射。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE首次将形式化方法引入LLM压缩，利用信号时序逻辑(STL)形式化指定并强制执行语言特性约束，通过STL鲁棒性引导的贝叶斯优化探索分层量化与剪枝配置，无需重训练或微调即可生成满足形式化约束的压缩模型，在4种LLM架构上实现最高3.3倍计算成本降低和68.8%模型大小缩减。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现出色，但需要大量计算资源，限制了在资源受限的边缘设备上的部署。现有的压缩技术（如量化和剪枝）往往会损害关键的语言特性，并且缺乏保持模型行为的正式保证。

Method: TOGGLE框架利用信号时序逻辑(STL)形式化指定语言特性约束，采用STL鲁棒性引导的贝叶斯优化系统探索分层量化和剪枝配置，生成满足形式化约束的压缩模型，无需重训练或微调。

Result: 在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B四种LLM架构上评估，TOGGLE实现了最高3.3倍计算成本(FLOPs)降低和最高68.8%模型大小缩减，同时满足所有语言特性约束。

Conclusion: TOGGLE首次将形式化方法集成到LLM压缩中，实现了高效、可验证的LLM在边缘硬件上的部署，为资源受限环境下的模型压缩提供了具有形式化保证的新范式。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [28] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 本文提出Generative Adversarial Reasoner（GAR），一种通过对抗性强化学习联合训练LLM推理器和判别器的框架，将推理链分割为逻辑完整的切片并由判别器评估其合理性，产生密集的步骤级奖励信号，显著提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管具有显式推理能力的大语言模型在数学推理方面表现出色，但仍存在过程错误，如计算错误、逻辑脆弱性和表面合理但无效的推理步骤。需要一种方法来增强推理质量并减少这些错误。

Method: 提出Generative Adversarial Reasoner框架，通过对抗性强化学习联合训练LLM推理器和LLM判别器。使用计算高效的审查计划将推理链分割为逻辑完整的切片，判别器评估每个切片的合理性并提供结构化理由。推理器获得逻辑一致且答案正确的奖励，判别器获得正确检测错误的奖励。

Result: 在多个数学基准测试中取得一致提升：在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7（+10.0）。模块化判别器还支持教师蒸馏、偏好对齐和基于数学证明的推理等灵活奖励塑造。

Conclusion: Generative Adversarial Reasoner通过对抗性强化学习联合训练推理器和判别器，产生密集、校准良好的步骤级奖励信号，显著提升大语言模型的数学推理能力，同时支持多种灵活的奖励塑造目标。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [29] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出社会责任感堆栈（SRS），这是一个六层架构框架，将社会价值观嵌入AI系统作为显式约束、保障措施、行为接口、审计机制和治理流程。SRS将责任建模为社会技术系统的闭环监督控制问题，将设计时保障与运行时监控和制度监督相结合。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统越来越多地部署在影响人类行为、机构决策和社会结果的领域。现有的负责任AI和治理工作提供了重要的规范原则，但往往缺乏在整个系统生命周期内可执行的工程机制。

Method: 提出社会责任感堆栈（SRS）六层架构框架：1）社会价值观层，2）规范约束层，3）系统接口层，4）运行时监控层，5）审计与评估层，6）治理与适应层。采用基于约束的统一表述，引入安全包络和反馈解释，将责任建模为社会技术系统的闭环监督控制问题。

Result: 通过临床决策支持、协作式自动驾驶车辆和公共部门系统的案例研究，展示了SRS如何将规范目标转化为可操作的工程和运营控制。该框架连接了伦理学、控制理论和AI治理，为可问责、自适应和可审计的社会技术AI系统提供了实践基础。

Conclusion: SRS框架为将社会价值观嵌入AI系统提供了实用的工程方法，通过闭环监督控制机制确保AI系统在整个生命周期中保持负责任的行为，为构建可问责、自适应和可审计的社会技术AI系统提供了理论基础和实践指导。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>
