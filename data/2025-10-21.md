<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.CL](#cs.CL) [Total: 87]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.MA](#cs.MA) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CrossRay3D的稀疏多模态3D检测器，通过引入Sparse Selector（SS）模块，包含Ray-Aware Supervision和Class-Balanced Supervision，显著提升了稀疏检测器的性能。该方法在nuScenes基准测试中达到72.4 mAP和74.7 NDS的SOTA性能，且运行速度比其他领先方法快1.84倍。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器忽视了token表示的质量，导致前景质量次优且性能受限。本文发现几何结构保持和类别分布是提升稀疏检测器性能的关键。

Method: 提出Sparse Selector（SS）模块，包含Ray-Aware Supervision（RAS）保持训练阶段的几何信息，Class-Balanced Supervision自适应重加权类别语义显著性，确保小物体相关token在采样时被保留。还设计了Ray Positional Encoding解决LiDAR与图像模态间的分布差异。

Result: 在nuScenes基准测试中，CrossRay3D达到72.4 mAP和74.7 NDS的SOTA性能，运行速度比其他领先方法快1.84倍。在LiDAR或相机数据部分或完全缺失的场景下也表现出强鲁棒性。

Conclusion: CrossRay3D通过改进token表示质量，在保持稀疏检测器优势的同时显著提升了性能，证明了几何结构保持和类别平衡监督对稀疏多模态检测的重要性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [2] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出IAD-GPT，一种基于多模态大语言模型的工业异常检测新范式，通过异常提示生成器、文本引导增强器和多掩码融合模块，实现了自监督和少样本异常检测与分割的先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和详细描述能力，而基于大预训练模型的方法尚未充分激发大模型在异常检测任务中的潜力。

Method: 提出IAD-GPT框架，包含异常提示生成器生成详细异常提示，文本引导增强器通过文本提示交互增强视觉定位能力，多掩码融合模块引入掩码作为专家知识增强像素级异常感知。

Result: 在MVTec-AD和VisA数据集上的大量实验表明，该方法在自监督和少样本异常检测与分割任务上达到了最先进的性能。

Conclusion: IAD-GPT成功结合了丰富的文本语义与图像级和像素级信息，为工业异常检测提供了有效的多模态解决方案。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [3] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 本研究评估了三种放射学报告模式（自由文本、结构化报告、AI辅助结构化报告）对图像分析行为、诊断准确性、效率和用户体验的影响。结果显示AI辅助结构化报告在诊断准确性和效率方面表现最佳，能显著改善放射科医生的工作流程。


<details>
  <summary>Details</summary>
Motivation: 探索结构化报告和人工智能如何改变放射科医生与影像研究的交互方式，评估不同报告模式对诊断准确性、效率和用户体验的实际影响。

Method: 前瞻性研究（2024年7-12月），8名读者（4名新手和4名非新手）使用定制化查看器和眼动追踪系统分析35张床旁胸片。比较三种报告模式：自由文本、结构化报告、AI辅助结构化报告，评估诊断准确性、报告时间、眼动指标和用户体验。

Result: AI辅助结构化报告的诊断准确性最高（κ=0.71），报告时间最短（25±9秒）。结构化报告和AI辅助报告显著减少了放射影像区域的扫视次数和报告区域的注视时间。AI辅助结构化报告是最受偏好的模式。

Conclusion: 结构化报告通过引导视觉注意力朝向图像来提高效率，而AI预填充的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [4] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 本文提出了一种数据驱动的框架来分析和缓解图像分类中的交叉偏见，引入了交叉公平性评估框架（IFEF）和偏见加权增强（BWA）方法，在Open Images V7数据集上显著提升了少数群体子组的准确率并减少了公平性指标差异。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在非平衡数据集上训练时出现的交叉偏见问题，这些偏见源于多个属性（如物体类别和环境条件）的交互作用，导致系统性错误。

Method: 提出了交叉公平性评估框架（IFEF）结合定量公平性指标和可解释性工具来识别偏见模式，并开发了偏见加权增强（BWA）策略，基于子组分布统计自适应调整数据增强的强度。

Result: 在Open Images V7数据集上，BWA方法将代表性不足的类别-环境交叉子组的准确率提高了24个百分点，同时将公平性指标差异减少了35%。统计分析确认了改进的显著性（p < 0.05）。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏见提供了一个可复现的框架，有效提升了模型的公平性和性能。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [5] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的方法，通过预训练的整流流模型和周期性引导，成功解决了在输入和外观对象几何差异较大时的外观迁移问题。该方法在纹理和几何细节迁移方面优于基线方法，并引入了基于GPT的客观评估系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入和外观对象几何差异较大时失败，直接应用3D生成模型无法产生理想结果。需要一种能够处理几何不匹配问题的外观迁移方法。

Method: 基于预训练的整流流模型（支持图像或文本条件），在采样过程中周期性添加引导。引导建模为可微分损失函数，包括外观感知损失和自相似性损失，无需额外训练。

Result: 方法成功将纹理和几何细节迁移到输入3D资产，在定性和定量评估中均优于基线方法。传统指标不适合此任务评估，因此开发了基于GPT的客观排名系统。

Conclusion: 该方法通用性强，可扩展到不同类型的扩散模型和引导函数，为游戏、增强现实和数字内容创作等领域的3D资产外观迁移提供了有效解决方案。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [6] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自监督框架，用于在缺血性卒中血栓切除术中自动化关键步骤。该框架通过回归式前置任务分类骨骼标志点，在回归和分类任务中均优于现有方法，且位置前置任务显著提升了下游分类性能。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是缺血性卒中最有效的治疗方法之一，但资源密集且对人员要求高。作者旨在利用深度学习自动化血栓切除术的关键环节，提高效率与安全性。

Method: 提出自监督学习框架，使用回归式前置任务对多种骨骼标志点进行分类。该方法通过位置预测任务学习特征表示，为下游分类任务提供更好的初始化。

Result: 实验表明，该模型在回归和分类任务中均优于现有方法。位置前置任务显著提升了下游分类性能，验证了自监督学习在医学图像分析中的有效性。

Conclusion: 该自监督框架成功提升了血栓切除术中骨骼标志点分类的准确性。未来工作将扩展该框架以实现完全自主的C臂控制，优化从骨盆到头部的轨迹规划，进一步提高手术效率。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [7] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种自主导航C臂到预定义解剖标志的管道，利用X射线图像预测3D位移向量，结合不确定度估计和保形预测确保可靠性，在合成X射线数据集上验证了定位准确性和良好校准的预测边界。


<details>
  <summary>Details</summary>
Motivation: 当前临床工作流程依赖手动对齐C臂，这会增加辐射暴露和手术延迟，需要开发自主导航系统来提高效率和安全性。

Method: 提出基于X射线图像的自主C臂导航管道，使用概率模型预测3D位移向量，结合不确定度估计和保形预测生成3D置信区域，训练框架结合概率损失和骨骼姿态正则化以确保解剖学合理性。

Result: 在DeepDRR生成的合成X射线数据集上验证，结果显示跨多个架构的强定位准确性以及良好校准的预测边界。

Conclusion: 该管道具有作为安全可靠自主C臂系统组件的潜力，能够减少辐射暴露和手术延迟。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [8] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 本文提出了一种自动图像质量评估预过滤方法，通过在生成图像流水线中引入自动筛选阶段，显著降低了手动质量评估的成本，在背景修复用例中实现了51.61%的成本节省。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型虽然能产生高质量图像，但尚未达到传统摄影方法的质量标准，导致生产流水线需要手动图像质量评估，这个过程既缓慢又昂贵，特别是因为自动生成图像通过质量标准的比例较低。

Method: 提出一个自动预过滤阶段，使用通用IQA引擎根据精度和通过率来估计成本节省的公式，在背景修复用例中应用简单的AutoML解决方案。

Result: 在背景修复用例中，通过简单的AutoML解决方案实现了51.61%的显著成本节省。

Conclusion: 自动图像质量预过滤可以显著降低生成图像生产流水线的成本，提高整体效率，为实际应用提供了可行的解决方案。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [9] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提出了一种数据为中心的人工智能（DCAI）方法来解决热带地区农业遥感制图的挑战，包括标注数据缺乏、标注成本高、数据变异性和区域泛化问题。文章强调了数据质量和整理的重要性，并推荐了25种成熟策略，特别提出了9种最成熟简单的方法用于大规模热带农业制图项目。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感制图面临独特挑战：高质量标注数据缺乏、标注成本高昂、数据变异性大、区域泛化困难。传统以模型为中心的方法受限于高云量、多样作物日历和有限数据集，需要转向数据为中心的方法来应对热带农业的动态现实。

Method: 采用数据为中心的人工智能（DCAI）视角和流程，重点应用置信学习、核心集选择、数据增强和主动学习等技术。通过数据质量管理和整理来提升模型鲁棒性和可扩展性，特别针对热带农业遥感场景优化数据处理流程。

Result: 识别并优先排序了25种适用于大规模农业制图流程的成熟策略，其中9种最成熟简单的方法被整合为实用流程，能够有效应对热带农业遥感中的云量、作物日历多样性和数据限制等问题。

Conclusion: 数据为中心的方法为热带农业遥感制图提供了实用解决方案，通过优化数据质量和整理流程，能够训练出更适合热带农业动态现实的AI模型，解决了传统模型中心方法的局限性。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [10] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个大规模多模态3D运动数据集，包含500小时来自439名参与者的3D运动数据，总计超过5400万帧追踪数据。数据集涵盖单人多模态运动（如提示动作、手势、移动）和多人行为数据（如讨论、情感对话、协作活动等）。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有3D人体运动数据集中缺乏大规模、多模态、真实社交交互数据的问题，为计算机视觉、人机交互和社交行为分析研究提供高质量的训练和评估资源。

Method: 通过在多相机采集环境中收集439名参与者的数据，使用3D运动追踪技术获取身体运动、手部追踪和体型数据，同时提供文本标注和独立音频轨道，构建大规模多模态数据集。

Result: 成功构建了包含500小时3D运动数据的Embody 3D数据集，涵盖单人和多人交互场景，提供了丰富的运动、手势、对话和行为数据，为相关研究提供了重要资源。

Conclusion: Embody 3D数据集填补了大规模多模态3D运动数据的空白，为人体运动分析、社交行为研究和人机交互系统开发提供了宝贵的基准数据集。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [11] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: 本文提出了Cerberus系统，这是一个用于实时视频异常检测的两级级联系统，通过结合轻量级过滤和细粒度视觉语言模型推理，在保持高精度的同时实现了151.79倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型的视频异常检测方法虽然具有优越的零样本检测能力，但计算成本巨大且视觉定位性能不稳定，阻碍了实时部署。

Method: Cerberus系统采用两级级联架构：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理。关键创新包括运动掩码提示和基于规则的偏差检测，前者引导VLM关注运动相关区域，后者将异常识别为对学习规范的偏离。

Result: 在四个数据集上的广泛评估显示，Cerberus在NVIDIA L40S GPU上平均达到57.68 fps，实现了151.79倍的加速，同时保持97.2%的准确率，与最先进的VLM-based VAD方法相当。

Conclusion: Cerberus为实时视频分析提供了一个实用的解决方案，在保持高精度的同时显著提升了处理速度。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [12] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了评估大型视觉语言模型成员推理攻击时的根本挑战。研究发现先前工作报告的高攻击成功率实际上源于检测数据集构建过程中引入的分布偏差，而非真实成员身份识别。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击评估存在偏差问题，先前报告的高成功率实际上源于数据集构建过程中的分布偏差，而非真正的攻击能力。需要建立无偏基准来准确评估成员推理攻击的实际效果。

Method: 构建包含6,000张图像的受控基准，精心平衡成员和非成员样本的分布，提供三个不同训练阶段的真实成员标签，确保评估环境的无偏性。

Result: 在无偏条件下，最先进的成员推理攻击方法性能收敛到随机猜测水平，表明先前报告的高成功率主要源于数据集偏差而非真正的攻击能力。

Conclusion: OpenLVLM-MIA为LVLM成员推理攻击研究提供了透明无偏的基准，揭示了当前方法的实际局限性，为开发更强的隐私保护技术奠定了基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [13] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一个无需训练的框架，通过跨图像笔画注意力机制实现精确的笔画属性转移，能够自适应地将参考图像的笔画特征整合到内容图像中，同时保持结构完整性。


<details>
  <summary>Details</summary>
Motivation: 生成具有参考风格引导的草图需要精确转移笔画属性（如线条粗细、变形和纹理稀疏度），同时保持语义结构和内容保真度。现有方法在表达性笔画控制和语义连贯性方面存在不足。

Method: 提出Stroke2Sketch框架，引入跨图像笔画注意力机制嵌入自注意力层，建立细粒度语义对应关系，实现准确的笔画属性转移。还开发了自适应对比度增强和语义聚焦注意力来加强内容保持和前景强调。

Result: Stroke2Sketch能够有效合成风格忠实的草图，与手工制作结果非常相似，在表达性笔画控制和语义连贯性方面优于现有方法。

Conclusion: Stroke2Sketch通过跨图像笔画注意力机制实现了精确的笔画属性转移，能够生成风格忠实且语义连贯的草图，为风格引导的草图生成提供了有效的解决方案。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [14] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文系统研究了深度伪造检测的缩放规律，发现检测误差随真实图像域数量和深度伪造方法数量的增加呈幂律衰减，类似于大语言模型的缩放规律。基于此构建了ScaleDF数据集，包含580万真实图像和880万伪造图像。


<details>
  <summary>Details</summary>
Motivation: 研究深度伪造检测任务中的缩放规律，分析模型性能与真实图像域数量、深度伪造生成方法和训练图像数量之间的关系，为应对不断演变的深度伪造技术提供数据驱动的解决方案。

Method: 构建ScaleDF数据集（包含51个真实图像域的580万图像和102种深度伪造方法的880万伪造图像），系统分析模型性能与数据规模的关系，发现幂律缩放规律，并研究预训练和数据增强在缩放中的作用。

Result: 观察到深度伪造检测误差随真实域数量或深度伪造方法数量的增加呈幂律衰减，可以预测达到目标性能所需的额外数据规模，揭示了数据规模对检测性能的关键影响。

Conclusion: 深度伪造检测遵循幂律缩放规律，通过增加真实图像域和深度伪造方法的多样性可以显著提升检测性能，为应对深度伪造技术提供了数据中心的解决方案，但缩放本身也存在局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [15] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 本文提出Scale-DiT框架，通过分层局部注意力和低分辨率全局引导，实现高效、可扩展的超高分辨率图像生成，在4K分辨率下比密集注意力基线快2倍以上且内存使用更低，无需额外高分辨率训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受限于注意力的二次复杂性和原生4K训练数据的稀缺，无法实现超高分辨率（超过1K×1K）的图像生成，需要新的方法来平衡细粒度纹理合成和全局结构一致性。

Method: Scale-DiT引入分层局部注意力机制，将高分辨率潜在空间划分为固定大小的局部窗口以降低注意力复杂度，同时使用低分辨率潜在空间提供全局语义引导。通过LoRA适配器桥接全局和局部路径，采用Hilbert曲线重排token序列和融合内核优化推理效率。

Result: Scale-DiT在4K×4K分辨率下实现超过2倍的推理加速和更低的内存使用，在FID、IS、CLIP Score等量化指标和定性比较中均优于或匹配依赖原生4K训练的先进方法，提供更好的全局一致性和更清晰的局部细节。

Conclusion: 分层局部注意力配合引导性低分辨率锚点是推进超高分辨率图像生成的有效方法，Scale-DiT框架在无需额外高分辨率训练数据的情况下可靠扩展到4K分辨率。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [16] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 本文提出了DiffusionX，一个云边协同框架，用于高效的多轮基于提示的图像生成。该系统通过轻量级设备端扩散模型快速生成预览图像，高容量云端模型进行最终优化，并引入噪声水平预测器动态平衡计算负载，在保持图像质量的同时显著降低生成时间。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了显著进展，但生成过程计算密集，用户通常需要迭代优化提示词，这会增加延迟并给云资源带来沉重负担。为了解决这一挑战，需要开发更高效的生成框架。

Method: 提出云边协同框架DiffusionX，包含轻量级设备端扩散模型用于快速生成预览图像，高容量云端模型进行最终优化。引入噪声水平预测器动态平衡计算负载，优化延迟与云工作负载之间的权衡。

Result: 实验表明，DiffusionX相比Stable Diffusion v1.5平均生成时间减少15.8%，同时保持相当的图像质量。相比Tiny-SD仅慢0.9%，但图像质量显著提升，展示了高效性和可扩展性。

Conclusion: DiffusionX通过云边协同和动态负载平衡，在保持图像质量的同时显著提高了扩散模型的生成效率，为多轮提示词优化场景提供了实用的解决方案。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [17] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出TokenAR框架，通过token级增强机制解决多参考图像生成中的身份混淆问题。该框架包含三个核心组件：Token索引嵌入、指令token注入和身份token解缠策略，显著提升了自回归模型在多主体图像生成中的身份一致性和背景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在多参考图像生成中存在不同参考身份解耦困难的问题，导致生成图像中出现身份混淆。本文旨在解决这一挑战，提升多主体图像生成的质量和多样性。

Method: 提出TokenAR框架，包含三个token级增强机制：1) Token索引嵌入，聚类相同参考图像的token索引；2) 指令token注入，作为额外视觉特征容器注入详细先验；3) 身份token解缠策略，明确引导token表示独立表征每个身份特征。

Result: 实验验证该方法在多个参考图像生成任务中超越了当前最先进模型，实现了良好的身份一致性和高质量的背景重建。同时发布了首个开源的大规模多参考图像生成数据集InstructAR，包含28K训练对。

Conclusion: TokenAR框架通过token级增强机制有效解决了多参考图像生成中的身份混淆问题，显著提升了自回归模型在条件图像生成中的性能，为多主体图像生成提供了新的解决方案。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [18] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度的语言辅助图像聚类框架GradNorm，通过理论分析和实验验证，解决了从无标注文本数据中筛选与图像语义相近的正向名词的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的语言辅助图像聚类方法主要依赖CLIP预训练特征空间来筛选正向名词，但这些策略缺乏严格的理论基础，需要开发具有理论保证的改进方法。

Method: 提出GradNorm框架，通过计算预测目标分布与softmax输出之间的交叉熵梯度幅度来衡量名词的正向性，理论证明该方法包含现有策略作为特例。

Result: 在多个基准测试中，GradNorm实现了最先进的聚类性能，实验验证了其有效性。

Conclusion: GradNorm不仅提供了理论保证，还在实践中表现出优越性能，为语言辅助图像聚类提供了可靠的正向名词筛选方法。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [19] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 本文提出了首个专门针对社交制造场景的异常检测基准数据集MIRAD，该数据集捕捉了个性化产品、分布式制造节点和成像异质性三个关键维度，并在该数据集上评估了多种SOTA异常检测方法，发现性能显著下降，揭示了真实世界个性化生产中缺陷检测的未解决复杂性。


<details>
  <summary>Details</summary>
Motivation: 社交制造模式虽然实现了大规模个性化生产，但带来了质量控制特别是缺陷检测的挑战，主要困难包括：产品配置高度定制化、生产订单碎片化小批量、分布式站点的成像环境差异大。现有方法缺乏真实世界数据集和针对性算法。

Method: 构建了MIRAD数据集，包含三个关键维度：多样化个性化产品（大类别内变异）、六个地理分散制造节点的数据收集、显著的成像异质性（光照、背景、运动条件变化）。在此基础上评估了单类别、多类别和零样本三种SOTA异常检测方法。

Result: 所有模型在MIRAD数据集上的性能相比传统基准都出现了显著下降，突显了真实世界个性化生产中缺陷检测的未解决复杂性。

Conclusion: MIRAD数据集通过连接工业需求和学术研究，为开发工业5.0所需的稳健质量控制解决方案提供了现实基础，填补了社交制造领域异常检测基准的空白。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [20] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 本文提出了一个包含3000个白内障超声乳化手术视频的数据集，来自两个手术中心，包含四种标注层：手术阶段、器械和结构分割、器械-组织交互跟踪以及技能评分，并提供了基准测试和领域适应基线。


<details>
  <summary>Details</summary>
Motivation: 当前白内障手术数据集缺乏多样性和深度标注，无法训练泛化性强的深度学习模型，因此需要构建大规模、多标注层的手术视频数据集来支持计算机辅助手术系统的发展。

Method: 收集3000个白内障超声乳化手术视频，来自两个手术中心，包含不同经验水平的外科医生。提供四种标注：手术阶段、实例分割、器械-组织交互跟踪和基于ICO-OSCAR的技能评分。建立基准测试和领域适应基线。

Result: 数据集支持关键手术AI任务的基准测试，包括工作流识别、场景分割和自动技能评估。在阶段识别任务中建立了领域适应基线，展示了模型在不同手术中心的泛化能力。

Conclusion: 该数据集填补了白内障手术数据资源的空白，为开发更通用和鲁棒的计算机辅助手术系统提供了重要基础，并通过基准实验验证了其技术质量。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [21] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: 本文提出了Demeter，一种数据驱动的参数化植物形态模型，能够编码植物的拓扑结构、形状、关节运动和变形等关键因素。该模型能够处理不同物种的拓扑变化，并建模三种形状变化来源：关节运动、子组件形状变化和非刚性变形。


<details>
  <summary>Details</summary>
Motivation: 虽然现有模型在人类和动物建模方面很强大，但缺乏对植物建模的同等表达能力。植物形态建模在农业、生态学和计算机图形学中具有重要应用价值。

Method: Demeter是一种数据驱动的参数化模型，通过紧凑的学习表示编码植物形态的关键因素。模型处理不同物种的拓扑变化，并建模三种形状变化来源：关节运动、子组件形状变化和非刚性变形。基于收集的大规模大豆农场数据集进行开发。

Result: 实验表明，Demeter能够有效合成形状、重建结构并模拟生物物理过程。模型在作物植物建模方面表现出色，特别是在大豆植物的形态建模上。

Conclusion: Demeter为植物形态建模提供了一个强大的参数化框架，填补了现有模型在植物建模方面的空白，在农业应用和计算机图形学中具有广泛的应用前景。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [22] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级框架，采用编码器-解码器架构，通过稀疏卷积、SPLite解码器和量化感知训练，在保持准确性的同时显著提升AR/VR设备上的推理效率，在树莓派5上实现2.98倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备的普及，边缘设备需要实时推理、低功耗和最小延迟，但现有框架难以平衡效率与性能。

Method: 基于ResNet-18编码器应用稀疏卷积利用手部姿态图像固有稀疏性，提出SPLite解码器提升解码帧率，采用量化感知训练减少内存使用。

Result: 端到端效率提升42%，解码帧率提升3.1倍，内存使用减少，PA-MPJPE仅从9.0mm增加到9.1mm，在树莓派5上实现2.98倍加速。

Conclusion: 该方法在保持与最先进方法相当准确性的同时，显著提升了计算效率，适用于AR/VR边缘设备部署。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [23] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL是一个新颖的框架，利用自监督学习任务作为可验证的奖励信号，用于视觉语言模型的强化学习微调，解决了传统方法缺乏可扩展和可靠奖励机制的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视觉中心任务中过度依赖语言先验，或在推理中使用文本捷径，缺乏有效利用视觉证据的能力。现有强化学习方法因缺乏可扩展可靠的奖励机制而难以应用。

Method: 将自监督学习目标（如图像旋转预测、掩码补丁重建）转化为密集的自动奖励信号，无需人工偏好数据或不可靠的AI评估器。通过强化学习微调视觉语言模型。

Result: SSL4RL显著提升了在视觉中心和视觉语言推理基准测试上的性能。系统消融实验确定了任务难度、模型规模和语义对齐等关键影响因素。

Conclusion: SSL4RL建立了一个通用有效的范式，使用可验证的自监督目标来对齐多模态模型，并在图学习领域也展示了良好的泛化能力。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [24] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了可解释深度伪造视频检测（EDVD）任务，设计了EDVD-LLaMA多模态大语言模型推理框架，通过时空细微信息标记化和细粒度多模态思维链机制，在提供准确检测结果的同时给出可追溯的推理过程和可信解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法存在原理不透明、泛化能力不足的问题，无法应对不断演变的伪造技术，迫切需要能够识别伪造内容并提供可验证推理解释的检测器。

Method: 提出EDVD-LLaMA框架：1）时空细微信息标记化（ST-SIT）提取融合全局和局部跨帧深度伪造特征；2）细粒度多模态思维链（Fg-MCoT）引入面部特征数据作为硬约束，实现像素级时空视频定位；3）构建可解释推理FF++基准数据集（ER-FF++set）支持推理和检测的双重监督。

Result: 大量实验表明，EDVD-LLaMA在检测精度、可解释性以及处理跨伪造方法和跨数据集场景方面表现出卓越的性能和鲁棒性，相比之前的DVD方法提供了更可解释和优越的解决方案。

Conclusion: EDVD-LLaMA框架成功解决了深度伪造检测的可解释性问题，通过多模态推理和时空特征提取，为深度伪造视频检测提供了透明、可靠且具有强泛化能力的解决方案。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [25] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了RefAVA++数据集和RefAtomNet++框架，用于解决基于自然语言描述的原子级视频动作识别问题。RefAVA++包含超过290万帧和7.5万个标注人物，RefAtomNet++通过多层级语义对齐交叉注意力和多轨迹Mamba建模，在跨模态信息对齐和检索方面取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统的动作识别和检测任务难以满足复杂多人物场景中基于语言描述的精确动作理解需求。现有的RefAtomNet模型在跨模态信息对齐和目标人物定位方面存在局限，需要更有效的框架来处理细粒度、原子级的动作识别。

Method: 提出RefAtomNet++框架，采用多层级语义对齐交叉注意力机制和多轨迹Mamba建模。在部分关键词、场景属性和整体句子三个层级构建扫描轨迹，动态选择最近的视觉空间标记，实现更有效的时空标记聚合。

Result: 实验表明RefAtomNet++在RefAVA++数据集上取得了最先进的性能，显著优于包括RefAtomNet在内的多个基线模型，在目标人物定位和细粒度动作预测方面表现优异。

Conclusion: RefAtomNet++通过创新的多层级语义对齐和Mamba建模机制，有效解决了基于语言描述的原子级视频动作识别问题，为复杂多人物场景中的交互式动作分析提供了有力工具。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [26] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 本文提出了一种改进的损失函数，通过高斯边界框表示和Bhattacharyya距离来增强旋转目标检测的准确性和鲁棒性，并采用各向异性高斯表示解决类方形物体的各向同性方差问题。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测框架在处理旋转目标时表现不佳，因为它们难以捕捉方向变化。在航空影像、遥感和自动驾驶等应用中，准确检测旋转目标是一个重要挑战。

Method: 提出基于高斯边界框表示和Bhattacharyya距离的旋转不变损失函数，采用各向异性高斯表示处理类方形物体，并将该损失函数集成到最先进的深度学习旋转目标检测器中。

Result: 广泛的实验表明，与现有方法相比，在平均精度均值指标上取得了显著改进，有望为旋转目标检测建立新的基准。

Conclusion: 该方法能够有效捕获旋转目标的几何特性，在各种应用中实现精确可靠的目标定位，不受方向影响。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [27] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督域自适应方法，用于电子显微镜图像中的线粒体分割。通过利用稀疏点标注和多任务学习框架，结合交叉教学机制和类聚焦跨域对比学习，显著提升了分割性能，缩小了与全监督方法的差距。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中线粒体实例分割对生物和神经科学研究很有价值，但全标注成本高昂。无监督域自适应方法在实际应用中性能较低，因此研究利用目标域上稀疏点标注的弱监督域自适应方法，以最小标注成本和专家知识实现有效分割。

Method: 提出多任务学习框架，联合进行分割和中心检测任务。采用交叉教学机制和类聚焦跨域对比学习，充分利用不完整和不精确的点标注。引入具有实例感知伪标签选择策略的分割自训练，语义选择可靠且多样的伪标签。

Result: 在挑战性数据集上的全面验证和比较表明，该方法优于现有的无监督域自适应和弱监督域自适应方法，显著缩小了与监督上界的性能差距。在无监督域自适应设置下，相比其他技术也实现了显著改进。

Conclusion: 该方法通过有效利用稀疏点标注和多任务学习，为电子显微镜图像中的线粒体分割提供了一种高效且性能优越的解决方案，在弱监督和无监督域自适应设置下均表现出色。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [28] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 本文提出了一种前瞻性的视觉语言导航方法，通过Q学习从大规模无标签轨迹数据中学习室内场景布局和物体关系的通用知识，生成描述潜在未来信息的Q特征，结合导航指令预测行动的未来前景得分，并与历史得分结合实现A*搜索策略。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法主要基于历史信息做决策，忽略了行动的长期后果和未来影响，因此需要开发能够预见未来的智能体。

Method: 使用Q学习训练Q模型，从大规模无标签轨迹数据中学习场景布局和物体关系知识，生成描述潜在未来信息的Q特征。通过跨模态未来编码器将任务无关的Q特征与导航指令结合，产生反映未来前景的行动得分，结合历史得分实现A*搜索策略。

Result: 在广泛使用的目标导向VLN数据集上进行的广泛实验验证了所提方法的有效性。

Conclusion: 提出的前瞻性视觉语言导航方法通过结合未来信息预测和历史信息，能够更有效地探索可能通往目的地的区域，在目标导向VLN任务中表现出色。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [29] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 本文提出OOS-DSD方法，通过辅助学习在YOLOv8架构上同时进行缺货检测、产品分割和场景深度估计，使用伪标签深度数据进行训练，并提出了深度归一化方法，在缺货检测任务上比现有最优方法提升了1.8%的mAP。


<details>
  <summary>Details</summary>
Motivation: 缺货检测是零售验证过程中的重要环节，现有方法性能有待提升。通过辅助学习结合深度信息可以增强模型对场景的理解能力，从而提高缺货检测的准确性。

Method: 基于YOLOv8目标检测架构，增加卷积分支同时进行缺货检测、产品分割和深度估计。深度估计分支使用Depth Anything V2模型生成的伪标签进行训练，并提出了深度归一化方法来稳定训练过程。

Result: 提出的方法在缺货检测任务上比现有最优方法提升了1.8%的mAP。消融研究表明辅助学习贡献了3.7%的mAP提升，深度归一化方法贡献了4.2%的mAP提升。

Conclusion: 通过辅助学习结合深度信息可以有效提升缺货检测性能，深度归一化方法对训练稳定性有重要贡献，该方法在零售验证场景中具有实用价值。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [30] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 本文提出了READ方法，通过在对比学习中添加两个辅助目标来增强视觉语言模型的组合推理能力：令牌级重建目标和句子级对齐目标。READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强基线提升达4.1%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在组合推理方面表现不佳，主要原因是文本编码器倾向于关注单个单词而非它们之间的关系，这种局限性被主要将单词与视觉对象对齐的对比训练所强化。

Method: READ方法在对比学习基础上添加两个辅助目标：1）令牌级重建目标，使用冻结预训练解码器基于原始标题嵌入重建替代标题；2）句子级对齐目标，在嵌入空间中显式对齐释义句子。

Result: READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强传统微调基线提升达4.1%。将READ应用于现有CLIP变体（包括NegCLIP和FSC-CLIP）也能提升这些基准测试的性能。

Conclusion: 重建和对齐目标提供互补优势：重建鼓励编码器捕捉标题内单词间关系，而对齐确保不同措辞表达的释义具有一致表示。READ方法有效提升了视觉语言模型的组合推理能力。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [31] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于区域感知的动态聚合与激励框架（GaitRDAE），用于步态识别。该框架能自动搜索运动区域，分配自适应时间尺度并应用相应注意力，解决了现有方法使用预定义区域和固定时间尺度难以建模动态变化运动区域的问题。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法通常使用预定义区域进行时序建模，并为不同类型区域分配固定或等效的时间尺度，这使得难以建模随时间动态变化的运动区域并适应其特定模式。特别是在协变量影响视觉外观时，考虑不同运动区域中独特多样的行为模式对于准确步态识别至关重要。

Method: 提出GaitRDAE框架，包含两个核心模块：区域感知动态聚合（RDA）模块动态搜索每个区域的最佳时间感受野；区域感知动态激励（RDE）模块强调包含更稳定行为模式的运动区域学习，同时抑制对更易受协变量影响的静态区域的注意力。

Result: 实验结果表明，GaitRDAE在多个基准数据集上实现了最先进的性能。

Conclusion: GaitRDAE通过自动搜索运动区域、分配自适应时间尺度和应用区域特定注意力，有效提升了步态识别的准确性，特别是在处理动态变化运动区域和协变量影响方面表现出色。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [32] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文首次将大型视觉语言模型(LVLM)中的物体幻觉问题追溯到视觉编码器，识别出统计偏差、固有偏差和脆弱性三个关键问题，并提出了无需训练的SHIELD框架来缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在跨模态任务中表现出色，但物体幻觉问题（模型生成看似合理但实际不准确的物体描述）仍然是一个重大挑战。与之前专注于LLM组件的研究不同，本文首次将LVLM幻觉问题追溯到视觉编码器。

Method: 提出SHIELD框架，包含三种策略：重新加权视觉token以减少统计偏差、引入噪声衍生token来对抗固有偏差、应用对抗攻击与对比解码来解决脆弱性问题。这是一个无需训练的框架。

Result: 实验表明SHIELD在各种基准测试和LVLM家族中有效缓解了物体幻觉问题，同时在通用LVLM基准测试上表现出色，证明了其广泛适用性。

Conclusion: SHIELD框架成功解决了LVLM中的物体幻觉问题，通过针对视觉编码器的三个关键问题提供了有效的训练免费解决方案，具有广泛的适用性和良好的性能表现。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [33] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出VisionSelector，一种轻量级即插即用的视觉令牌压缩框架，通过可学习的决策过程解决多模态大语言模型中视觉令牌过多导致的计算和内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像或多图像输入时，会产生大量视觉令牌，导致显著的计算和内存瓶颈。现有的令牌压缩技术受限于启发式规则，可能丢弃关键信息，并且在激进压缩率下性能急剧下降。

Method: 提出VisionSelector框架，包含解耦的评分器模块，采用可微分Top-K机制和课程退火策略，实现端到端可学习的令牌选择过程。该方法仅需12.85M可训练参数，支持任意压缩率下的自适应令牌选择。

Result: 在30%保留预算下保持MME基准100%准确率，在10%保留预算下比先前方法提升12.14%性能，同时将预填充速度提高一倍。

Conclusion: VisionSelector提供了一种高效且自适应的令牌压缩解决方案，在各种压缩预算下均表现出优越性能，解决了现有方法的局限性。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [34] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: 本文提出了MultiVerse，一个新颖的多轮对话基准测试，包含647个对话，涵盖12个流行的VLM评估基准的484个任务。研究发现即使最强模型在复杂多轮对话中成功率仅50%，并提供完整对话上下文能显著提升较小或较弱模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话数据集（如MMDU、ConvBench）仅部分覆盖用户遇到的各种对话场景的广度和深度，而真实世界应用通常需要更复杂的多轮对话。

Method: 从12个流行的VLM评估基准中提取647个对话，每个对话平均4轮，涵盖484个任务和交互目标。提出基于检查表的评估方法，使用GPT-4o作为自动评估器，测量37个关键方面的性能。

Result: 评估了18个VLM，发现即使最强模型（如GPT-4o）在复杂多轮对话中仅达到50%成功率。提供完整对话上下文能显著提升较小或较弱模型的性能。

Conclusion: MultiVerse是评估VLM多轮交互能力的景观性基准，揭示了当前模型在多轮对话中的局限性，并强调了上下文学习的重要性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [35] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 本文提出了一种使用检索增强生成（RAG）方法，通过图数据库和Cypher查询语言接口来连接大型语言模型与3D场景图，解决了传统文本序列化方法在处理大规模复杂3D场景图时的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D场景图编码为序列化文本放入LLM上下文窗口，但这种方法无法扩展到大型或丰富的3D场景图。需要一种更高效的方式来连接自然语言与机器人的世界表示。

Method: 使用检索增强生成方法，将3D场景图存储在图形数据库中，为LLM提供Cypher查询语言作为工具接口，使其能够检索与任务相关的场景图数据。

Result: 在指令跟随和场景问答任务上的评估表明，使用Cypher接口的方法在本地和云端模型上都能显著扩展到大型复杂图，在语言接地任务中带来大幅性能提升，同时大幅减少场景图内容的token数量。

Conclusion: Cypher作为3D场景图的接口，相比传统的上下文窗口和代码生成方法，在处理大规模复杂图时具有更好的扩展性，显著提高了语言接地任务的性能。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [36] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 本文提出了针对病理学基础模型的通用可迁移对抗扰动（UTAP），这是一种固定且微弱的噪声模式，能够系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能显著下降。UTAP具有普遍性和可迁移性两大特征，能够跨数据集和模型发挥作用，构成了对病理学AI应用的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型在对抗攻击下的关键脆弱性，建立高标准的模型鲁棒性评估基准，推动防御机制的发展，确保AI在病理学中的安全可靠部署。

Method: 通过深度学习优化生成固定的微弱噪声模式UTAP，该扰动可添加到病理图像中，系统性地破坏多个病理学基础模型的特征表示能力。UTAP具有普遍性（跨不同视野和数据集）和可迁移性（对未见过的黑盒模型有效）两大关键特征。

Result: 在多个最先进的病理学基础模型和数据集上系统评估显示，UTAP通过视觉不可察觉的固定噪声模式，显著降低了模型性能，导致广泛的误分类。该攻击对未见过的外部黑盒模型同样有效。

Conclusion: UTAP构成了对新兴病理学基础模型及其应用的广泛威胁，其开发为模型鲁棒性评估建立了关键的高标准基准，强调了推进防御机制和对抗训练的必要性，以确保病理学AI的安全可靠部署。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [37] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出了一种名为HYDRA的新方法，通过混合知识蒸馏和光谱重建架构，解决了传统多尺度注意力方法在密集光谱重建中的局限性。该方法使用教师模型封装潜在高光谱图像数据，学生模型学习从自然图像到教师编码域的映射，实现了高质量的光谱重建，在准确率和推理速度方面均优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像在计算机视觉中有广泛应用前景，但现有方法在处理密集光谱（数百个通道）时泛化能力不足。传统多尺度注意力方法仅对稀疏光谱表现出足够的泛化能力，无法满足现代高光谱传感器的需求。

Method: 提出HYDRA架构，包含教师模型和学生模型。教师模型封装潜在高光谱图像数据，学生模型学习从自然三通道彩色图像到教师编码域的映射。采用新颖的训练方法，结合知识蒸馏技术，实现高质量的光谱重建。

Result: 在所有评估指标上达到最先进性能，准确率提升18%，并且在各种通道深度下都比当前最先进模型具有更快的推理时间。

Conclusion: HYDRA方法有效解决了先前光谱重建模型的关键限制，为密集光谱重建提供了高质量的解决方案，在准确性和效率方面均优于现有方法。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [38] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出MSSR框架，通过构建最小充分信息集来解决视觉语言模型在空间推理中的两个关键瓶颈：2D预训练导致的3D理解不足和冗余3D信息引发的推理失败。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在空间推理任务中的两个根本瓶颈：由于2D中心预训练导致的3D理解能力不足，以及冗余3D信息引发的推理失败问题。

Method: 提出MSSR双智能体框架：感知智能体使用感知工具箱程序化查询3D场景提取充分信息，包括新颖的SOG模块用于稳健提取语言接地方向；推理智能体迭代精炼信息以追求最小化，在闭环中修剪冗余细节并请求缺失信息，直到构建出最小充分集。

Result: 在多个挑战性基准测试中，通过显式追求充分性和最小化，显著提高了准确性并实现了最先进的性能。

Conclusion: MSSR框架通过构建最小充分信息集有效解决了空间推理中的关键问题，不仅提升了性能，还产生了可解释的推理路径，为未来模型提供了高质量训练数据源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [39] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SDPA++的自监督去噪框架，专门针对光学相干断层扫描(OCT)图像。该方法仅使用含噪OCT图像，通过自融合和自监督去噪生成伪真实图像，然后训练集成去噪模型，有效提升图像清晰度。


<details>
  <summary>Details</summary>
Motivation: OCT图像分析对眼科疾病诊断至关重要，但获取配对的干净和含噪OCT图像数据集面临巨大挑战，主要由于固有散斑噪声和临床成像环境的实际限制。

Method: 提出SDPA++自监督去噪框架，仅使用含噪OCT图像，通过自融合和自监督去噪生成伪真实图像，然后采用基于块的策略训练集成去噪模型，有效增强图像清晰度。

Result: 在IEEE SPS视频和图像处理杯的真实数据集上验证，通过对比噪声比(CNR)、均方比(MSR)、纹理保持(TP)和边缘保持(EP)等指标显示性能提升。该数据集仅包含真实含噪OCT图像，无干净参考图像。

Conclusion: 该方法在仅有含噪图像的情况下有效提升OCT图像质量，具有改善临床实践中图像质量和诊断结果的潜力。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [40] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 本文提出了一种新的领域连接对比学习（DCCL）方法来解决领域泛化问题。通过增强跨领域的类内连通性，DCCL在五个标准DG基准测试中优于现有方法，且无需领域监督。


<details>
  <summary>Details</summary>
Motivation: 在领域泛化（DG）中，训练和测试样本之间的分布偏移会阻碍模型泛化性能。虽然对比学习（CL）理论上应该能改善DG，但实际应用CL反而会降低性能。研究发现这是由于DG设置中缺乏类内连通性导致的。

Method: 提出DCCL方法：在数据层面使用更激进的数据增强和跨领域正样本；在模型层面采用模型锚定技术利用预训练表示中的类内连通性，并结合生成变换损失来增强对未见测试领域的嵌入能力。

Result: 在五个标准DG基准测试上的广泛实验验证了DCCL优于现有最先进基线方法，即使在没有领域监督的情况下也能取得优异表现。

Conclusion: DCCL通过增强跨领域的类内连通性，成功解决了对比学习在领域泛化中的性能下降问题，提供了一种有效的领域泛化解决方案。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [41] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一个基于一致性模型的一步人体运动预测框架，通过单步生成实现高效推理，相比扩散模型减少推理步骤达两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的人体运动预测方法需要多步去噪过程，推理效率较低。本文旨在开发一个能够通过单步生成实现高效推理的框架，同时保持与扩散模型相当的预测精度。

Method: HumanCM采用基于Transformer的时空架构，结合时间嵌入来建模长程依赖关系并保持运动连贯性。核心创新在于学习噪声运动状态与干净运动状态之间的自一致映射，实现一步生成。

Result: 在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM在保持与最先进扩散模型相当或更优精度的同时，将推理步骤减少了两个数量级。

Conclusion: HumanCM证明了通过一致性模型实现高效单步人体运动预测的可行性，为实时运动预测应用提供了有前景的解决方案。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [42] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出了SCENECOT框架，首次将思维链推理成功应用于3D场景理解，通过解耦复杂推理任务为简单问题并构建视觉线索，在185K数据集上实现强性能和高推理-问答一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在接地问答方面仍存在困难，主要原因是缺乏对人类场景-对象接地推理机制的深入探索。本文旨在填补这一空白。

Method: 提出基于接地思维链的3D场景推理方法SCENECOT，将复杂任务分解为简单问题，利用多模态专家模块构建视觉线索。开发了首个大规模接地思维链推理数据集SCENECOT-185K。

Result: 在多个复杂3D场景推理基准测试中，新框架实现了强性能和高推理-问答一致性，是首个成功将思维链推理应用于3D场景理解的方法。

Conclusion: 该方法实现了逐步的人类式推理，显示出扩展到更广泛3D场景理解场景的潜力，为3D场景理解开辟了新方向。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [43] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM是一个隐式残差世界模型，专注于建模世界的当前状态和演化，通过仅预测变化残差来避免冗余建模静态背景，在nuScenes基准测试中实现了4D占用预测和轨迹规划的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统中的视觉中心世界模型存在效率问题，它们完全重建未来场景，在静态背景的冗余建模上耗费了大量能力。

Method: IR-WM首先从视觉观察中建立当前状态的鲁棒鸟瞰图表示，然后利用前一时刻的BEV特征作为强时序先验，仅预测基于自车动作和场景上下文的变化残差。采用对齐模块校准语义和动态错位，并探索不同的预测-规划耦合方案。

Result: 在nuScenes基准测试中，IR-WM在4D占用预测和轨迹规划方面都取得了最佳性能，世界模型生成的隐式未来状态显著提高了规划精度。

Conclusion: IR-WM通过残差建模方法有效解决了世界模型中的冗余重建问题，证明了隐式未来状态表示对规划任务的重要价值。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [44] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: 本文提出UKANFormer模型，在Allen Coral Atlas噪声标签下实现高精度珊瑚礁语义分割，挑战了数据质量直接限制模型性能的传统观念。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁分布产品（如Allen Coral Atlas）在空间精度和语义一致性方面存在局限，特别是在需要精细边界划分的区域。需要开发能够在噪声监督下实现高精度映射的方法。

Method: 基于UKAN架构，在解码器中引入全局-局部变换器（GL-Trans）模块，同时提取全局语义结构和局部边界细节，在噪声标签下实现高精度语义分割。

Result: UKANFormer在珊瑚类IoU达到67.00%，像素精度83.98%，优于传统基线方法，且预测结果在视觉和结构上比训练用的噪声标签更准确。

Conclusion: 架构设计可以缓解标签噪声问题，支持在非完美监督下进行可扩展的映射，为生态监测提供了可靠基础。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [45] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 本文提出了一个关于具身AI中世界模型的统一框架，包括问题形式化、学习目标和三轴分类法（功能、时序建模、空间表示），系统化整理了数据资源和评估指标，并对最先进模型进行了定量比较，指出了当前面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器，能够捕捉环境动态，支持感知、预测和决策制定，但目前缺乏统一的框架来系统化这一领域。

Method: 提出了一个三轴分类法：功能（决策耦合vs通用目的）、时序建模（序列模拟与推理vs全局差异预测）、空间表示（全局潜在向量、令牌特征序列、空间潜在网格、分解渲染表示）。系统化整理了机器人、自动驾驶和通用视频设置中的数据资源和评估指标。

Result: 对最先进模型进行了定量比较，识别出关键开放挑战：统一数据集的稀缺性、需要评估物理一致性而非像素保真度的指标、模型性能与实时控制计算效率之间的权衡、实现长期时序一致性同时减轻误差累积的核心建模难度。

Conclusion: 世界模型在具身AI中扮演着关键角色，本文提供了一个统一的框架来系统化这一领域，指出了当前面临的挑战，并为未来研究提供了方向。维护了一个精选的参考文献库。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [46] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 本文研究发现，离散的视觉自回归模型能够有效应用搜索策略来提升图像生成质量，使2B参数的自回归模型在文本到图像生成任务中超越12B参数的扩散模型。这表明模型架构（而不仅仅是规模）对于视觉生成中的推理时优化至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管搜索策略在大型语言模型中取得了显著成功，但在图像生成领域应用类似方法却遇到困难。现有研究尝试将搜索策略应用于连续扩散模型效果有限，简单的随机采样往往表现最佳。本文旨在探索离散自回归模型是否能够有效利用搜索策略来提升图像生成性能。

Method: 本研究采用离散自回归模型架构，利用beam search策略进行图像生成。通过离散token空间实现早期剪枝和计算重用，系统性地比较了不同搜索策略在视觉生成任务中的效果，并分析了验证器在速度与推理能力之间的权衡。

Result: 实验结果表明，beam search显著提升了文本到图像生成质量，2B参数的自回归模型在多个基准测试中超越了12B参数的扩散模型。系统消融研究证实这种优势源于离散token空间带来的早期剪枝和计算重用能力。

Conclusion: 研究证明模型架构（特别是离散自回归结构）对于推理时优化在视觉生成中具有关键作用，而不仅仅是模型规模。这一发现为未来视觉生成模型的设计提供了重要指导，强调了架构选择在实现高效推理时搜索中的重要性。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [47] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 本文提出了一个基于显著性的超分辨率伪影评估方法，通过构建包含1302个伪影样本的数据集，训练轻量级回归器生成空间显著热力图，改进了现有伪影检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式超分辨率模型能力的增强，其产生的伪影问题日益突出。不同伪影对人类观察者的视觉影响差异很大，因此需要基于显著性的评估方法而非简单的二元分类。

Method: 构建包含1302个伪影样本的数据集，每个样本配有众包显著性评分。基于该数据集训练轻量级回归器，生成空间显著性热力图来检测突出伪影。

Result: 训练的回归器在检测突出伪影方面优于现有方法，能够准确识别对人类观察者影响最大的伪影区域。

Conclusion: 提出了基于显著性的超分辨率伪影评估框架，通过数据集和回归器实现了对伪影视觉影响的量化评估，为超分辨率模型的优化提供了新方向。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [48] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 本文提出WaMaIR框架，通过全局多尺度小波变换卷积扩大感受野，结合Mamba通道感知模块捕获长程依赖关系，并使用多尺度纹理增强损失函数，显著提升了图像恢复中的纹理细节重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法在图像恢复中难以充分恢复精细纹理细节，主要受限于CNN结构的小感受野和缺乏通道特征建模能力。

Method: 提出WaMaIR框架：1）全局多尺度小波变换卷积扩大感受野并保留纹理特征；2）Mamba通道感知模块捕获特征通道间的长程依赖关系；3）多尺度纹理增强损失函数指导模型有效保留纹理结构。

Result: 大量实验证实WaMaIR优于现有最先进方法，在图像恢复质量和计算效率方面均表现出色。

Conclusion: WaMaIR框架通过扩大感受野、增强通道感知和优化损失函数，有效解决了图像恢复中纹理细节重建的挑战，实现了更好的恢复效果和计算性能。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [49] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 本文提出了Region in Context框架，通过多级语义对齐实现文本条件图像编辑，解决了现有方法在处理图像区域时缺乏全局上下文考虑的问题，能够生成更一致和协调的编辑结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑方法通常将图像区域孤立处理，仅依赖局部线索而不考虑各部分对整体视觉和语义构成的贡献，导致编辑不一致、过渡不自然或图像整体连贯性丧失。

Method: 提出双级引导机制：区域在全图像上下文中表示并与详细区域级描述对齐，同时整个图像与大型视觉语言模型生成的全面场景级描述匹配。这些描述作为显式语言参考，指导局部修改和全局结构。

Result: 实验表明该方法能产生更连贯且与指令对齐的结果，解决了现有方法中的不一致编辑和过渡不自然问题。

Conclusion: Region in Context框架通过多级语义对齐实现了更精确和协调的文本条件图像编辑，使区域能够在全局图像上下文中理解其角色，提升了编辑的一致性和质量。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [50] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE是一种新颖的6D物体姿态估计方法，通过结合3D高斯泼溅和束调整原理，解决了纹理缺失物体和光照变化下的姿态估计难题，在多个数据集上实现了精度提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D-3D特征对应的6D姿态估计方法在处理纹理缺失物体和变化光照条件时面临困难，需要一种更鲁棒的姿态估计方法。

Method: GS2POSE提出了一种基于束调整原理的姿态回归算法，利用李代数扩展3DGS能力，构建姿态可微渲染管道，通过迭代比较输入图像与渲染图像来优化姿态，同时更新3DGS模型中的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上，GS2POSE相比先前模型分别实现了1.4%、2.8%和2.5%的精度提升。

Conclusion: GS2POSE通过结合3DGS和束调整原理，有效解决了纹理缺失和光照变化下的6D姿态估计问题，在多个基准数据集上表现出优越性能。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [51] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的零样本视频理解框架，通过将预训练视觉语言模型的语义先验与经典机器学习算法相结合，将视频理解重新定义为高维语义特征空间中的自监督时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据进行任务特定训练，成本高且可扩展性有限。本文旨在探索如何将大规模视觉语言模型在静态图像上的零样本推理能力有效迁移到视频领域。

Method: 使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹，通过核时间分割算法将连续特征流分割为语义连贯的事件片段，然后进行无监督密度聚类识别重复场景，最后利用VLM生成文本描述形成多模态视频摘要。

Result: 该框架实现了零样本、自动化的视频内容结构分析，能够有效识别视频中的语义事件和重复场景，生成结构化的多模态视频摘要。

Conclusion: 该方法提供了一种有效、可解释且模型无关的零样本视频理解途径，无需端到端训练即可实现视频内容的自动化结构分析。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [52] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种新颖的即插即用方法，通过冻结MLLM并附加轻量级头部，利用注意力图中的空间线索提取关键点，实现像素级分割，同时完全保留模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要微调MLLM以产生与掩码解码器兼容的特定输出，这会改变模型的输出空间并损害其内在泛化能力，违背构建统一模型的目标。

Method: LENS在完全冻结的MLLM上附加轻量级可训练头部，通过精炼注意力图中的空间线索提取关键点，并将其描述为与掩码解码器直接兼容的点级特征。

Result: LENS实现了与基于重新训练方法相竞争甚至更优的分割性能，同时完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。

Conclusion: LENS的可附加设计为扩展MLLM建立了一个高效且强大的范式，为构建真正多才多艺的统一模型铺平了道路。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [53] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的二元道路分割方法，利用场景几何和时间一致性来区分道路与非道路区域，无需人工标注数据，在Cityscapes数据集上达到0.82 IoU。


<details>
  <summary>Details</summary>
Motivation: 传统道路分割方法依赖昂贵的人工标注数据集，限制了在自动驾驶等应用中的可扩展性。本文旨在开发一种无需人工标注的无监督方法，通过几何先验和时间一致性实现准确的道路分割。

Method: 基于几何先验生成弱标签：地平线以上像素标记为非道路，车辆前方预定义四边形标记为道路。通过跨帧跟踪局部特征点，利用互信息最大化惩罚不一致的标签分配，增强精度和时间稳定性。

Result: 在Cityscapes数据集上实现了0.82的IoU，表明该方法在简单设计下仍能达到高精度。

Conclusion: 结合几何约束和时间一致性为自动驾驶中的可扩展无监督道路分割提供了有效途径，证明了无监督方法在道路分割任务中的潜力。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [54] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种个性化图像滤镜(PIF)方法，能够从参考图像中学习摄影风格概念，并将其转移到内容图像上，同时保持内容图像的原始内容。该方法基于预训练的文本到图像扩散模型，通过文本反转技术优化摄影概念提示词。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么无法从参考图像中学习有意义的摄影概念，要么无法保持内容图像的内容。为了解决这些问题，需要一种能够理解照片如何从原始外观编辑而来的方法。

Method: 基于预训练的文本到图像扩散模型，PIF利用生成先验学习摄影概念的平均外观，并根据文本提示调整它们。通过文本反转技术优化摄影概念的提示词来学习参考图像的摄影风格。

Result: PIF在提取和转移各种摄影风格方面表现出色性能，能够有效保持内容图像的内容同时应用参考图像的风格。

Conclusion: PIF方法成功解决了摄影风格学习和转移中的关键问题，通过结合扩散模型和文本反转技术，实现了高质量的个性化图像风格化处理。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [55] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 本文构建了一个荔枝检测和成熟度分类的开源数据集，包含11,414张图像，涵盖不同品种、天气条件和成熟阶段，为基于视觉的荔枝采摘机器人开发提供高质量数据支持。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中具有一致性和全面注释的开源荔枝数据集，这限制了基于视觉的荔枝采摘机器人的发展。高质量数据对于开发此类机器人至关重要。

Method: 采集了多种荔枝品种在不同天气条件和时间段的彩色（RGB）图像，包含三个成熟度阶段。数据集包括878张原始RGB图像、8,780张增强RGB图像和1,756张深度图像，共11,414张图像，标注了9,658对荔枝检测和成熟度分类标签。

Result: 构建了包含11,414张图像的综合荔枝数据集，涵盖多种品种和生长条件。通过三人独立标注和第四人验证确保标注一致性。使用三种代表性深度学习模型进行实验验证数据集的有效性。

Conclusion: 该数据集为荔枝检测和成熟度分类研究提供了高质量的开源资源，将促进基于视觉的荔枝采摘机器人的发展，提高生产效率并减少对劳动力的依赖。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [56] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大型公共珊瑚礁图像数据集，包含约92.5万个属级硬珊瑚标注，映射到世界海洋物种名录。该数据集提供细粒度、全球尺度的分类标签，并提出了两种评估设置：源内基准和跨源基准，旨在推动领域泛化和细粒度珊瑚分类的发展。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化等人为压力导致珊瑚礁快速衰退，迫切需要可扩展的自动化监测方法。现有数据集在规模、地理分布或标签粒度方面存在限制，且不适合机器学习应用。

Method: ReefNet整合了76个CoralNet来源和红海Al Wajh站点的图像，提供专家验证的属级硬珊瑚标注。提出了两种评估设置：源内基准（划分每个来源的图像）和跨源基准（保留整个来源测试领域泛化）。分析了监督学习和零样本分类性能。

Result: 监督学习在源内性能表现良好，但在跨域情况下性能急剧下降。零样本模型整体性能较低，特别是对于稀有和视觉相似的属。这为领域泛化和细粒度珊瑚分类提供了具有挑战性的基准。

Conclusion: ReefNet数据集、基准代码和预训练模型的发布将推动鲁棒、领域自适应的全球珊瑚礁监测和保护技术的发展。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [57] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 该研究提出了一种名为AdaptMoist的域适应方法，利用木材碎片的纹理特征来预测水分含量，解决了不同来源木材碎片数据分布变化导致模型性能下降的问题。该方法将预测准确率提高了23%，平均准确率达到80%。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的直接方法（烘箱干燥）处理时间长且破坏样品，而现有的间接方法虽然快速但在木材碎片来源多样时准确性不足。源材料的变异性会改变数据分布，削弱数据驱动模型的性能，因此需要一种能够有效缓解源变异性影响的稳健方法。

Method: 本研究全面分析了从木材碎片图像中提取的五种不同纹理特征类型来预测水分含量。提出AdaptMoist域适应方法，利用纹理特征将知识从一个木材碎片数据源转移到另一个数据源，并基于调整互信息提出了模型保存标准。

Result: 结合所有五种纹理特征的特征集实现了95%的准确率，在预测水分含量方面始终优于单个纹理特征。AdaptMoist方法将跨域预测准确率提高了23%，平均准确率达到80%，而非适应模型的准确率为57%。

Conclusion: AdaptMoist作为一种稳健的解决方案，在跨域木材碎片水分含量估计方面表现出色，使其成为依赖木材碎片的行业的潜在解决方案。纹理特征组合和域适应方法有效解决了源变异性问题。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [58] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出了M2HVideo框架，用于将人台服装展示视频转换为身份可控、逼真的人类视频，解决了头部与身体运动不对齐和身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 人台服装展示成本低但缺乏真实感和表现细节，需要一种方法将人台视频转换为逼真的人类视频，同时保持身份一致性和服装细节。

Method: M2HVideo采用动态姿态感知头部编码器融合面部语义与身体姿态，使用镜像损失保护面部细节，设计分布感知适配器对齐身份和服装特征分布，增强时间一致性。

Result: 在UBC时尚数据集、自建ASOS数据集和现场采集的MannequinVideos数据集上，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有方法。

Conclusion: M2HVideo有效解决了人台到人类视频生成中的关键挑战，为在线时尚展示提供了更真实、身份可控的视频生成解决方案。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [59] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 本文提出2DGS-R方法，通过分层训练策略在保持几何精度的同时提升渲染质量，仅需1%额外存储和少量训练时间即可实现高质量渲染和精细几何结构。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)在渲染质量方面表现出色但难以准确表示表面，而2DGS在几何保真度方面有改进但渲染质量仍受限制，目前无法在单阶段训练中同时优化几何和渲染质量。

Method: 2DGS-R采用分层训练方法：首先用法向一致性正则化训练原始2D高斯；然后选择渲染质量不足的2D高斯进行原地克隆操作；最后冻结不透明度微调模型。

Result: 相比原始2DGS，2DGS-R仅需1%额外存储和少量训练时间，即可实现高质量渲染结果同时保持精细几何结构，在视觉保真度和几何重建精度方面均有提升。

Conclusion: 该方法有效平衡了效率与性能，证明了通过分层训练策略可以在保持几何精度的同时显著提升渲染质量，为神经场表示提供了新的解决方案。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [60] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 本文提出ArmFormer，一种轻量级基于Transformer的语义分割框架，通过集成CBAM注意力模块与MixVisionTransformer架构，在保持计算效率的同时实现像素级武器检测，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法仅提供粗略的边界框定位，缺乏细粒度分割能力；现有语义分割模型要么牺牲精度换取效率，要么计算资源需求过高，无法在边缘设备上部署。

Method: ArmFormer结合CBAM增强的编码器主干和注意力集成汉堡解码器，采用MixVisionTransformer架构，实现五类武器（手枪、步枪、刀具、左轮手枪、人体）的语义分割，专为资源受限的边缘设备设计。

Result: ArmFormer在保持实时推理速度（82.26 FPS）的同时，达到80.64% mIoU和89.13% mFscore的最优性能，仅需4.886G FLOPs和3.66M参数，比重量级模型计算量减少高达48倍。

Conclusion: ArmFormer是部署在便携安全摄像头、监控无人机和嵌入式AI加速器上的最优解决方案，在计算效率和分割精度之间实现了理想平衡。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [61] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出了一种基于配准诱导的旋转不变特征提取框架，将点云配准与基于内存的异常检测目标相结合，解决了现有方法在特征变换不一致性和局部几何细节捕捉能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于内存库的3D点云异常检测方法存在特征变换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些问题会导致不可靠的检测结果。作者认为点云配准不仅在对齐几何结构中起关键作用，还能引导特征提取朝向旋转不变和局部判别表示。

Method: 提出配准诱导的旋转不变特征提取框架，将点云配准与基于内存的异常检测目标集成。核心思想是将特征提取嵌入到配准学习过程中，联合优化对齐和表示学习。该方法利用两个任务都依赖建模局部几何结构和利用跨样本特征相似性的特点。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，该方法在有效性和泛化性方面持续优于现有方法。

Conclusion: 通过将特征提取嵌入到配准学习过程中，该框架能够获得既对旋转鲁棒又对异常检测高度有效的特征，解决了现有方法在旋转不变性和局部几何细节捕捉方面的局限性。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [62] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种分类引导的扩散模型Class-N-Diff，通过在扩散模型中集成分类器来同时生成和分类皮肤镜图像，解决了传统类别条件生成模型在医学图像生成中的准确性不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型在生成医学图像时难以准确表示特定医学类别，限制了其在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成过程。该模型通过分类器反馈优化生成过程，实现更好的类别控制，生成更真实多样的图像。

Result: 模型能够生成更真实和多样化的皮肤镜图像，同时分类器性能也得到提升，表明其在诊断任务中的有效性。

Conclusion: Class-N-Diff通过独特的分类器集成机制，显著提升了扩散模型在合成皮肤镜图像方面的质量和实用性，是一个强大的医学图像生成工具。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [63] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的指令图像编辑后训练框架，通过DiffusionNFT方法和MLLM奖励模型解决了监督微调过拟合问题，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决基于指令的图像编辑模型中监督微调导致的过拟合问题，提升模型在训练分布之外的泛化能力。

Method: 使用Diffusion Negative-aware Finetuning（DiffusionNFT）策略优化方法，结合多模态大语言模型作为训练免费的统一奖励模型，并设计了低方差组过滤机制来减少评分噪声。

Result: UniWorld-V2在ImgEdit和GEdit-Bench基准测试中分别获得4.49和7.83分，达到最先进水平。该框架对Qwen-Image-Edit和FLUX-Kontext等不同基础模型都能带来显著性能提升。

Conclusion: Edit-R1是一个模型无关的通用框架，能够有效提升指令图像编辑模型的性能，具有广泛的适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [64] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种基于地面摄像机的尾迹-航班归因方法，用于解决卫星观测中尾迹漂移变形导致的归因困难问题。该方法利用地面摄像机在尾迹形成初期捕获的高分辨率图像，结合航班监视和气象数据，建立了模块化的归因框架。


<details>
  <summary>Details</summary>
Motivation: 航空非CO2效应（特别是凝结尾迹）对气候影响显著，可能超过CO2排放的辐射强迫。现有物理模型需要验证和校准，但卫星观测因空间和时间分辨率限制难以准确归因尾迹到源航班。地面摄像机可在尾迹形成初期提供高分辨率观测数据。

Method: 基于地面可见摄像机尾迹序列数据集，开发模块化框架，将地面摄像机观测的尾迹与基于航班监视和气象数据计算的理论尾迹进行匹配。框架支持多种几何表示和距离度量，包含时间平滑处理，并采用灵活的概率分配策略。

Result: 建立了可靠的尾迹-航班归因基准框架，能够有效解决尾迹漂移变形导致的归因挑战，为未来研究提供了模块化工具。

Conclusion: 地面摄像机方法为尾迹-航班归因提供了可行的替代方案，建立的模块化框架为后续研究奠定了基础，有助于更准确评估航空非CO2效应的气候影响。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [65] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本文评估了四种基于Transformer的架构（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer）在热成像武器分割任务中的表现。SegFormer-b5取得了最佳分割性能（94.15% mIoU），而SegFormer-b0提供了最快的推理速度（98.32 FPS）。这些模型在低光照和遮挡的热成像环境中展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下至关重要，而传统CNN方法在捕捉长距离依赖关系和精细结构细节方面存在局限。虽然Vision Transformers在RGB分割任务中表现出色，但在热成像武器分割领域的潜力尚未充分探索。

Method: 在MMSegmentation框架中采用四种Transformer架构（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer）进行二元武器分割。使用包含9,711张真实世界监控视频图像的自定义热成像数据集，通过SAM2自动标注，并采用标准数据增强策略进行模型训练和公平比较。

Result: SegFormer-b5达到最高mIoU（94.15%）和像素精度（97.04%），SegFormer-b0提供最快推理速度（98.32 FPS）且mIoU保持竞争力（90.84%）。SegNeXt-mscans在85.12 FPS下达到92.24% mIoU，DeepLabV3+ R101-D8在29.86 FPS下达到92.76% mIoU。

Conclusion: Transformer架构在热成像武器分割任务中表现出色，提供了灵活的速度-精度权衡，适用于各种实时安全应用场景，在低光照和遮挡的热成像环境中具有强大的泛化能力。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [66] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 本文提出了Res-Bench基准测试，用于评估多模态大语言模型在不同输入分辨率下的性能稳定性，而不仅仅是语义性能。该基准包含14,400个样本，涵盖12个分辨率级别和6个核心能力维度，并引入了新的鲁棒性评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型评估主要关注语义性能，忽视了分辨率鲁棒性这一关键问题，即模型在不同输入分辨率下性能是否保持稳定。

Method: 设计了Res-Bench基准测试，包含14,400个样本，覆盖12个分辨率级别和6个核心能力维度。提出了新的评估框架，引入Spearman相关性、绝对/相对连续误差等鲁棒性指标，对领先MLLMs进行大规模评估，包括模型中心/任务中心鲁棒性分析、预处理策略研究和微调稳定性增强探索。

Result: 通过大规模评估揭示了多模态大语言模型在不同分辨率下的性能稳定性问题，分析了预处理策略（如填充和超分辨率）的影响，并探索了通过微调提升稳定性的方法。

Conclusion: Res-Bench为多模态大语言模型的分辨率鲁棒性评估提供了系统化框架，揭示了当前模型在分辨率变化下的性能稳定性问题，为未来模型优化提供了重要指导。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [67] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文对医学图像分析中的基础模型进行了系统性综述，分析了视觉专用和视觉语言基础模型的架构、训练策略和临床应用，并通过元分析揭示了数据集使用趋势和应用领域变化，同时讨论了领域适应、高效微调等挑战及联邦学习等解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像分析领域快速发展，但该领域仍缺乏对架构演进、训练范式和临床应用的系统性综合，需要统一的框架来理解这一新兴领域的发展脉络。

Method: 本文采用系统性综述方法，将研究分为视觉专用和视觉语言基础模型两类，分析其架构基础、训练策略和下游临床任务，并进行定量元分析以表征数据集使用和应用的时序趋势。

Result: 研究发现基础模型在医学图像分析中展现出强大的零样本和少样本性能，能够从多模态数据中学习通用表示，并通过少量微调适应各种临床应用。元分析揭示了该领域数据集使用和应用的明显趋势变化。

Conclusion: 基础模型有望加速医学图像分析的临床应用，但需要解决领域适应、计算约束和可解释性等挑战。未来研究方向应关注增强模型的鲁棒性、可解释性和临床集成能力。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [68] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出了Di-Bregman框架，将扩散模型蒸馏统一为基于Bregman散度的密度比匹配问题，为现有蒸馏方法提供了共同的理论基础，并在CIFAR-10和文本到图像生成任务上实现了优于反向KL蒸馏的单步FID性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和流模型虽然生成质量高，但多步采样计算成本昂贵。现有的蒸馏方法缺乏统一的理论基础，作者希望建立一个紧凑的理论框架来统一各种蒸馏目标。

Method: 提出Di-Bregman框架，将扩散蒸馏表述为基于Bregman散度的密度比匹配问题。该凸分析视角将多个现有目标通过共同的理论透镜连接起来，为扩散蒸馏提供了统一的理论基础。

Result: 在CIFAR-10和文本到图像生成任务上的实验表明，Di-Bregman相比反向KL蒸馏实现了改进的单步FID，同时保持了与教师模型相当的高视觉保真度。

Conclusion: Bregman密度比匹配是通往高效单步扩散生成的一个实用且理论基础扎实的路径，为扩散模型蒸馏提供了统一的理论框架。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [69] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 本文提出CARE框架，通过序列-图像对比对齐方法解决日常活动识别中序列和图像表示方法的局限性，在三个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有日常活动识别方法存在表示层面的限制：序列方法保持时间顺序但对噪声敏感且缺乏空间意识；图像方法捕获全局模式和空间相关性但压缩时间动态并扭曲传感器布局。简单的特征融合方法无法有效对齐两种表示视图。

Method: 提出CARE端到端框架，集成时间感知的序列编码和空间感知的图像表示，通过序列-图像对比对齐和交叉熵分类的联合目标函数，学习对齐且具有区分性的嵌入表示。

Result: 在三个CASAS数据集上达到最先进性能：Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%，并表现出对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过对比对齐有效结合了序列和图像表示的互补优势，为智能家居中可靠的日常活动识别提供了有前景的解决方案。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [70] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、在线执行的视频步骤定位方法BaGLM，利用大型多模态模型的零样本能力，通过贝叶斯滤波整合历史帧信息，在三个数据集上超越了需要训练的传统离线方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频步骤定位方法需要带标签的训练数据和离线处理完整视频，这限制了其在需要在线决策场景中的应用。本文旨在探索无需训练、在线执行的视频步骤定位方案。

Method: 利用大型多模态模型的零样本能力，仅基于有限帧预测步骤。提出BaGLM方法，通过贝叶斯滤波整合历史帧信息，使用大型语言模型提取的依赖矩阵和步骤进度估计来建模步骤转换。

Result: 在线无训练策略优于离线训练模型。BaGLM在三个数据集上超越了最先进的基于训练的离线方法，展示了优越性能。

Conclusion: BaGLM证明了利用大型多模态模型的零样本能力，结合贝叶斯滤波方法，可以在无需训练的情况下实现高性能的在线视频步骤定位，为实际应用提供了新思路。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [71] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文通过实证研究探讨不同视频特征对时序视频定位任务的影响，发现仅改变视频编码器就能显著影响模型性能，并揭示了不同特征之间的互补潜力。


<details>
  <summary>Details</summary>
Motivation: 当前时序视频定位研究过度集中于少数视频表示方法，可能导致架构过拟合。为应对这一问题，作者希望系统研究不同视频特征对经典架构的影响。

Method: 在三个基准数据集（Charades-STA、ActivityNet-Captions和YouCookII）上，使用基于CNN、时序推理和Transformer的视频编码器提取特征，并在经典架构上评估这些特征的影响。

Result: 研究结果显示，仅改变视频编码器就能导致模型性能的显著差异，同时揭示了特定特征使用带来的明显模式和错误，表明不同特征之间存在互补潜力。

Conclusion: 视频特征选择对时序视频定位任务具有重要影响，不同特征类型之间存在互补性，这为未来研究提供了新的方向。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [72] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 本文挑战了专业遥感基础模型优于通用视觉基础模型的观点，通过设计基准测试和训练改进的自监督模型，发现在ViT-B规模下，专业遥感模型并未带来一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证专业遥感基础模型是否真的比通用视觉基础模型更有用，特别是在小规模场景下，考虑到遥感图像的独特特征和特定应用需求。

Method: 设计了一个简单基准测试来衡量遥感模型在低分辨率图像上的泛化能力，同时训练了基于iBOT的自监督视觉编码器，在MillionAID数据集上进行了遥感特定的改进。

Result: 结果显示，在ViT-B规模下，这些预训练的专业遥感模型相比通用基线模型并未带来一致的性能改进。

Conclusion: 结论表明，至少在较小规模下，专门为遥感应用设计的基础模型并不比通用视觉基础模型更有优势。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [73] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种利用多模态大语言模型进行细粒度视频时序定位的方法，通过两阶段处理将语言查询转换为包含缺失细节的丰富句子，然后使用轻量级解码器进行精确定位，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频时序定位方法在处理自然语言查询时存在信息不足和幻觉问题，需要一种能够充分利用多模态大语言模型能力来增强查询表达并准确定位的方法。

Method: 采用两阶段处理：第一阶段将语言查询转换为包含缺失细节的丰富句子；第二阶段使用轻量级解码器基于上下文化的丰富查询表示预测准确边界。通过多实例学习目标动态选择最佳查询版本，减少噪声和幻觉影响。

Result: 在视频时序定位和段落定位设置的各种基准测试中达到最先进结果，显著优于所有先前提出的基于LLM的时序定位方法，在零样本评估场景中保持明显优势。

Conclusion: ED-VTG通过有效利用多模态大语言模型的能力，在视频时序定位任务中实现了卓越性能，特别是在零样本场景下表现出色，为细粒度视频理解提供了有效解决方案。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [74] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 本文提出W2R2框架解决多模态3D定位中的2D语义偏见问题，通过解耦表示学习和针对性捷径抑制，将2D特征作为语义信标，3D特征作为空间锚点，无需修改推理架构即可实现精确3D定位。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D定位模型存在严重的"2D语义偏见"，过度依赖2D图像特征进行粗略定位，而忽视3D几何输入，导致融合性能不佳。

Method: W2R2框架通过解耦表示学习，将2D特征作为"What"语义识别信标，3D特征作为"Where"空间定位锚点。采用双目标损失函数：对齐损失使用适应交叉熵监督融合预测，伪标签损失通过基于边界的机制惩罚过度有效的2D主导伪输出。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2在定位准确性和鲁棒性方面取得显著提升，特别是在杂乱户外场景中表现突出。

Conclusion: W2R2框架有效解决了多模态3D定位中的2D语义偏见问题，通过解耦表示学习和针对性捷径抑制，在不修改推理架构的情况下显著提升了3D定位性能。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [75] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种使用条件StyleGAN2-ADA和StyleGAN3生成高分辨率合成活体指纹，并通过CycleGANs转换为逼真欺骗指纹的新方法，解决了生物特征数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 传统指纹数据集收集耗时昂贵且需要严格隐私保护，研究人员探索使用合成指纹数据来解决这些问题。

Method: 使用条件StyleGAN2-ADA和StyleGAN3生成特定手指身份的高分辨率合成活体指纹，然后通过CycleGANs将其转换为模拟多种攻击材料（如EcoFlex、Play-Doh）的逼真欺骗指纹。

Result: StyleGAN3模型FID低至5，生成指纹在0.01% FAR下达到99.47% TAR；StyleGAN2-ADA在相同条件下达到98.67% TAR。创建了两个合成数据集（DB2和DB3），各包含1,500个指纹图像，匹配实验证实了强大的隐私保护特性。

Conclusion: 该方法成功生成了高质量的合成指纹数据集，在保持强隐私保护的同时，为开发稳健的欺骗检测系统提供了重要数据支持。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [76] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一个临床医生在环的深度学习管道，用于肺癌CT图像分割和预后预测。VNet模型在分割性能、放射组学稳定性和预测准确性方面表现最佳，半监督学习始终优于监督学习。放射科医生更倾向于使用AI生成的初始掩模进行细化而非替换。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主要原因，CT成像是筛查、预后和治疗的核心。手动分割存在变异性大且耗时的问题，而深度学习虽然提供自动化但面临临床采用障碍。本研究旨在开发一个临床医生在环的深度学习管道，以提高可重复性、预后准确性和临床信任度。

Method: 使用来自12个公共数据集的999名患者的多中心CT数据，采用5种深度学习模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D）进行分割，并与专家轮廓进行基准测试。通过497个放射组学特征评估分割可重复性，比较监督学习和半监督学习在38种降维策略和24种分类器下的预后建模性能。

Result: VNet获得最佳性能（Dice = 0.83，IoU = 0.71），放射组学稳定性最高（平均相关性 = 0.76，ICC = 0.65），在半监督学习下预测准确率最高（准确率 = 0.88，F1 = 0.83）。半监督学习在所有模型中始终优于监督学习。放射科医生更青睐VNet的肿瘤周围表示和平滑边界，偏好使用AI生成的初始掩模进行细化。

Conclusion: 将VNet与半监督学习相结合能够产生准确、可重复且临床可信的基于CT的肺癌预后预测，为以医生为中心的AI转化提供了可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [77] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种广义选择方法，用于改进行人重识别中的类别表示选择，超越了传统的类中心方法，在准确率和平均精度均值之间取得平衡，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前行人重识别研究主要关注特征提取和损失函数改进，而类别表示选择这一重要方向研究不足。虽然已有工作尝试在训练时使用图库图像类别的中心点，但在检索阶段探索替代表示的研究很少。

Method: 提出广义选择方法，不限于类中心表示，可调整每个类别的实际表示数量以满足特定应用需求。该方法在多种重识别嵌入上应用，通过选择更优的类别表示来提升性能。

Result: 该方法在所有测试的重识别嵌入上都显著超越了现有技术水平，在准确率和平均精度均值方面均取得改进。

Conclusion: 广义类别表示选择方法为行人重识别提供了有效的性能提升途径，证明了优化类别表示选择的重要性，并展示了该方法在不同嵌入模型上的普适有效性。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [78] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 本文提出V-Reason方法，通过熵信号分析高质量模型的微观探索和利用行为，在推理时无需RL或监督微调即可优化LMM的推理过程，显著提升视频推理性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前视频推理使用大型多模态模型依赖昂贵的强化学习和冗长的思维链，导致训练和推理计算成本高，且推理过程的控制机制有限。

Method: 利用模型输出熵作为信号，发现高质量模型经历微观探索和利用过程。提出V-Reason方法，在推理时通过小型可训练控制器使用基于熵的目标优化LMM的值缓存，无需任何数据集监督或RL。

Result: 在多个视频推理数据集上显著优于基础指令调优模型，与RL训练模型的准确率差距缩小到0.6%以内，同时输出token减少58.6%，效率大幅提升。

Conclusion: 基于熵的推理时优化方法能有效改善模型的微观探索和利用行为，在无需额外训练的情况下显著提升视频推理性能，同时大幅降低计算开销。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [79] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 本文研究了通用视觉基础模型与专用模型之间的权衡，通过比较Hiera通用编码器和SAM2分割专用模型的特征适应性，量化了专业化的信息论成本。研究发现SAM2在空间相关任务上表现出色，但在概念距离较远的任务上表现不佳，揭示了专业化带来的语义信息损失。


<details>
  <summary>Details</summary>
Motivation: 研究通用基础视觉模型与专用模型之间的权衡对于高效特征编码设计至关重要，但目前尚未完全理解这种权衡关系。

Method: 使用轻量级可训练颈部来探测冻结特征的适应性，比较Hiera通用编码器和SAM2分割专用模型的特征多样性，通过信息论方法量化专业化成本，并进行跨颈部分析。

Result: SAM2在深度估计等空间相关任务上表现出色，但在姿态估计和图像描述等概念距离较远的任务上表现不如通用模型Hiera，显示出专业化带来的语义信息损失。跨颈部分析显示每个适应层级都会产生进一步的表示瓶颈。

Conclusion: 研究揭示了特征通用性方面的权衡，为设计针对不同下游应用的高效特征编码和适应策略提供了量化基础。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [80] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProDAT的密度感知渐进式点云编码方法，通过密度信息引导潜在特征和坐标的自适应解码，实现了单一模型支持多比特率下的渐进式解码，在编码效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 三维点云在自动驾驶、增强现实等应用中需要实时处理和低延迟，但大数据量和带宽限制阻碍了在资源受限环境中的高质量服务部署。现有学习型点云几何编码方法的固定潜在表示不支持渐进式解码，需要解决这一技术瓶颈。

Method: 提出ProDAT密度感知尾丢弃机制，利用密度信息作为指导信号，根据特征重要性自适应解码潜在特征和坐标，通过单一模型实现多比特率下的渐进式解码。

Result: 在基准数据集上的实验结果表明，ProDAT不仅实现了渐进式编码，而且在编码效率上优于最先进的学习型编码技术，在SemanticKITTI上PSNR-D2的BD-rate提升超过28.6%，在ShapeNet上提升超过18.15%。

Conclusion: ProDAT方法成功解决了点云渐进式编码的技术难题，通过密度感知机制实现了高效的渐进式解码，为资源受限环境下的点云应用提供了可行的解决方案。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [81] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 本文提出FMCAF架构，通过频域滤波和跨注意力融合模块，在RGB和红外图像的多模态目标检测中实现更好的特征融合，相比传统拼接方法在VEDAI数据集上提升13.9% mAP@50，在LLVIP数据集上提升1.1%。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测通过融合多种传感器模态的互补信息来提高在挑战性条件下的鲁棒性，但现有方法往往针对特定数据集优化，缺乏通用性。

Method: FMCAF包含频域滤波块(Freq-Filter)抑制冗余频谱特征，以及基于跨注意力的融合模块(MCAF)改善模态间特征共享。该架构不依赖数据集特定调优，旨在实现通用性。

Result: 在LLVIP(低光行人检测)和VEDAI(航空车辆检测)数据集上，FMCAF优于传统拼接融合方法，VEDAI上mAP@50提升13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF作为灵活的多模态融合基础架构，在多种检测挑战中展现出良好性能，为未来稳健的多模态检测流程提供了潜力。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [82] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane方法通过引入平面先验，在保持渲染质量的同时显著提高了高斯溅射重建场景中平面区域的几何精度和网格拓扑结构，并支持平面对象的解耦和灵活操作。


<details>
  <summary>Details</summary>
Motivation: 解决现有高斯溅射方法在重建平面区域时平滑度和精度不足的问题，为下游场景编辑和物理仿真提供结构化参数化表示。

Method: 利用现成分割和法向预测模型提取平面先验，建立结构化平面高斯坐标表示，通过动态高斯重分类器自适应处理高梯度平面高斯，并优化网格布局减少顶点和面数。

Result: 在保持渲染质量的同时，显著提高了各种基线方法提取网格的几何精度，减少了顶点和面数，改善了拓扑结构。

Conclusion: GSPlane通过引入平面先验有效解决了高斯溅射在平面重建中的几何精度问题，为场景编辑和物理仿真提供了高质量的平面表示。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [83] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种使用LED环境照明为消费级相机生成视觉不可见水印的方法。该方法通过优化LED光源的光谱特性，使其对人眼几乎不可见但对相机高度可检测，实现了在标准帧率下提取水印的能力。


<details>
  <summary>Details</summary>
Motivation: 传统可见光通信通常需要高频调制，限制了在标准视频帧率下的应用。本文旨在开发一种既保护隐私又验证内容的水印技术，同时确保对人眼不可见。

Method: 采用光谱调制而非强度调制，联合考虑人眼视觉系统敏感性、消费相机传感器光谱敏感性和窄带LED生成宽带白光的能力。优化LED光谱配置文件，使其在D65照明下对人眼不可见但对相机可检测。

Result: 该方法能够在标准低帧率（30-60 fps）下提取水印，在10秒视频片段中嵌入128位信息，足以支持隐私保护和内容验证所需的元数据。

Conclusion: 提出的LED环境照明水印方法实现了对人眼不可见但对相机可检测的水印嵌入，为隐私保护和内容验证提供了一种实用的解决方案。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [84] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 本文提出了GOOD框架，通过直接引导扩散采样轨迹来生成分布外样本，解决了现有方法中语义不稳定和多样性不足的问题。该框架采用双级引导机制，结合图像级和特征级指导，实现了更可控和多样化的OOD样本生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的OOD样本生成方法通常依赖于扰动文本条件嵌入，导致语义不稳定和多样性不足，限制了在真实OOD场景中的泛化能力。

Method: GOOD框架使用现成的分布内分类器直接引导扩散采样轨迹。包含双级引导：图像级引导基于对数分割梯度减少输入似然，驱动样本到像素空间低密度区域；特征级引导基于k-NN距离在分类器潜在空间中促进特征稀疏区域采样。

Result: 通过GOOD生成的样本训练可以显著提升OOD检测性能。全面的定量和定性分析验证了GOOD的有效性。

Conclusion: GOOD框架通过双级引导机制实现了更可控和多样化的OOD样本生成，提出的统一OOD评分自适应结合图像和特征差异，增强了检测鲁棒性。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [85] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D是一个统一的框架，通过运动学感知扩散模型从单视角输入重建铰接物体形状和估计姿态。该方法结合了运动学感知VAE编码、条件扩散模型和迭代优化模块，能够准确重建多样化的铰接物体实例并估计其运动学属性。


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如笔记本电脑和抽屉）由于多部件几何结构和变化的关节配置，在3D重建和姿态估计方面面临重大挑战。这些物体的结构多样性在不同状态下引入复杂性，需要专门的方法来处理其独特的运动学特性。

Method: 提出KineDiff3D框架：首先通过运动学感知VAE将完整几何（SDFs）、关节角度和部件分割编码到结构化潜在空间；然后使用两个条件扩散模型分别回归全局姿态（SE(3))和关节参数，以及从部分观测生成运动学感知潜在代码；最后通过迭代优化模块双向优化重建精度和运动学参数。

Result: 在合成、半合成和真实世界数据集上的实验结果表明，该方法能够准确重建铰接物体并估计其运动学属性，验证了方法的有效性。

Conclusion: KineDiff3D提供了一个统一的解决方案，成功解决了铰接物体的3D重建和姿态估计挑战，通过运动学感知的扩散模型实现了准确的形状重建和运动学参数估计。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [86] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 本文研究发现人脸识别系统中的预处理环节（特别是人脸检测模型）对对抗攻击的迁移性有显著影响，可降低攻击成功率高达78%。作者提出了一种预处理不变的方法，通过输入变换将攻击迁移性提升达27%。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统通常包含预处理环节，但在黑盒攻击场景中这一环节常被忽视。本文旨在研究不同预处理技术对对抗攻击迁移性的影响，并探索提升攻击泛化能力的方法。

Method: 研究多种现成的最先进对抗攻击方法在不同预处理技术下的迁移性，分析人脸检测模型和降采样插值方法的影响。提出基于输入变换的预处理不变方法以提升攻击迁移性。

Result: 人脸检测模型的选择可使攻击成功率降低高达78%，而降采样插值方法影响相对较小。提出的预处理不变方法可将攻击迁移性提升达27%。

Conclusion: 预处理在人脸识别系统中具有重要作用，考虑预处理环节对于提升面部对抗样本的对抗泛化能力至关重要。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [87] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的分层采样策略GtR，通过将图像生成分解为结构生成和细节重建两个阶段，在保持生成质量的同时实现了3.72倍的加速。该方法基于图像细节比显著区域包含更多语义信息的观察，进一步提出了频率加权令牌选择策略。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码自回归模型虽然具有并行生成能力，但由于需要单步建模空间相关的视觉令牌，其加速潜力仍然受限。为了解决这一限制，作者旨在开发一种能够显著加速生成过程同时保持生成质量的方法。

Method: GtR方法将生成过程分解为两个阶段：结构生成阶段建立全局语义框架，细节重建阶段高效完成剩余令牌。基于图像细节比显著区域包含更多语义信息的观察，提出了频率加权令牌选择策略，为图像细节区域的令牌分配更多计算资源。

Result: 在ImageNet类条件生成和文本到图像生成任务上的广泛实验表明，该方法在MAR-H模型上实现了3.72倍的加速，同时保持了可比的生成质量（FID: 1.59, IS: 304.4 vs. 原始1.59, 299.1），显著优于现有的各种加速方法。

Conclusion: GtR提供了一种有效的训练免费分层采样策略，通过智能地分配计算资源到图像的不同区域，在保持生成质量的同时显著加速了掩码自回归模型的生成过程，为高效视觉生成提供了新的解决方案。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [88] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文通过系统分析揭示了多模态大语言模型的三阶段跨模态交互过程，并基于此提出了无需训练的剪枝框架VisiPruner，能够减少高达99%的视觉相关注意力计算和53.9%的FLOPs，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中表现出色，但由于注意力计算随多模态token数量呈二次增长，存在显著的计算开销问题。现有token剪枝方法缺乏对MLLMs如何处理和融合多模态信息的基本理解。

Method: 基于对MLLMs跨模态交互的三阶段分析（浅层识别任务意图、中层跨模态融合、深层语言精炼），提出VisiPruner训练免费剪枝框架，通过识别关键视觉token来减少计算开销。

Result: VisiPruner在LLaVA-v1.5 7B上减少了高达99%的视觉相关注意力计算和53.9%的FLOPs，显著优于现有token剪枝方法，并在多种MLLMs上具有良好的泛化性。

Conclusion: 研究揭示了MLLMs的内在层间处理动态，为训练高效MLLMs提供了可操作的指导原则，通过使模型架构与其内在处理动态对齐来提升效率。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [89] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文首次在浮游生物识别领域进行了大规模、系统的分布外数据检测方法评估，基于DYB-PlanktonNet数据集构建了多种分布偏移场景的基准测试，发现ViM方法在远分布外场景中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物识别模型在真实部署中面临分布偏移挑战，由于浮游生物形态复杂、物种多样性大且不断发现新物种，导致推理时出现不可预测错误。该领域缺乏最新计算机视觉进展的系统整合和大规模评估基准。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列分布外基准测试，模拟各种分布偏移场景，系统评估了22种分布外检测方法，重点关注不同分布偏移情况下的性能表现。

Result: 大量实验结果表明，ViM方法在构建的基准测试中显著优于其他方法，特别是在远分布外场景中，关键指标有显著提升。

Conclusion: 这项综合评估不仅为自动浮游生物识别中的算法选择提供了可靠参考，也为未来浮游生物分布外检测研究奠定了坚实基础，是该领域的首次大规模系统评估。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [90] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了一种联合学习详细头部化身和手-脸互动引起的非刚性变形的新框架，解决了现有方法忽略自然手脸交互的问题。通过深度顺序损失和接触正则化确保正确空间关系，并学习手诱导面部变形的PCA基，结合物理启发的接触损失减少穿插伪影。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了传达认知状态的自然手脸交互（如手托下巴、手指轻触脸颊等），这些交互对于真实的3D头部化身至关重要。

Method: 提出联合学习框架，结合深度顺序损失和接触正则化进行姿态跟踪，学习手诱导面部变形的PCA基以减少参数空间，并引入物理启发的接触损失减少穿插伪影。

Result: 在iPhone拍摄的RGB(D)视频和合成数据集上评估，相比SOTA表面重建方法，能够捕捉更好的外观和更准确的面部变形几何。

Conclusion: 该方法能够有效建模手脸交互引起的面部变形，生成更真实、物理合理的3D头部化身，为远程呈现、游戏和VR应用提供更自然的交互体验。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [91] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出HIDISC，一种双曲表示学习框架，用于解决领域泛化与广义类别发现问题。该方法通过GPT引导的扩散增强生成最小但多样的领域变化，并使用切线空间插值合成伪新样本，避免了现有方法的高计算成本和误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法假设训练时能同时访问标记和未标记数据且来自同一领域，限制了在开放世界场景中的适用性。领域泛化GCD要求模型在训练时不访问目标域数据的情况下泛化到包含新类别的未见领域。

Method: HIDISC使用双曲表示学习框架，通过GPT引导的扩散增强生成领域变化，引入切线CutMix进行曲率感知插值合成伪新样本，结合惩罚Busemann对齐、混合双曲对比正则化和自适应离群排斥的统一损失函数，以及可学习的曲率参数。

Result: HIDISC在PACS、Office-Home和DomainNet数据集上取得了最先进的结果，持续优于现有的欧几里得和双曲(DG)-GCD基线方法。

Conclusion: HIDISC通过双曲几何表示学习和领域增强技术，有效解决了领域泛化GCD问题，在多个基准数据集上表现出优越性能，为开放世界场景中的类别发现提供了高效解决方案。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [92] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的零样本方法，通过引入提示感知视角来重构视觉令牌剪枝问题，在保持最小精度损失的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型能力的提升，处理大规模输入会产生显著的视觉令牌冗余，导致推理成本过高。现有方法通常忽略文本提示的指导，无法优先考虑任务相关性。

Method: 采用分层方法：首先选择任务相关的核心视觉令牌集，然后补充多样性令牌以保留更广泛的上下文信息。通过平衡任务相关性和信息多样性来优化令牌剪枝。

Result: 在多个模型和基准测试中，该方法在剪枝高达90%令牌的情况下，性能达到或超过最先进水平，同时显著减少GPU内存占用和推理延迟。

Conclusion: 提出的提示感知视觉令牌剪枝方法在保持性能的同时有效降低了计算成本，为视觉语言模型的高效推理提供了实用解决方案。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [93] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 本研究开发了一种基于Segment Anything Model (SAM)的AI方法，用于精确监测孟加拉国河流侵蚀造成的土地损失。通过构建首个包含消失定居点标注的数据集，并微调SAM模型，实现了86.30%的IoU和92.60%的Dice分数，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国的主要河流既是商业和生计的动脉，也是无情的破坏者，每年吞噬整个村庄和大片农田，导致数千家庭流离失所。传统的人工分析难以有效跟踪这种缓慢发展的灾难。

Method: 首先使用简单的颜色通道分析进行粗略的土地和水域分割，然后微调SAM的掩码解码器以识别河岸侵蚀的细微特征。构建了2003-2025年孟加拉国脆弱地区的历史Google Earth影像数据集，包含消失定居点的手动标注。

Result: 最终模型在河流侵蚀检测方面表现出色，平均IoU达到86.30%，Dice分数达到92.60%，显著优于传统方法和现成的深度学习模型。

Conclusion: 该研究提供了三个关键贡献：首个孟加拉国河流侵蚀消失定居点的标注数据集、专门针对此关键任务微调的AI模型，以及通过视觉证据量化土地损失的方法，为政策制定者和灾害管理机构提供了强大的监测工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [94] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 本文针对FPS游戏VALORANT，提出了一种基于小地图视频分析的回合结果预测模型。通过结合TimeSformer视频识别模型和从游戏小地图中提取的战术特征（如角色位置信息），显著提升了预测准确率。实验结果显示，在添加战术事件标签的数据集上训练的模型达到了约81%的准确率，特别是在回合中后期表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前电竞比赛结果预测研究大多基于比赛日志数据和统计信息，而VALORANT作为需要复杂策略的FPS游戏，其小地图信息包含丰富的战术特征。本文旨在通过分析比赛录像中的小地图信息来构建更准确的回合结果预测模型。

Method: 基于TimeSformer视频识别模型，从VALORANT比赛录像的小地图中提取详细的战术特征，包括角色位置信息和游戏内事件。通过数据增强技术添加战术事件标签，构建预测模型来识别回合结果。

Result: 在添加战术事件标签的数据集上训练的模型达到了约81%的预测准确率，特别是在回合中后期阶段表现显著优于仅使用小地图信息本身训练的模型。

Conclusion: 利用比赛录像中的战术特征对于VALORANT回合结果预测非常有效，该方法为电竞分析提供了新的视角，超越了传统的基于统计数据的预测方法。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [95] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 本文提出Glyph框架，通过将长文本渲染为图像并用视觉语言模型处理，实现3-4倍的token压缩，同时保持与主流LLM相当的准确率，显著提升长上下文处理效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM对长上下文建模需求的增长，扩展到百万token级别会带来高昂的计算和内存成本，限制了长上下文LLM的实际应用。

Method: 提出Glyph框架，将长文本渲染为图像，使用视觉语言模型处理。设计了LLM驱动的遗传搜索来寻找最佳视觉渲染配置，平衡准确性和压缩率。

Result: 在多个长上下文基准测试中，实现了3-4倍token压缩，准确率与Qwen3-8B相当，预填充和解码速度提升约4倍，SFT训练速度提升约2倍。128K上下文VLM可扩展到处理1M token任务。

Conclusion: 视觉上下文扩展为长文本处理提供了高效解决方案，在保持性能的同时显著降低计算成本，并为多模态任务带来额外优势。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [96] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 本文提出了EndoCIL框架，专门针对内窥镜图像的类增量学习问题。该框架通过三个关键组件：基于最大均值差异的重放策略、先验正则化类平衡损失和全连接梯度校准，有效缓解了灾难性遗忘和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析在真实临床应用中需要持续适应新数据，同时保持对已学习类别的性能。现有重放式类增量学习方法由于内窥镜图像中严重的领域差异和类别不平衡问题，无法有效缓解灾难性遗忘。

Method: EndoCIL框架包含三个核心组件：1）MDBR使用分布对齐的贪心策略选择多样化和代表性样本；2）PRCBL通过整合先验类别分布和平衡权重来缓解类间和类内不平衡；3）CFG调整分类器梯度以减少对新类别的偏置。

Result: 在四个公共内窥镜数据集上的广泛实验表明，EndoCIL在不同缓冲区大小和评估指标下普遍优于最先进的类增量学习方法。该框架有效平衡了终身内窥镜诊断中的稳定性和可塑性。

Conclusion: EndoCIL框架在终身内窥镜诊断中有效平衡了稳定性和可塑性，显示出良好的临床可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [97] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINOv2的面部欺骗攻击检测方法，通过使用带寄存器的DINOv2模型提取可泛化特征并抑制注意力机制中的扰动，能够有效区分真实和欺骗面部图像的细微差异。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统虽然对头部姿态、光照和图像模糊等变化具有鲁棒性，但恶意攻击者可能通过展示注册用户的照片来绕过认证过程。因此需要在面部识别之前检测这种欺骗攻击。

Method: 采用带寄存器的DINOv2模型提取通用特征，抑制注意力机制中的扰动，使模型能够聚焦于关键细微特征。该方法在ICCV2025第六届面部反欺骗研讨会提供的统一物理-数字攻击检测数据集和SiW数据集上进行验证。

Result: 实验证明了所提方法的有效性，能够准确检测面部欺骗攻击。

Conclusion: 基于DINOv2的面部欺骗攻击检测方法通过关注细微特征差异，能够有效区分真实和欺骗面部图像，为面部识别系统提供了可靠的安全保障。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [98] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 本文提出了一种公平性感知的深度伪造检测框架，通过整合时序特征学习和人口统计感知的数据增强，解决了现有方法存在的偏见、缺乏透明度和无法捕捉时序信息的问题。该方法在多个数据集上实现了公平性和准确性的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度且无法有效捕捉时序信息，导致在不同人口统计群体中产生有偏决策和不可靠结果。

Method: 提出公平性感知深度伪造检测框架，结合时序特征学习和人口统计感知数据增强。使用基于序列的聚类进行深度伪造视频的时序建模，概念提取提高检测可靠性。引入人口统计感知数据增强方法平衡代表性不足群体，应用频域变换保留深度伪造伪影。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上使用Xception、ResNet等最先进架构进行广泛实验，证明该方法在公平性和准确性之间取得了最佳权衡。

Conclusion: 所提出的框架有效提升了深度伪造检测的公平性和可解释性，同时保持了高检测精度，为解决现有方法的偏见问题提供了有效解决方案。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [99] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: 本文介绍了FineVision，一个经过精心收集、整理和统一的2400万样本视觉语言数据集，是目前最大的开放资源。通过半自动化流程整合200多个数据源，并进行严格去重和去污染处理，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型发展受到碎片化、不一致和受污染的公共数据集的阻碍，需要构建一个高质量、统一的大规模数据集来推动研究进展。

Method: 采用半自动化、人机协同的流水线：自动化处理批量数据摄入和模式映射，人工审核员检查映射准确性、验证标注忠实度、格式多样性和安全性，发现问题时进行针对性修复和重新运行。同时进行严格的内部和跨源去重，并对66个公共基准进行去污染处理。

Result: 在FineVision上训练的模型在广泛评估套件中持续优于现有开放混合数据集训练的模型，证明了规模、数据卫生以及人机平衡监督的优势。

Conclusion: FineVision数据集和整理工具的发布将加速以数据为中心的视觉语言模型研究，强调了高质量数据整理对模型性能提升的重要性。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [100] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 本文提出Plug-and-Forecast (PnF)方法，通过多模态大语言模型增强现有运动预测模型，利用自然语言描述复杂场景，实现零样本推理能力，显著提升运动预测性能且无需微调。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在标准条件下表现可靠，但难以经济高效地泛化到多样化现实场景。需要一种能够快速适应目标行为的方法来处理复杂场景。

Method: PnF方法设计提示从MLLMs提取结构化场景理解，并将这些信息蒸馏为可学习嵌入来增强现有行为预测模型。利用MLLMs的零样本推理能力，无需微调即可提升性能。

Result: 在Waymo Open Motion Dataset和nuScenes Dataset上验证，使用两个最先进的运动预测模型均获得一致的性能提升。

Conclusion: PnF方法通过自然语言描述复杂场景，有效提升了运动预测模型的泛化能力，为自动驾驶系统提供了实用的增强方案。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [101] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv11目标检测算法的新型手术照明系统，通过自动识别蓝色标记并引导LED光源，解决传统手动调节手术灯导致的疲劳、颈部劳损和照明不一致问题。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统依赖手动调节，导致外科医生疲劳、颈部劳损，以及由于漂移和阴影造成的照明不一致问题，影响手术精度和安全性。

Method: 使用YOLOv11目标检测算法识别手术部位上方的蓝色标记，通过两个配备倾斜-平移支架的伺服电机将高功率LED光源引导至识别位置，实现自动化照明调节。

Result: YOLO模型在模拟手术场景的验证集上达到96.7%的mAP@50精度，系统能够自动跟踪标记位置，提供稳定照明。

Conclusion: 这种基于机器视觉的自动化照明解决方案能够减轻外科医生的身体负担，提高照明一致性，从而支持改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [102] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习中的反直觉现象：更长的训练时间反而会损害密集预测任务的性能，称为自监督密集退化(SDD)。作者提出了密集表示结构估计器(DSE)来无监督评估密集任务性能，并基于此开发了模型选择策略和正则化方法，有效缓解了密集退化问题。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习中存在反直觉现象——训练时间过长会损害密集预测任务性能，而目前缺乏有效的无监督方法来评估密集任务性能，这阻碍了模型选择和训练优化。

Method: 提出了密集表示结构估计器(DSE)，包含类相关性度量和有效维度度量，用于无监督评估密集任务性能。基于DSE开发了模型选择策略和正则化方法，以缓解自监督密集退化问题。

Result: 在16种自监督方法和4个基准测试上的实验表明，模型选择策略平均提高mIoU 3.0%，计算成本可忽略；DSE正则化方法能持续缓解密集退化效应。

Conclusion: 自监督密集退化是普遍存在的现象，提出的DSE指标能有效评估密集任务性能，基于DSE的模型选择和正则化方法能显著改善自监督学习在密集预测任务上的表现。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [103] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于长视频理解的基准测试，整合视觉、音频和文本多模态，包含约1000个信息密集的长视频，设计6个挑战性任务场景，并采用三步半自动质量保证流程。实验显示全模态模型在时间定位和长距离因果推理任务中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估模型理解长视频能力的基准测试，特别是在整合多模态信息（视觉、音频、文本）方面。需要评估模型对人类语言、观点、行为等上下文元素的理解能力。

Method: 从FineVideo数据集中筛选约1000个长时长、信息密集的视频（讲座、访谈、vlog等），设计6个任务场景（包括事件内和事件间任务），采用三步半自动质量保证流程确保问题难度和有效性。

Result: 实验结果表明全模态模型在精确时间定位和长距离因果推理任务中表现不佳，扩展实验揭示了多模态融合中的信息丢失和处理偏差问题。

Conclusion: LongInsightBench为长视频理解提供了首个综合性基准测试，揭示了当前全模态模型在多模态长视频理解方面的局限性，特别是在时间定位和因果推理方面的挑战。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [104] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文研究发现，通过增大对抗性补丁的尺寸可以绕过现有的防御方法。作者评估了针对覆盖人体大部分区域的对抗性衣服的防御效果，发现所有防御方法在数字和物理世界中都表现不佳，并成功制作出能同时攻破多个防御方法的对抗性衣服。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性补丁防御方法在面对简单增大补丁尺寸时就会失效，这促使研究者评估这些防御方法在面对覆盖人体大部分区域的对抗性衣服时的有效性。

Method: 通过制作覆盖人体大部分区域的对抗性衣服作为测试案例，评估多种防御方法在数字和物理世界中的表现。使用Faster R-CNN作为目标检测器，测试九种防御模型。

Result: 所有防御方法在对抗性衣服攻击下表现都很差。成功制作出单一对抗性衣服，在物理世界中攻击成功率：对未防御检测器达96.06%，对九个防御模型均超过64.84%。

Conclusion: 现有对抗性防御方法在面对大尺寸、自然的对抗性衣服时存在共同脆弱性，揭示了当前防御方法的局限性。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [105] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为CharDiff的新型扩散框架，通过字符级引导有效恢复和识别在真实条件下捕获的严重退化车牌图像。该方法利用外部分割和OCR模块提取细粒度字符级先验，并引入CHARM模块确保每个字符的引导仅限制在其自身区域，避免区域间干扰。实验表明，CharDiff在恢复质量和识别准确率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复的重要性不仅在于作为车牌识别系统的预处理阶段，还在于增加证据价值、提升视觉界面清晰度以及促进车牌图像的进一步利用。现有方法在处理严重退化的真实车牌图像时效果有限，需要更有效的恢复和识别方法。

Method: CharDiff是一种基于扩散的框架，利用外部分割和OCR模块提取字符级先验。核心创新是CHARM模块，通过区域掩码确保每个字符的引导仅作用于自身区域，避免交叉干扰。该方法结合了扩散模型和字符级结构化指导。

Result: 在Roboflow-LP数据集上，CharDiff显著优于基线恢复模型，在恢复质量和识别准确率方面都表现出色，相比性能最佳的基线模型实现了28%的相对CER降低。

Conclusion: 结构化字符引导条件化有效增强了基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性，证明了字符级指导在车牌图像恢复任务中的重要性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [106] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出了iDETEX——一个统一的多模态大语言模型，能够同时执行质量定位、感知和描述三个关键任务，在ViDA-UGC基准测试中实现了最先进的性能，并在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估从标量质量预测向更可解释、与人类对齐的评估范式发展的新兴挑战，实现详细且可解释的图像质量评估。

Method: 设计了任务特定的离线增强模块和数据混合策略，辅以在线增强策略充分利用多源监督，构建统一的多模态大语言模型同时处理质量定位、感知和描述任务。

Result: 在ViDA-UGC基准测试中实现了所有子任务的最先进性能，并在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。

Conclusion: iDETEX模型在提供准确且可解释的质量评估方面表现出有效性和鲁棒性，为详细图像质量评估提供了统一的解决方案。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [107] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 本文提出了一种后处理开放集识别方法，通过测量模型特征与预测logits之间的一致性来识别未知类。该方法基于输入到最近类均值的距离构建概率分布，并与softmax概率进行比较，无需重新训练预训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物分类模型在封闭世界设定下训练，当遇到未知类别时会过度自信。现有开放集识别方法大多需要重新训练模型，这限制了实际应用。本文旨在开发一种无需重新训练的后处理方法。

Method: 提出基于最近类均值的概率分布，通过比较NCM分布与softmax概率来衡量特征空间和logit空间之间的一致性。该方法作为后处理模块，可直接应用于预训练模型。

Result: 在两个数据集上均排名前三，性能表现稳定。在非洲和瑞典动物数据集上分别达到93.41和95.35的AUROC，优于现有方法。

Conclusion: 该方法提供了一种有效的后处理开放集识别策略，无需重新训练模型，在两个不同数据集上均表现出色且性能稳定。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [108] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 本文提出了Semantic-E2VID框架，通过引入跨模态特征对齐模块和语义感知特征融合块，将SAM模型的视觉语义知识迁移到事件相机数据中，显著提升了事件到视频重建的质量和语义信息恢复能力。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然具有低延迟、高动态范围等优势，但由于只捕捉强度变化而忽略静态信息，导致重建视频缺乏语义内容。现有E2V方法往往忽视语义信息的重要性，因此需要开发能够有效恢复语义信息的事件到视频重建方法。

Method: 提出Semantic-E2VID框架，包含跨模态特征对齐模块将SAM模型的语义知识迁移到事件编码器，语义感知特征融合块整合学习到的语义特征，以及语义感知E2V监督机制利用SAM生成的类别标签指导重建过程。

Result: 在多个基准测试中，Semantic-E2VID显著提升了帧质量，超越了现有的最先进E2V方法，证明了语义信息对事件到视频重建的重要价值。

Conclusion: 通过有效利用视觉语义知识，Semantic-E2VID成功解决了事件相机数据缺乏语义信息的问题，为事件到视频重建任务提供了新的解决方案，显著提升了重建质量。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [109] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过LLM引导的视觉令牌选择、递归处理和基于字幕的问答，在流式视频场景中实现高效处理，可丢弃约95%不重要的视觉令牌，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决视频大语言模型在流式场景中的挑战，即处理长时间视频时需要在线处理和及时响应问题，而现有模型难以在效率和效果之间取得平衡。

Method: 提出训练无关的三步法：1）基于LLM注意力选择关键视觉令牌，可丢弃95%不重要令牌；2）递归处理历史选中的令牌以保持时序连贯性；3）基于字幕的轻量级问答机制。该方法与标准Video-LLMs兼容。

Result: 在流式视频基准测试中达到最先进性能，在保持准确性的同时显著提升处理效率，实现了效率与效果的平衡。

Conclusion: 该方法为流式视频理解提供了一种高效解决方案，通过选择性处理视觉信息，在不牺牲性能的前提下大幅降低计算开销，具有良好的实用价值。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [110] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究首次全面评估了合成人脸数据在面部识别中的可行性，发现最佳合成数据集（VariFace、VIGFace）识别准确率达到95.67%和94.91%，超越了真实数据集CASIA-WebFace（94.70%），证明合成数据是科学可行且符合伦理的替代方案。


<details>
  <summary>Details</summary>
Motivation: 解决面部识别系统面临的两难困境：高精度需要大量未经同意收集的真实人脸数据，这导致数据集撤回和GDPR等法规下的法律责任风险。合成人脸数据作为隐私保护替代方案缺乏全面实证证据。

Method: 通过系统文献回顾识别25个合成人脸识别数据集（2018-2025），结合实验验证。评估七个关键隐私保护要求：身份泄露预防、类内变异性、身份可分离性、数据集规模、伦理数据来源、偏见缓解和基准可靠性。实验涉及超过1000万个合成样本，比较五个标准基准的结果。

Result: 最佳合成数据集VariFace和VIGFace分别达到95.67%和94.91%的识别准确率，超过真实数据集CASIA-WebFace（94.70%）。合成数据确保适当的类内变异性同时保持身份可分离性。尽管合成数据继承有限偏见，但通过生成参数提供了前所未有的偏见控制能力。

Conclusion: 合成人脸数据被确立为面部识别研究的科学可行且伦理必要的替代方案，为隐私保护和偏见缓解提供了新的可能性。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [111] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 本文提出了一种基于多面部表情特征的帕金森病严重程度诊断方法，通过注意力机制融合多种表情特征，并采用自适应类别平衡策略解决类别不平衡问题，显著提升了PD严重程度诊断性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于面部表情的PD诊断方法存在依赖单一表情类型导致误诊、忽略不同PD阶段类别不平衡问题，且大多仅进行二分类（PD/非PD）而非严重程度诊断。

Method: 提出基于注意力机制的多面部表情特征融合方法，整合多种表情特征；采用自适应类别平衡策略，根据类别分布和分类难度动态调整训练样本贡献度。

Result: 实验结果表明该方法在PD严重程度诊断方面表现出色，注意力特征融合和自适应类别平衡策略均显示出良好效果。

Conclusion: 所提出的多表情特征融合和自适应类别平衡方法有效解决了现有PD诊断方法的局限性，为PD严重程度诊断提供了可靠的技术方案。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [112] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出LoopTrans闭环框架，通过双向知识转移（从外中心到自我中心再返回）来增强弱监督功能定位能力，解决了传统单向转移在复杂交互场景中的局限性，即使在人体完全遮挡交互区域的情况下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 传统弱监督功能定位方法仅从外中心图像单向转移到自我中心图像，限制了在复杂交互场景中的适用性。人类能够通过观察他人与物体的交互来学习新物体的功能，这启发了需要更全面的知识转移框架。

Method: LoopTrans采用闭环框架，包含统一跨模态定位和去噪知识蒸馏机制。不仅从外中心转移到自我中心，还反向转移以增强外中心知识提取，有效桥接以物体为中心的自我中心图像和以交互为中心的外中心图像之间的领域差距。

Result: 实验表明，LoopTrans在图像和视频基准测试的所有指标上都实现了持续改进，即使在人体完全遮挡物体交互区域的挑战性场景下也能有效工作。

Conclusion: LoopTrans通过闭环双向知识转移框架，显著提升了弱监督功能定位的性能，为复杂交互场景中的视觉理解提供了更强大的解决方案。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [113] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 本文开发了一个基于视觉的马匹行为自动监测系统，使用YOLOv11和BoT-SORT技术检测和跟踪马厩中的马匹和人员，通过物体轨迹和空间关系推断五种事件类型，并考虑摄像头盲区。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监测方法劳动密集且耗时，需要自动化系统来早期检测健康福利问题。

Method: 使用YOLOv11进行目标检测和BoT-SORT进行多目标跟踪，基于物体轨迹和空间关系推断事件状态，利用CLIP和GroundingDINO构建自定义数据集。

Result: 定性评估显示系统在马匹相关事件检测上表现可靠，但人员检测因数据稀缺存在局限性。

Conclusion: 该工作为马场实时行为监测奠定了基础，对动物福利和稳定管理具有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [114] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一种智能、一体化、密集的关键点检测器，通过融合7种关键点检测器和2种边缘检测器的输出来创建真实标签掩码，然后训练轻量级ESPNet模型，在关键点密度、可重复性和正确匹配数量方面超越了现有检测器。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器（如SIFT、SURF、ORB等）和学习方法（如SuperPoint、R2D2等）存在对光度变化敏感、关键点密度和可重复性低、对挑战性场景适应性有限、缺乏语义理解等问题，无法优先关注视觉重要区域。

Method: 首先融合7种关键点检测器和2种边缘检测器的输出创建真实标签掩码，提取从角点、斑点到显著边缘和纹理的多样化视觉线索。然后使用这些掩码作为标签训练轻量高效的ESPNet模型，使DeepDetect能够语义化地关注图像，同时产生高度密集的关键点。

Result: 在Oxford Affine Covariant Regions数据集上的评估显示，DeepDetect在关键点密度（0.5143）、平均可重复性（0.9582）和正确匹配数量（59,003）方面均达到最大值，超越了其他检测器。

Conclusion: DeepDetect通过融合多种检测器的优势并利用深度学习，成功解决了传统关键点检测器的局限性，在关键点密度、可重复性和匹配性能方面实现了显著提升，能够适应多样化和视觉退化条件。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [115] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 该论文提出利用AV1视频压缩中的运动向量来生成密集亚像素对应关系和短轨迹，作为视觉SLAM前端，在保持与SIFT相当性能的同时显著降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统视觉SLAM方法如SIFT计算成本高，而视频压缩中已包含丰富的运动信息。研究旨在利用压缩域中的运动向量作为替代方案，实现资源高效的视觉前端处理。

Method: 重新利用AV1视频压缩的运动向量生成密集亚像素对应关系和余弦一致性过滤的短轨迹。该方法在压缩域运行，无需完全解码视频，显著降低CPU使用。

Result: 在117帧视频上，MV匹配成功注册所有图像并重建46-62万点，重投影误差0.51-0.53像素。性能与顺序SIFT相当但CPU使用显著减少，匹配密度更高。

Conclusion: 压缩域对应关系是一种实用且资源高效的前端解决方案，具有在完整流程中扩展的清晰路径，为视觉SLAM系统提供了新的优化方向。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [116] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文研究发现初始化是稀疏视图3D高斯泼溅性能的决定性因素，提出了基于SfM的增强初始化方法，包括频率感知SfM、3DGS自初始化和点云正则化，在稀疏视图设置下实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅容易过拟合训练视图，导致新视角渲染出现模糊等伪影。现有方法主要通过增强初始化或添加训练时约束来解决，但研究发现初始化才是决定性因素。

Method: 基于SfM构建增强初始化策略：1）频率感知SfM通过低频视图增强和宽松多视图对应改进低纹理区域覆盖；2）3DGS自初始化将光度监督提升为额外点，用学习的高斯中心补偿SfM稀疏区域；3）点云正则化通过几何/可见性先验确保多视图一致性和均匀空间覆盖。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视图设置下实现了持续的性能提升，确立了其作为更强初始化策略的地位。

Conclusion: 初始化是稀疏视图3D高斯泼溅的关键因素，提出的基于SfM的增强初始化方法通过频率感知、自初始化和正则化机制，有效提升了稀疏视图下的渲染质量。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [117] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld是一种创新的4D占用世界模型，通过稀疏动态查询实现灵活、自适应和高效的感知与预测。它解决了现有占用模型依赖静态嵌入和网格分类的局限性，提出了范围自适应感知模块和状态条件预测模块，在感知、预测和规划任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义占用世界模型大多依赖静态固定嵌入或网格，限制了感知的灵活性，且基于网格的"原地分类"与现实场景的动态连续性存在潜在不匹配问题。

Method: 提出SparseWorld模型，包含范围自适应感知模块（通过自车状态调制可学习查询并增强时空关联）和状态条件预测模块（用回归引导公式替代基于分类的预测）。采用时间感知自调度训练策略实现平滑高效训练。

Result: 大量实验表明，SparseWorld在感知、预测和规划任务中达到最先进性能。可视化分析和消融研究验证了其在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询成功构建了灵活、自适应且高效的4D占用世界模型，解决了现有模型在动态连续场景中的局限性，为自动驾驶等领域提供了更强大的环境表示能力。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [118] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种无需像素级标签的显著目标检测方法AutoSOD，通过POTNet生成高质量伪掩码来监督标准编码器-解码器，在多个基准测试中显著优于无监督和弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测是计算机视觉的基础任务，作者认为在获得可靠伪掩码的情况下，无需像素级标签也能达到接近监督方法的精度。现有原型方法存在边界和内部像素几何差异未被充分利用的问题。

Method: 提出POTNet，改进原型最优传输方法，使用熵引导的双聚类头：高熵像素通过谱聚类组织，低熵像素通过k-means聚类，然后通过最优传输对齐两个原型集，在单次前向传播中生成更清晰的伪掩码。

Result: 在五个基准测试中，AutoSOD在F-measure指标上比无监督方法提升高达26%，比弱监督方法提升高达36%，进一步缩小了与全监督模型的差距。

Conclusion: AutoSOD提供了一种端到端的无监督显著目标检测流程，无需离线投票即可生成高质量伪掩码，在准确性和训练效率上均有提升，证明了无监督方法可以达到接近监督方法的性能。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [119] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于评分标准引导的伪标签提示框架，用于零样本视频摘要。该方法将少量真实标注转化为高置信度伪标签，构建数据集自适应的评分标准，在推理时结合上下文信息评估场景重要性，在SumMe和TVSum数据集上取得了优于无监督和先前零样本方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且跨数据集泛化能力有限，无监督方法难以捕捉高级语义和细粒度叙事线索，而零样本提示方法对人工设计的提示模板和数据集特定的分数归一化高度敏感。需要一种既能保持零样本优势又能稳定性能的视频摘要方法。

Method: 提出评分标准引导的伪标签提示框架：将少量真实标注转化为高置信度伪标签，聚合成结构化、数据集自适应的评分标准。推理时，首尾片段仅基于描述评分，中间片段结合相邻场景的上下文摘要来评估叙事进展和冗余度，无需参数调优即可平衡局部显著性和全局连贯性。

Result: 在SumMe和TVSum数据集上分别达到57.58和63.05的F1分数，超越了无监督和先前零样本基线方法，接近监督方法的性能水平。

Conclusion: 评分标准引导的伪标签方法有效稳定了基于LLM的评分，为视频摘要建立了一个通用且可解释的零样本范式，展示了在减少标注依赖的同时保持良好性能的潜力。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [120] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 本文提出了一个针对大规模视频生成模型的优化训练框架，通过数据预处理、模型架构、训练策略和基础设施四个支柱的优化，显著提升了训练效率和性能。最终开发的MUG-V 10B模型在整体性能上达到最新水平，在电商视频生成任务上超越开源基线，并开源了完整的训练代码和模型。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉内容生成模型取得了显著进展，但训练大规模视频生成模型仍面临跨模态文本-视频对齐、长序列处理和复杂时空依赖等挑战，导致训练资源密集且困难。

Method: 提出四支柱优化框架：数据预处理、模型架构、训练策略和基础设施优化。包括视频压缩、参数缩放、课程式预训练和对齐后训练等创新方法，利用Megatron-Core实现高效训练和近线性多节点扩展。

Result: 开发的MUG-V 10B模型在整体性能上匹配最新视频生成器，在电商视频生成任务上通过人工评估超越了领先的开源基线。训练效率显著提升，实现了大规模视频生成的高效训练。

Conclusion: 该优化框架成功解决了大规模视频生成模型的训练挑战，实现了高效训练和优异性能。更重要的是，开源了完整的训练堆栈，包括模型权重、基于Megatron-Core的大规模训练代码和推理管道，为社区提供了首个利用Megatron-Core实现高效训练的大规模视频生成训练代码。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [121] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出了MambaX-Net，一种用于前列腺癌主动监测的半监督双扫描3D分割架构，通过利用先前时间点的MRI和分割掩码来改进当前时间点的分割，解决了纵向分析中多时间点和专家标注稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型通常基于单时间点和专家标注数据训练，不适合纵向主动监测分析，因为多时间点和专家标注稀缺阻碍了有效微调。

Method: MambaX-Net包含两个新组件：Mamba增强的交叉注意力模块（集成Mamba块以捕获时间演化和长距离空间依赖）和形状提取器模块（将先前分割掩码编码为潜在解剖表示）。采用半监督自训练策略，利用预训练nnU-Net生成的伪标签进行无专家标注学习。

Result: 在纵向主动监测数据集上的评估显示，MambaX-Net显著优于最先进的U-Net和Transformer模型，即使在有限和噪声数据上训练也能实现优越的前列腺区域分割。

Conclusion: MambaX-Net通过有效利用时间信息和半监督学习，为前列腺癌主动监测的纵向分析提供了强大的分割解决方案，克服了现有模型的局限性。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [122] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一种端到端的弱监督方法，仅使用图像级标签进行像素级道路裂缝检测。该方法通过分类器、重建器和检测器的对抗学习，结合路径感知注意力模块和中心增强CAM一致性模块，在减少标注成本的同时实现了与监督方法相当的检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少智能基础设施维护中对昂贵像素级标注的依赖，开发一种仅需图像级标签就能进行像素级道路裂缝检测的弱监督方法，以推动可扩展的道路检测应用。

Method: WP-CrackNet包含三个组件：生成类激活图的分类器、测量特征可推断性的重建器、产生像素级检测结果的检测器。通过对抗学习使分类器和重建器交替训练，检测器从后处理的CAM伪标签中学习。还设计了路径感知注意力模块和中心增强CAM一致性模块来提升性能。

Result: 在三个图像级数据集上的大量实验表明，WP-CrackNet实现了与监督方法相当的结果，并优于现有的弱监督方法，显著推进了可扩展的道路检测。

Conclusion: WP-CrackNet通过弱监督学习框架有效减少了标注成本，同时保持了高精度的道路裂缝检测性能，为智能基础设施维护提供了可行的解决方案。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [123] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D扩展了VGGT模型，通过引入动态感知聚合器解决动态场景中的多任务冲突，在相机姿态估计、深度预测和点云重建方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D前馈模型在静态数据集上训练，难以处理包含动态元素（如移动人物、可变形物体）的真实世界场景，需要开发能够同时处理静态和动态信息的4D重建方法。

Method: 提出PAGE-4D模型，基于VGGT扩展，引入动态感知聚合器预测动态感知掩码，分离静态和动态信息：抑制运动线索用于姿态估计，增强运动线索用于几何重建。

Result: 在动态场景中持续优于原始VGGT，在相机姿态估计、单目和视频深度估计、密集点云重建方面取得更优结果。

Conclusion: PAGE-4D成功解决了4D重建中的任务冲突问题，通过动态感知机制实现了对动态场景的有效处理，为真实世界应用提供了更好的解决方案。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [124] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个多智能体框架，通过图结构程序化形状表示将自然语言转换为结构化、可交互的3D资产，解决了现有方法生成非结构化网格和交互性差的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到3D生成方法产生的非结构化网格和交互性差的问题，使其更适合艺术工作流程。

Method: 提出基于图的程序化形状表示，将复杂自然语言分解为子任务图结构，利用LLM智能体分层解析用户输入并迭代优化程序化建模和绘制过程。

Result: 定性和定量实验表明ShapeCraft在生成几何精确和语义丰富的3D资产方面优于现有基于LLM的方法，并展示了动画和用户自定义编辑的潜力。

Conclusion: ShapeCraft通过结构化表示和多智能体框架实现了高质量的文本到3D生成，为更广泛的交互应用提供了可能性。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [125] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的自动化3D点云分割框架，结合无人机扫描的真实点云和BIM生成的合成数据，解决了基础设施3D模型中结构组件分割的挑战。该方法在铁路轨道数据集上验证了高精度分割效果，并通过小规模数据集+BIM数据的方式显著减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 无人机技术结合摄影测量能够高效获取基础设施的高分辨率3D模型，但传统手动分割结构组件的方法耗时且易出错，需要开发自动化分割解决方案。

Method: 提出机器学习框架，利用无人机扫描的真实点云和BIM生成的合成数据的互补优势，克服手动标注的局限性。通过结合小规模数据集和BIM数据，实现高效训练和准确分割。

Result: 在铁路轨道数据集上验证，能够高精度识别和分割铁轨、枕梁等主要组件。使用小规模数据集+BIM数据显著减少训练时间，同时保持合理的分割精度。

Conclusion: 该自动化方法提高了3D基础设施模型分割的精度和效率，推动了无人机与BIM技术在结构健康监测和基础设施管理中的集成应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [126] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是首个全谱图像无监督异常检测统一框架，通过五个简单元素的协同工作，在标准重建框架中实现卓越性能。该方法在12个基准测试中展现了跨多模态、任务设置和应用领域的全面优势，特别是在多类检测中达到前所未有的性能水平。


<details>
  <summary>Details</summary>
Motivation: 当前无监督异常检测领域已从专用单类模型发展到统一多类模型，但现有多类模型性能显著落后于最先进的单类对应模型。此外，该领域已分裂为针对特定场景的专门方法，造成部署障碍，迫切需要统一解决方案。

Method: 基于'少即是多'理念，Dinomaly2通过五个简单元素的协同工作在标准重建框架中实现异常检测。该方法采用简约设计，无需修改即可自然扩展到多样化任务，证明简单性是真正通用性的基础。

Result: 在12个UAD基准测试中，Dinomaly2展示了全谱优势：多类模型在MVTec-AD和VisA上分别达到99.9%和99.3%的图像级AUROC；多视图和多模态检测中达到最先进性能；仅使用每类8个正常样本即超越先前全样本模型，在MVTec-AD和VisA上分别达到98.7%和97.4% I-AUROC。

Conclusion: Dinomaly2的简约设计、计算可扩展性和通用适用性使其成为真实世界异常检测应用全谱的统一解决方案，成功弥合了多类模型的性能差距，同时无缝扩展到多样化数据模态和任务设置。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [127] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 该论文提出了PICABench基准测试，系统评估图像编辑的物理真实性，涵盖光学、力学和状态转换等8个子维度，并开发了可靠的评估协议PICAEval。研究发现当前主流模型在物理真实性方面仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要关注指令完成度，但忽视了编辑操作伴随的物理效应（如移除物体时应同时移除其阴影、反射等），导致编辑结果缺乏物理真实性。

Method: 构建PICABench基准测试，系统评估8个物理子维度；提出PICAEval评估协议，使用VLM作为评判器结合人工标注；从视频中学习物理知识，构建PICA-100K训练数据集。

Result: 评估主流模型后发现，物理真实性仍然是一个具有挑战性的问题，存在很大的探索空间。

Conclusion: 该研究为从简单内容编辑向物理一致性真实感的方向发展奠定了基础，提出的基准测试和解决方案可作为未来工作的参考。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [128] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 本文提出了一种名为IC-MoE的智能通信混合专家增强医学图像分割基础模型，通过构建三类专家模块和像素概率自适应投票策略，以及语义引导对比学习方法，解决了现有微调方法中高层特征表示不足和预训练权重结构完整性被破坏的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割基础模型的微调方法存在两个关键问题：1）高层特征表示不足；2）微调过程破坏了预训练权重的结构完整性。这些问题限制了模型在医学图像分割任务中的性能表现。

Method: IC-MoE模型构建了基础专家、语义专家和自适应专家三类模块，采用像素概率自适应投票策略进行专家选择和融合。同时提出语义引导对比学习方法，通过标签一致性和负载平衡来增强高层特征表示能力，同时保持预训练权重的结构完整性。

Result: 在三个公共医学图像分割数据集上的广泛实验表明，IC-MoE模型优于其他最先进的模型，有效补充了基础医学图像分割模型的高层特征和预训练结构完整性，并在多样化的医学图像分割场景中展现出优越的泛化能力。

Conclusion: IC-MoE模型通过智能通信混合专家架构和语义引导对比学习，成功解决了医学图像分割基础模型微调中的关键问题，为医学图像分割任务提供了更有效的解决方案，具有广泛的应用前景。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [129] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了多语言文本到图像行人检索任务，开发了首个多语言TIPR基准数据集，并提出了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决模态异质性问题，在所有多语言TIPR数据集上实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像行人检索方法存在的两个主要问题：一是全局方法忽略细粒度跨模态差异，局部方法需要先验信息进行显式部分对齐；二是现有方法主要针对英语，限制了在多语言环境中的应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过双向预测掩码图像和文本来隐式增强跨语言和跨模态的局部关系建模）和多维全局对齐模块（桥接模态异质性）。利用大语言模型进行初始翻译并整合领域知识构建多语言基准数据集。

Result: 所提出的方法在所有多语言TIPR数据集上实现了新的最先进结果，证明了其在多语言环境下的有效性。

Conclusion: Bi-IRRA框架通过结合双向隐式关系推理和多维全局对齐，成功解决了多语言文本到图像行人检索中的模态异质性问题，为多语言环境下的行人检索提供了有效解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [130] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 本文提出了4DSegStreamer框架，采用双线程系统处理流式4D全景分割，能够在动态环境中实现实时细粒度感知，特别适用于高FPS条件下的复杂场景。


<details>
  <summary>Details</summary>
Motivation: 解决高度动态环境（如密集人群疏散、复杂自动驾驶场景）中实时细粒度感知的需求，在有限时间预算内实现4D全景分割。

Method: 采用双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过对齐最新内存并补偿自运动和动态物体移动，确保对传入帧的及时预测。

Result: 在室内HOI4D数据集和室外SemanticKITTI、nuScenes数据集上的综合实验表明，该方法在复杂场景中准确预测动态物体方面表现优异，特别在高FPS条件下展现出比现有流式感知方法更强的鲁棒性。

Conclusion: 4DSegStreamer是一个通用框架，可无缝集成到现有3D和4D分割方法中实现实时能力，在动态环境感知方面具有显著优势。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [131] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 本文提出了一种新的帕金森病检测方法，通过分块策略和集成学习解决现有方法在数据集不足和未见患者数据鲁棒性方面的局限性。该方法将手绘图像分为2x2块分别处理特征，最终通过集成决策进行分类，在未见患者数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 帕金森病导致手部协调障碍，影响书写和绘画能力。现有基于手绘图像的早期检测方法存在两大局限：数据集不足和未见患者数据的鲁棒性差。本文旨在克服这些限制，提高帕金森病检测的准确性和泛化能力。

Method: 提出两阶段方法：第一阶段按绘图类型（圆形、波形、螺旋）分类；第二阶段采用分块策略，将每张图像分为2x2块，分别提取特征和检测帕金森病指标，最后使用集成方法合并各块决策进行最终分类。

Result: 在NewHandPD数据集上，对已见患者达到97.08%准确率，对未见患者达到94.91%准确率。与先前工作相比，准确率差距仅为2.17个百分点，而先前工作下降了4.76个百分点。

Conclusion: 提出的分块策略和集成方法有效提高了帕金森病检测的准确性和鲁棒性，特别是在处理未见患者数据时表现优异，显著优于现有最先进方法。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [132] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了MT-Video-Bench，这是首个专门评估多模态大语言模型在多轮视频对话中能力的基准测试，涵盖6项核心能力，包含987个精心策划的多轮对话，揭示了现有模型在处理多轮视频对话时的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准仅限于单轮问答，无法反映真实场景中多轮对话的复杂性，需要开发专门针对多轮视频对话的评估基准来推动多模态大语言模型的发展。

Method: 构建MT-Video-Bench基准测试，主要评估6项核心能力（感知性和交互性），涵盖987个来自不同领域的多轮对话，并与真实应用场景（如交互式体育分析和多轮视频智能辅导）严格对齐。

Result: 广泛评估了各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话时存在显著的性能差异和局限性。

Conclusion: MT-Video-Bench基准测试将公开可用，以促进未来研究，填补了多轮视频对话评估的空白，为MLLMs在复杂交互场景中的发展提供了重要工具。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [133] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 本文研究了签名伪造检测的特征学习策略，重点提升跨数据集泛化能力。通过对比原始签名图像和壳预处理两种方法，发现原始图像模型在基准测试中表现更好，而壳预处理方法显示出未来改进的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在离线签名验证中难以跨数据集泛化，手写风格和采集协议的差异导致性能下降，需要研究能提升跨数据集鲁棒性的特征学习策略。

Method: 使用CEDAR、ICDAR和GPDS Synthetic三个公开基准数据集，开发了两种实验流程：基于原始签名图像的方法和采用壳预处理的预处理方法，对比分析两种方法的特征学习效果。

Result: 原始图像模型在基准测试中取得了更高的性能，而壳预处理模型显示出未来改进的潜力，但两种方法之间没有确立明确的优越性。

Conclusion: 原始图像方法在当前表现更优，但壳预处理方法为构建鲁棒的跨域签名验证系统提供了有前景的研究方向，值得进一步探索和改进。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [134] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 本文提出了OP3Det，一种无需文本提示的开放世界3D检测器，通过结合2D语义先验和3D几何先验，利用跨模态专家混合实现广义3D物体性学习，在开放世界3D检测中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测器难以泛化到开放世界场景，而直接使用3D开放词汇模型又面临词汇扩展和语义重叠问题，因此需要研究能够检测训练中未见物体的广义3D物体性学习方法。

Method: OP3Det结合2D基础模型的强泛化能力和零样本能力，利用2D语义先验和3D几何先验生成类别无关的候选区域，通过跨模态专家混合动态路由单模态和多模态特征来学习广义3D物体性。

Result: 在广泛实验中，OP3Det显著超越现有开放世界3D检测器达16.0%的AR提升，相比封闭世界3D检测器也有13.5%的改进。

Conclusion: OP3Det通过有效结合2D和3D先验信息，成功实现了开放世界3D物体检测，为广义3D物体性学习提供了有效解决方案。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [135] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型采样加速方法——广义对抗求解器(GAS)，通过简单的ODE求解器参数化和对抗训练，在保持生成质量的同时显著减少采样步骤，相比现有方法在相同资源约束下表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成质量上达到最优，但采样过程计算成本高昂。现有方法虽然通过蒸馏技术减少了采样步骤，但依赖复杂训练技巧且未能很好地保持细节保真度。

Method: 提出广义求解器——一种简单的ODE采样器参数化方法，无需额外训练技巧；结合原始蒸馏损失与对抗训练，减少伪影并增强细节保真度，形成广义对抗求解器(GAS)。

Result: 在相似资源约束下，广义对抗求解器相比现有求解器训练方法表现出更优越的性能，能够更好地保持细粒度细节。

Conclusion: 广义对抗求解器提供了一种简单有效的扩散模型采样加速方案，通过参数化ODE求解器和对抗训练的结合，在减少采样步骤的同时提升了生成质量。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [136] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 本文提出了一种基于双编码器注意力机制的皮肤病变分类框架，结合精确的病灶分割和临床元数据，显著提高了皮肤癌分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测对患者预后至关重要，但现有深度学习模型存在高类内变异性和细微类间差异的挑战，且多为"黑箱"模型，限制了临床信任。

Method: 采用新型Deep-UNet架构结合双注意力门和ASPP进行病灶分割，分类阶段使用两个DenseNet201编码器分别处理原始图像和分割病灶，通过多头交叉注意力融合特征，并集成基于transformer的临床元数据模块。

Result: 在HAM10000数据集和ISIC 2018、2019挑战赛上，该方法实现了最先进的分割性能，显著提高了分类准确率和平均AUC，Grad-CAM热图验证了模型基于病灶区域进行预测的可靠性。

Conclusion: 结合精确病灶分割、临床数据和注意力融合的方法能够构建更准确和可解释的皮肤癌分类模型，为临床诊断提供可靠支持。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [137] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: 本文提出了RaindropGS基准测试，用于评估3D高斯溅射在雨滴干扰下的完整重建流程，揭示了现有方法在无约束雨滴图像上的性能限制和不同流程组件的影响。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射在真实雨滴条件下因相机镜头污染导致的严重遮挡和光学畸变问题，现有基准测试仅使用合成雨滴图像且假设理想条件，无法应对真实场景中雨滴干扰相机姿态估计和点云初始化的挑战。

Method: 提出RaindropGS基准测试，包含数据准备、数据处理和雨滴感知3DGS评估三个部分。收集真实世界雨滴重建数据集，包含雨滴聚焦、背景聚焦和无雨地面真实三种对齐图像集，评估不同聚焦条件下的重建质量。

Result: 通过综合实验分析，揭示了现有3DGS方法在无约束雨滴图像上的性能限制，以及不同流程组件的不同影响：相机聚焦位置对3DGS重建性能的影响，不准确的姿态和点云初始化对重建的干扰。

Conclusion: 为开发在雨滴条件下更鲁棒的3DGS方法确立了明确方向，通过全面基准测试揭示了关键性能限制因素和组件影响差异。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [138] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 本文研究了基于扩散变换器（DiT）的图像到视频（I2V）模型在生成拥挤公共场景中真实行人运动模式的能力，通过使用行人轨迹基准中的关键帧作为条件输入，并评估其轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 探索高性能I2V模型是否能够生成拥挤公共场景中真实的行人运动模式，利用这些模型在大规模视频数据集训练中获得的世界建模能力。

Method: 基于扩散变换器（DiT）的图像到视频模型，使用行人轨迹基准中提取的关键帧作为条件输入，通过定量测量行人动力学来评估轨迹预测性能。

Result: 研究发现I2V模型能够生成逼真的行人运动模式，在拥挤公共场景中表现出良好的轨迹预测能力。

Conclusion: 基于DiT的I2V模型具备生成真实行人运动模式的能力，为行人轨迹预测提供了新的有效方法。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [139] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、描述符无关的多参考视觉地点识别方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配。该方法在多种外观和视角变化下显著提升了识别性能，同时保持轻量化。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉地点识别中，现有方法面临训练成本高、计算复杂度大、以及传统描述符级融合方法在应对外观和视角变化时性能有限的问题。

Method: 提出基于矩阵分解的联合建模方法，将多个参考描述符分解为基表示，通过投影残差匹配实现位置识别。该方法无需训练，描述符无关，能够有效处理外观和视角变化。

Result: 在多外观数据上，该方法将Recall@1提升约18%，优于单参考和多参考基线方法；在非结构化数据上获得约5%的性能提升，展现出强大的泛化能力。

Conclusion: 该方法提供了一种轻量级、无需训练的多参考视觉地点识别解决方案，在保持计算效率的同时显著提升了在复杂环境变化下的识别性能。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [140] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种新的高效视觉语言模型推理范式，通过在预填充阶段剪枝冗余视觉token，在解码阶段仅检索与查询相关的token，实现视觉稀疏性的解耦，显著提升推理速度而不损失精度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的推理可扩展性受到视觉token数量增长的限制，这些token主导了推理延迟。需要一种方法在保持模型能力的同时显著加速推理过程。

Method: SparseVILA采用解耦的视觉稀疏性设计：预填充阶段进行查询无关的剪枝，解码阶段进行查询感知的token检索。该方法基于AWQ优化的推理流程，无需训练且与架构无关。

Result: 在长上下文视频任务中实现4.0倍预填充加速、2.5倍解码加速和2.6倍端到端加速，同时在文档理解和推理任务上提高准确性。

Conclusion: 通过解耦查询无关剪枝和查询感知检索，SparseVILA为高效多模态推理开辟了新方向，提供无需训练、架构无关的加速框架，且不牺牲模型能力。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [141] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 本文提出了一种名为ConsistEdit的新型注意力控制方法，专门针对MM-DiT架构设计，解决了现有训练自由编辑方法在编辑强度与源一致性之间的平衡问题，支持多轮和视频编辑，并实现了细粒度的属性修改。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由注意力控制方法难以同时实现强编辑能力和与源内容的一致性，特别是在多轮和视频编辑中视觉错误会随时间累积。大多数方法强制全局一致性，限制了修改单个属性（如纹理）同时保留其他属性的能力，阻碍了细粒度编辑。

Method: 通过深入分析MM-DiT的注意力机制，提出ConsistEdit方法，包含仅视觉注意力控制、掩码引导的前注意力融合，以及对查询、键和值令牌的差异化操作，以产生一致且与提示对齐的编辑结果。

Result: 大量实验表明，ConsistEdit在广泛的图像和视频编辑任务中实现了最先进的性能，包括结构一致和不一致场景。它是首个无需手工操作即可在所有推理步骤和注意力层执行编辑的方法，显著提高了可靠性和一致性。

Conclusion: ConsistEdit通过专门针对MM-DiT架构设计的注意力控制机制，成功解决了现有方法在编辑强度与一致性之间的权衡问题，支持稳健的多轮和多区域编辑，并实现了渐进式结构一致性调整，为细粒度编辑提供了更精细的控制。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [142] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究了量子自然语言处理在自然语言推理任务中的应用，比较了量子、混合和经典模型在少样本设置下的表现。量子模型在参数数量大幅减少的情况下达到与经典模型相当的性能，并展现出更高的每参数学习效率。


<details>
  <summary>Details</summary>
Motivation: 探索量子自然语言处理在语义建模中的潜力，特别是在资源受限的少样本学习场景下，验证量子模型相对于经典模型在参数效率和结构敏感性方面的优势。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路处理句子对，训练用于语义相关性和推理分类。引入信息增益每参数指标评估学习效率，并提出基于聚类的架构促进参数共享。

Result: 量子模型在推理任务中优于随机初始化的transformer，在相关性任务中测试误差更低。量子模型的每参数学习效率比经典模型高出最多五个数量级。

Conclusion: 量子自然语言处理在低资源、结构敏感场景中具有显著优势，量子模型能以更少参数达到同等性能，并展现出更高的学习效率，为自然语言处理提供了有前景的新方向。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [143] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的多模型融合框架，结合ChatGPT和Claude两种大型语言模型，在CheXpert数据集上提升胸部X光片解读的可靠性。通过相似度共识方法，在单模态和多模态输入条件下均显著提高了诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助放射学诊断存在可靠性不足的问题，需要开发能够减少诊断错误并提高临床实用性的方法。通过整合互补模态和输出级共识，可以提升AI诊断的可信度。

Method: 使用ChatGPT和Claude两种LLM构建多模型融合框架。采用基于95%输出相似度阈值的共识方法，在CheXpert数据集上评估单模态（仅图像）和多模态（图像+合成临床笔记）输入条件下的性能。

Result: 单模态条件下：ChatGPT准确率62.8%，Claude准确率76.9%，共识方法提升至77.6%。多模态条件下：ChatGPT准确率84%，Claude准确率76%，共识准确率达到91.3%。共识融合方法在所有实验条件下均优于单个模型。

Conclusion: 整合互补模态和使用输出级共识能够显著提高AI辅助放射学诊断的可靠性和临床实用性，为减少诊断错误提供了一条计算开销最小的实用路径。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [144] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 本文提出了CorrectBench基准测试，系统评估了LLM自校正策略在常识推理、数学推理和代码生成任务中的效果，发现自校正能提升准确性但降低效率，而简单的思维链基线表现具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种LLM自校正方法被提出，但这些方法的全面评估仍很缺乏，且LLM是否真正能自我校正仍是一个重要问题，因此需要建立系统性的评估基准。

Method: 开发CorrectBench基准测试，评估内在、外部和微调三种自校正策略，在常识推理、数学推理和代码生成三个任务上进行系统测试，并与思维链基线进行比较。

Result: 自校正方法能提高准确性，特别是复杂推理任务；混合不同策略可进一步提升但降低效率；推理LLM在额外自校正下优化有限且时间成本高；简单思维链基线具有竞争性准确率和效率。

Conclusion: 自校正有潜力增强LLM推理性能，但需在推理能力和操作效率间取得平衡，建议未来研究聚焦于优化这一平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [145] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 本文提出了EvolveR框架，使LLM智能体能够通过完整的闭环经验生命周期进行自我改进，包含离线自蒸馏和在线交互两个关键阶段，在复杂多跳问答基准上优于现有智能体基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在工具使用方面表现出色，但缺乏从自身经验中系统学习的能力。现有框架主要关注弥补外部知识差距，未能解决更根本的限制：无法迭代优化问题解决策略。

Method: EvolveR框架包含两个阶段：离线自蒸馏阶段将智能体交互轨迹合成为结构化、可重用的抽象策略原则库；在线交互阶段智能体与任务交互并主动检索蒸馏原则指导决策，积累多样化行为轨迹。采用策略强化机制迭代更新智能体。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于强大智能体基线的性能表现。

Conclusion: 这项工作为智能体提供了全面的蓝图，使其不仅从外部数据学习，还能从自身行为后果中学习，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [146] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大语言模型在自动化系统文献综述筛选阶段的交互作用。研究发现CoT少样本提示能提供最可靠的精确度-召回率平衡，零样本提示在高敏感度筛选中能最大化召回率，而自我反思提示因过度包容性和不稳定性表现不佳。GPT-4o和DeepSeek提供稳健的整体性能，GPT-4o-mini在显著更低的成本下表现具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的筛选阶段通常耗时且劳动密集，需要人工评估大量文献。本研究旨在探索大语言模型如何通过不同的提示策略来自动化这一过程，并分析模型与提示策略之间的交互效应，为实际应用提供指导。

Method: 评估了6个大语言模型在5种提示策略下执行相关性分类和6个二级任务的表现。使用准确率、精确度、召回率和F1分数作为评估指标，分析模型与提示策略的交互效应，并进行成本-性能分析。

Result: 结果显示显著的模型-提示交互效应：CoT少样本提示提供最可靠的精确度-召回率平衡；零样本提示在高敏感度筛选中最大化召回率；自我反思提示因过度包容性和不稳定性表现不佳。GPT-4o和DeepSeek提供稳健性能，GPT-4o-mini在低成本下表现具有竞争力。

Conclusion: 推荐采用分阶段工作流程：首先使用低成本模型和结构化提示进行初步筛选，仅将边界案例升级到更高容量模型。这些发现突显了大语言模型在自动化文献筛选方面不均匀但有前景的潜力，为任务自适应的大语言模型部署提供了比较基准和实用指导。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [147] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一个灵活的合成测试平台，用于系统分析语言模型中统计规律与事实关联之间的相互作用对泛化能力的影响。研究发现上下文多样性对分布内和分布外事实泛化的影响取决于上下文结构，并识别了导致泛化失败的优化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对语言模型中统计规律与事实关联相互作用如何影响泛化能力的系统分析。本文旨在通过可控实验环境来理解这种相互作用对模型泛化性能的影响机制。

Method: 设计了一个合成测试平台，将通用标记的统计流与源-目标标记对的事实流相结合，通过独立控制流组成（上下文结构）和多样性水平来操纵它们的相互作用。

Result: 研究发现更高的上下文多样性会延迟分布内事实准确性，但对分布外事实泛化的影响关键取决于上下文结构。在某些情况下，分布外性能与分布内趋势一致，而在其他情况下，多样性对于非平凡事实回忆至关重要。

Conclusion: 上下文设计和多样性水平的相互作用以不同方式影响泛化能力。通过模型组件干预实验，发现嵌入层和解嵌入层是导致分布外失败的关键优化瓶颈，该合成框架为未来研究提供了可控测试环境。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [148] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文首次对生成式AI的信任与不信任进行了计算研究，使用2022-2025年Reddit多子论坛数据集，发现信任与不信任随时间基本平衡，技术性能和可用性是主要维度，个人经验是态度形成的最常见原因。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统融入日常生活，理解公众对其的信任对于负责任采用和治理至关重要。现有AI信任研究主要来自心理学和人机交互领域，缺乏计算化、大规模和纵向的方法来衡量对生成式AI和大语言模型的信任与不信任。

Method: 使用2022-2025年Reddit多子论坛数据集（39个子论坛，197,618个帖子），结合众包标注和分类模型进行扩展分析，研究信任与不信任的演变模式和维度。

Result: 发现信任与不信任随时间基本平衡，在主要模型发布时出现波动；技术性能和可用性是最主要的信任维度；个人经验是态度形成的最常见原因；不同信任者群体（专家、伦理学家、普通用户）表现出不同的模式。

Conclusion: 本研究为大规模信任分析提供了方法论框架，并揭示了公众对生成式AI不断演变的认知，为负责任AI治理提供了重要见解。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [149] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了EgMM-Corpus，一个专门针对埃及文化的多模态数据集，包含3000多张图像，涵盖313个概念，包括地标、食物和民间传说。该数据集旨在为埃及文化背景下的视觉语言模型评估和训练提供可靠资源。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏中东和非洲地区的多模态文化多样性数据集，特别是针对埃及文化的资源有限，这限制了视觉语言模型在这些文化背景下的表现和发展。

Method: 通过设计和运行新的数据收集流程，收集了3000多张图像，涵盖313个埃及文化概念。每个条目都经过人工验证文化真实性和多模态一致性。

Result: 在EgMM-Corpus上评估CLIP的零样本性能，Top-1准确率为21.2%，Top-5准确率为36.4%，显示出大规模视觉语言模型存在文化偏见。

Conclusion: EgMM-Corpus作为开发文化感知模型的重要基准，突显了解决视觉语言模型中文化偏见的必要性，并为埃及文化背景下的模型评估提供了可靠资源。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [150] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 该论文提出了一个理论框架来分析语言模型的语法知识，通过研究字符串概率与语法性之间的关系，验证了三个关键预测：最小对字符串概率相关性、模型与人类判断相关性、以及语法/非语法字符串在概率空间中的分离度。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否真正学习了语法知识，以及如何通过字符串概率来揭示模型的底层语法结构，这对语言学理论有重要意义。

Method: 基于语料库数据生成过程的简单假设，构建理论框架分析语法、意义和字符串概率之间的关系，使用28万英语和中文句子对进行实证验证。

Result: 验证了三个预测：(1)最小对字符串概率相关性；(2)模型与人类判断相关性；(3)语法/非语法字符串在概率空间中分离度差。

Conclusion: 为使用概率研究语言模型的结构知识提供了理论基础，并为未来语言模型语法评估工作指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [151] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过将规划器定义的通过标准编码为子任务验证函数，解决了多智能体协作中的任务分解、协调和验证挑战，显著提升了系统鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型智能体被部署处理复杂任务，多智能体协作面临规划、协调和验证的新挑战。执行失败往往源于任务解释、输出格式或智能体间交接的微妙偏差，而非单纯的推理缺陷。

Method: VeriMAP规划器分解任务、建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数，实现验证感知的规划。

Result: 在多样化数据集上的评估表明，VeriMAP优于单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。

Conclusion: 验证感知规划能够在多智能体系统中实现可靠的协调和迭代优化，无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [152] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本文提出两种方法（多元解码和模型引导）来增强语言模型的多元对齐能力，在低资源设置下仅需50个标注样本就能显著提升模型性能，减少高风险任务中的误判，并改善对人类价值观的分布对齐。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型对社会影响日益增大，需要确保它们能够对齐多样化的观点并反映人类价值观的细微差别。当前主流训练范式假设每个查询只有一个最优答案，导致响应过于通用且对齐效果不佳。

Method: 提出两种低资源方法：多元解码和模型引导。模型引导通过少量标注样本（仅50个）引导模型学习多元视角，在零样本和少样本基准上实现一致改进。

Result: 方法在仇恨言论检测和错误信息检测等高风险任务中显著减少误判，在GlobalOpinionQA上改善了与人类价值观的分布对齐效果。

Conclusion: 这项工作强调了多样性的重要性，展示了语言模型如何适应考虑细微视角，为构建更具包容性和对齐性的AI系统提供了可行路径。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [153] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 本文研究了后训练技术增强的大语言模型是否具有对其学习内容和推理过程的意识。通过定义三个核心能力（策略意识、跨领域泛化、推理与输出对齐），对比了SFT、DPO和GRPO三种训练方法的效果。发现RL训练模型在策略意识和泛化能力上优于SFT模型，但在推理与输出对齐方面较弱，特别是GRPO模型。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术赋予大语言模型处理复杂逻辑任务的能力，需要探究这些模型是否真正理解自己的学习内容和推理过程。

Method: 定义了三个核心能力评估框架，在多个需要学习不同策略的任务上进行实证评估，对比了监督微调(SFT)、直接策略优化(DPO)和组相对策略优化(GRPO)三种后训练方法的效果。

Result: RL训练模型（DPO和GRPO）在策略意识和跨领域泛化能力上显著优于SFT模型，但推理轨迹与最终输出的对齐性较弱，GRPO模型在这方面表现最差。

Conclusion: 后训练方法显著影响大语言模型的元认知能力，RL方法增强策略意识和泛化但削弱推理一致性，需要在模型能力与推理透明度之间权衡。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [154] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究开发了BenCao，一个基于ChatGPT的中医多模态助手，通过自然语言指令调优而非参数重训练，整合了结构化知识库、诊断数据和专家反馈精炼，在中医问答和诊断任务中优于通用领域和中医领域模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑和多模态诊断线索，现有中医领域大语言模型在文本理解方面取得进展，但缺乏多模态整合、可解释性和临床适用性。

Method: 开发基于ChatGPT的中医多模态助手BenCao，整合超过1000部经典和现代文本的知识库、基于场景的指令框架、可解释推理的思维链模拟机制，以及执业中医师参与的反馈精炼过程，连接外部API进行舌象分类和多模态数据库检索。

Result: 在单选择题基准测试和多模态分类任务评估中，BenCao在诊断、草药识别和体质分类方面优于通用领域和中医领域模型，已在OpenAI GPTs Store部署，截至2025年10月有近1000名全球用户访问。

Conclusion: 本研究证明了通过自然语言指令调优和多模态集成开发中医领域大语言模型的可行性，为生成式AI与传统医学推理对齐提供了实用框架，并为实际部署提供了可扩展路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [155] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本研究探索利用大语言模型生成针对疫苗错误信息的实时反驳论点，通过多种提示策略和微调方法优化反驳生成，并训练分类器对反疫苗推文进行多标签分类，实现更情境感知的反驳。评估显示整合标签描述和结构化微调能显著提升反驳效果。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体影响公共卫生的时代，打击疫苗怀疑论和错误信息成为关键社会目标。疫苗相关的误导性叙述广泛传播，阻碍了高免疫接种率的实现并削弱了对健康建议的信任。虽然错误信息检测已取得进展，但生成针对此类主张的实时定制反驳论点仍是一个研究不足的领域。

Method: 本研究探索LLMs生成疫苗错误信息反驳论点的能力，基于先前错误信息驳斥研究，实验多种提示策略和微调方法优化反驳生成。同时训练分类器将反疫苗推文分类为疫苗效力担忧、副作用、政治影响等多标签类别，实现更情境感知的反驳。

Result: 通过人工判断、LLM评估和自动指标的评估显示，这些方法之间存在强一致性。研究发现整合标签描述和结构化微调能增强反驳论点的有效性。

Conclusion: 整合标签描述和结构化微调的方法为大规模缓解疫苗错误信息提供了一个有前景的途径，能够生成更有效的定制化反驳论点。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [156] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 本文提出了一种自回归论证结构预测（AASP）框架，用于联合建模论证组件和论证关系，在三个标准基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过生成范式将论证结构扁平化处理，难以有效建模论证组件和论证关系之间的复杂依赖关系。

Method: AASP框架基于自回归结构预测，将论证结构建模为预定义的约束动作集，使用条件预训练语言模型逐步构建论证结构，以有效捕捉论证推理流程。

Result: 在三个标准AM基准测试中，AASP在两个基准测试的所有AM任务上取得了最先进的结果，在一个基准测试中表现强劲。

Conclusion: 自回归论证结构预测框架能够有效建模论证组件和关系之间的依赖关系，在论证挖掘任务中取得了优异的性能。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [157] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级方法，通过线性变换特定层激活来引导LLM在心理健康评估任务中的表现，无需计算密集型技术即可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型快速发展，但小规模模型在特定领域应用中仍表现不佳，特别是在心理健康评估等敏感高影响领域，需要更高效且成本可控的适应方法。

Method: 采用轻量级线性变换方法，对特定层激活应用转向向量来引导模型输出。该方法不依赖计算密集型技术，通过简单的数学变换实现模型在心理健康领域的适应性优化。

Result: 该方法在两个不同任务中均取得改进：抑郁症状相关性预测任务和基于用户Reddit历史的标准化心理筛查问卷完成任务，证明了转向机制作为计算高效工具的潜力。

Conclusion: 转向机制展示了作为大型语言模型心理健康领域适应的计算高效工具的未开发潜力，为领域特定优化提供了轻量级解决方案。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [158] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景和23000多个评估标准的新基准，用于评估AI的道德推理过程。研究发现现有模型在数学、代码等任务上的表现无法预测其道德推理能力，且模型对特定道德框架存在偏好。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与决策，需要理解AI的决策过程而不仅仅是最终结果。道德困境是评估AI程序性推理的理想测试平台，因为道德问题允许多个合理的结论。

Method: 开发了MoReBench基准，包含1000个道德场景和专家制定的23000多个评估标准，涵盖识别道德考量、权衡利弊、提供可行建议等维度。同时创建MoReBench-Theory测试AI在五种规范伦理学框架下的推理能力。

Result: 研究发现扩展定律和现有数学、代码、科学推理基准无法预测模型的道德推理能力。模型对特定道德框架（如边沁功利主义和康德义务论）表现出偏好，这可能是流行训练范式的副作用。

Conclusion: MoReBench基准推动了以过程为重点的推理评估，有助于开发更安全、更透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [159] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本文探索了Whisper语音识别模型在二语口语评估中的潜力，通过提取隐藏表示中的声学和语言特征，仅需训练轻量级分类器即可超越现有最先进方法，并揭示了模型内在编码了口语熟练度模式和语义信息。


<details>
  <summary>Details</summary>
Motivation: 探索Whisper这一成熟语音识别基础模型在二语口语评估中的未开发潜力，超越以往仅分析其转录文本的研究方法。

Method: 从Whisper的隐藏表示中提取声学和语言特征，仅训练轻量级分类器，并融入图像和文本提示作为辅助相关线索。

Result: 在GEPT图片描述数据集上表现优异，超越现有最先进基线方法（包括多模态方法），通过融入图像和文本提示信息获得额外性能提升。

Conclusion: 即使没有任务特定微调，Whisper模型内在编码了口语熟练度的序数模式和语义方面，凸显其作为口语评估和其他口语理解任务的强大基础模型的潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [160] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt是一个新颖的LLM提示压缩框架，通过保留最具语义重要性的token来减少输入长度，在保持性能的同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的长输入上下文带来了高昂的货币成本、碳足迹和推理延迟，而典型提示中存在大量冗余的低效用token，只有少数token承载主要语义权重。

Method: 使用GlobEnc和DecompX两种先进的token归因方法为输入序列中的每个token分配显著性分数，按原始顺序保留前k%的token，获得稀疏的压缩提示。

Result: 在情感分析、常识问答和摘要任务中，20%的提示压缩仅导致性能轻微损失；但在数学推理任务中性能急剧下降，反映了对完整token连续性的更强依赖。

Conclusion: 该工作有助于更细致地理解LLM在性能-效率权衡中的行为，并界定了容忍上下文稀疏性的任务与需要详尽上下文的任务之间的边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [161] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器（仅0.6B参数）评估推理轨迹质量，在保持较低推理成本的同时，在Best-of-32设置中比多数投票准确率高4.61%，比现有过程奖励模型高4.31%到12.21%。


<details>
  <summary>Details</summary>
Motivation: 现有外部测试时扩展方法（特别是Best-of-N选择范式）面临两个关键限制：过程奖励模型的高计算开销，以及未充分利用LLM的内在潜在表示。

Method: TrajSelector利用采样器LLM中的隐藏状态进行过程级评分，使用仅0.6B参数的轻量级验证器评估逐步推理轨迹质量，并通过完全数据驱动的端到端训练方法聚合分数以识别最优推理轨迹，无需大量步骤级标注。

Result: 在五个基准测试中，TrajSelector实现了持续的性能提升。在Best-of-32设置中，比多数投票准确率高4.61%，比现有过程奖励模型高4.31%到12.21%，同时保持较低的推理成本。

Conclusion: TrajSelector提供了一种高效有效的Best-of-N框架，通过利用LLM的内在表示和轻量级验证器，在降低计算开销的同时显著提升了复杂推理任务的性能。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [162] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个用于广告视频违规检测的新框架，通过结合课程强化学习和多模态大语言模型，解决了现有方法在时间定位、噪声标注和泛化能力方面的局限性。该框架采用渐进式训练策略，无需显式推理标注即可发展推理能力，并在工业数据集上表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确时间定位、噪声标注处理和泛化能力方面存在不足，需要开发能够更好理解视频内容并进行准确违规检测的新方法。

Method: RAVEN框架整合课程强化学习与多模态大语言模型，采用渐进式训练策略结合精确和粗略标注数据，利用组相对策略优化发展推理能力，通过多层次复杂奖励机制确保精确时间定位和一致类别预测。

Result: 在工业数据集和公共基准测试中，RAVEN在违规类别准确性和时间间隔定位方面实现优越性能，在线A/B测试验证了其实际适用性，在精确率和召回率方面有显著提升，并展现出强大的泛化能力。

Conclusion: RAVEN通过创新的课程强化学习与多模态大语言模型结合，有效解决了广告视频违规检测中的关键挑战，在准确性和泛化能力方面均优于现有方法，具有重要的实际应用价值。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [163] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文扩展了自然语言推理中人类标注变异的研究范围，不仅关注标注者间标签一致但解释不同的情况，还分析了标签和解释都不同的情况。研究发现表面标签不一致可能掩盖深层的解释一致性，且标注者存在个体偏好。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注标注者标签一致但解释不同的情况，本文旨在扩展研究范围，分析标注者在标签和解释类型上都存在分歧的情况，以更全面理解自然语言推理中的标注变异。

Method: 使用LiTEx分类法对两个英文NLI数据集的自由文本解释进行分类，从NLI标签一致性、解释相似性和分类法一致性三个维度分析标注变异，并考虑标注者的选择偏差因素。

Result: 发现标注者有时标签不一致但提供高度相似的解释，表明表面分歧可能掩盖深层理解一致性。分析还揭示了标注者在解释策略和标签选择上的个体偏好。

Conclusion: 推理类型的一致性比单纯标签一致性更能反映自由文本解释的语义相似性，强调了基于推理的解释的丰富性，以及将标签视为绝对真实时需要谨慎。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [164] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 本文提出在LLM智能体中引入"退出"机制作为安全防护手段，当智能体缺乏信心时主动退出任务。通过ToolEmu框架对12个先进LLM进行评估，结果显示退出指令能显著提升安全性而几乎不影响帮助性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂环境中运行并产生现实后果，其安全性变得至关重要。多轮交互场景中的不确定性和模糊性会累积，导致超出传统文本生成失败的严重风险。

Method: 利用ToolEmu框架系统评估12个先进LLM的退出行为，通过添加明确的退出指令，让智能体在缺乏信心时主动退出任务。

Result: 退出机制带来高度有利的安全-帮助性权衡：所有模型安全性平均提升+0.39（0-3分制），专有模型提升+0.64，而帮助性仅平均下降-0.03。

Conclusion: 简单的退出指令是一种高度有效的安全机制，可立即部署到现有智能体系统中，作为高风险应用中自主智能体的有效第一道防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [165] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 本文提出了一种基于在线背包问题的自动化智能体系统组合框架，能够综合考虑性能、预算约束和兼容性，实现最优组件选择。相比传统的语义检索方法，该框架在单智能体设置中成功率提升达31.6%，在多智能体系统中从37%提升至87%。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组件发现方法主要依赖静态语义检索，存在能力描述不完整、检索方法局限等问题，无法基于能力、成本和实时效用进行组件选择，导致现有组件的有效重用和组合面临挑战。

Method: 提出受背包问题启发的结构化自动化框架，通过composer智能体系统识别、选择和组装最优智能体组件集，动态测试候选组件并实时建模其效用，综合考虑性能、预算约束和兼容性。

Result: 在五个基准数据集上使用Claude 3.5 Sonnet进行实证评估，在线背包组合器始终位于帕累托前沿，相比基线方法以显著更低的组件成本实现更高的成功率。单智能体设置中成功率提升达31.6%，多智能体系统中从37%提升至87%。

Conclusion: 该方法在不同领域和预算约束下展现出强大的适应性，显著性能差距证实了其鲁棒性，为智能体系统的可扩展资源重用提供了有效解决方案。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [166] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出了ReviewGuard系统，这是首个基于大语言模型的同行评审缺陷检测系统。该系统通过四阶段框架收集真实评审数据、进行标注、通过LLM生成合成数据增强训练集，并微调模型来检测和分类有缺陷的评审。研究发现有缺陷的评审具有评分低、自信度高、结构简单和负面情绪多的特征，且ChatGPT出现后AI生成评审显著增加。


<details>
  <summary>Details</summary>
Motivation: 随着论文提交量激增和大语言模型在学术评估中的广泛应用，同行评审系统面临前所未有的挑战。来自人类专家和AI系统的有缺陷评审可能系统性地破坏同行评审生态系统并损害学术诚信，因此需要自动化系统来检测和分类这些有缺陷的评审。

Method: ReviewGuard采用四阶段LLM驱动框架：1)从OpenReview收集ICLR和NeurIPS论文及其评审；2)使用GPT-4.1标注评审类型并人工验证；3)通过LLM驱动的合成数据增强解决类别不平衡和数据稀缺问题；4)微调编码器模型和开源LLM。最终构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的语料库。

Result: 研究发现有缺陷的评审相比充分评审具有更低的评分、更高的自报告信心、更简单的结构复杂度和更高的负面情绪比例。AI生成文本检测显示ChatGPT出现后AI生成评审显著增加。在缺陷评审检测模型评估中，使用合成和真实评审数据的混合训练显著提高了二元任务的召回率和F1分数。

Conclusion: 本研究提出了首个用于检测有缺陷同行评审的LLM驱动系统，为同行评审中的AI治理提供了证据，并为维护学术诚信的人机协作提供了宝贵见解。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [167] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本文通过分析大语言模型在回答跨文化和跨语言问题时的激活路径重叠情况，揭示了模型内部文化理解机制。研究发现：相同语言不同国家的问题激活路径重叠度高于不同语言相同国家的问题，表明存在强烈的语言特定模式；韩国-朝鲜这对语言相似但文化不同的国家表现出低重叠度和高变异性，说明语言相似性不能保证内部表征的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在不同文化背景下的广泛应用，准确的文化理解变得至关重要。现有评估主要关注输出层面的性能，而忽略了驱动响应差异的内部因素；同时，使用电路分析的研究覆盖语言较少且很少关注文化方面。

Method: 通过测量大语言模型在回答语义等价问题时激活路径的重叠度来追踪其内部文化理解机制。设置两种条件：固定问题语言改变目标国家，以及固定国家改变问题语言。使用相同语言的国家对来分离语言和文化因素。

Result: 结果显示：相同语言、不同国家问题的内部路径重叠度高于不同语言、相同国家的问题，表明存在强烈的语言特定模式。韩国-朝鲜这对语言相似但文化不同的国家表现出低重叠度和高变异性，说明语言相似性不能保证内部表征的一致性。

Conclusion: 大语言模型的内部文化理解机制表现出强烈的语言特定模式，语言相似性不能保证文化表征的一致性。这强调了在跨文化应用中需要更深入地理解模型的内部工作机制。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [168] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 本文提出了SHALLOW基准框架，这是首个系统分类和量化ASR系统中幻觉现象的方法，涵盖词汇、语音、形态和语义四个维度，能够识别传统WER指标无法区分的细粒度错误模式。


<details>
  <summary>Details</summary>
Motivation: ASR系统中的幻觉现象会产生与原始语音信号完全无关但语法语义上看似合理的转录结果，这在医疗和法律等关键领域带来严重风险。传统基于错误的评估指标无法区分语音不准确和幻觉，需要新的评估框架来有效识别和评估模型生成幻觉内容的倾向。

Method: SHALLOW框架系统地将ASR幻觉现象分类为词汇、语音、形态和语义四个互补维度，在每个类别中定义针对性指标来生成可解释的模型行为配置文件。

Result: 评估显示SHALLOW指标在识别质量高时与WER强相关，但随着WER增加相关性显著减弱。SHALLOW能够捕捉在退化和挑战性条件下WER无法区分的细粒度错误模式。

Conclusion: SHALLOW框架支持对模型弱点的具体诊断，并提供超越聚合错误率所能提供的模型改进反馈，为ASR系统的幻觉检测和评估提供了有效工具。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [169] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 本文提出了一种针对乌尔都语的AI生成文本检测框架，开发了包含1800篇人类撰写和1800篇AI生成文本的平衡数据集，使用多语言transformer模型进行微调，mDeBERTa-v3-base模型在测试集上取得了91.29%的F1分数和91.26%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本能力越来越接近人类写作，区分人类和机器生成文本变得困难，特别是在乌尔都语等低资源语言中缺乏有效的AI文本检测工具，这助长了错误信息和学术不端行为。

Method: 构建乌尔都语平衡数据集，进行语言学和统计分析，微调三种多语言transformer模型（mdeberta-v3-base、distilbert-base-multilingualcased、xlm-roberta-base），使用字符词数、词汇丰富度、N-gram模式等特征。

Result: mDeBERTa-v3-base模型表现最佳，在测试集上F1分数达到91.29%，准确率为91.26%，显著优于其他模型。

Conclusion: 该研究推进了乌尔都语社区打击错误信息和学术不端行为的努力，为低资源语言的NLP工具开发做出了贡献。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [170] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 本文提出Capsule Prompt-Tuning (CaPT)方法，通过将实例感知信息与任务感知信息结合，使用单个胶囊提示实现参数高效的微调，解决了传统提示学习方法依赖网格搜索、缺乏实例感知信息的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的学习方法严重依赖网格搜索来确定最佳提示长度，计算负担大，且缺乏实例感知信息，导致与输入序列的注意力交互受限。研究发现简单加入实例感知信息能提升模型性能，而无需额外微调。

Method: CaPT方法创新性地将实例感知和任务感知信息集成到单个胶囊提示中，采用近乎参数免费的方式。该方法利用现成的信息实例语义，在序列最早位置加入实例感知标记作为"注意力锚点"，保持对关键结构信息的强注意力。

Result: 实验结果表明，该方法在多种语言任务上表现优异（如T5-Large平均准确率达84.03%），同时保持高参数效率（如Llama3.2-1B仅需0.003%的模型参数）。

Conclusion: CaPT方法通过引入实例感知信息作为注意力锚点，有效提升了提示调优的性能和效率，为参数高效的LLM微调提供了新思路。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [171] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [172] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文探讨了思维链（CoT）推理在自然语言理解（NLU）任务中的应用，发现随着模型规模增大，CoT推理从阻碍性能转变为超越直接标签预测，且专门设计的训练方法能持续提升性能，使小模型在未见任务上达到大模型水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注思维链在推理任务中的作用，而忽视了其在自然语言理解任务中的潜力。本文旨在系统性地探索思维链是否也能类似地提升NLU任务的性能。

Method: 构建了NLURC数据集，包含高质量的自然语言理解任务和思维链，开发了多种思维链增强方法，并在NLU任务上系统评估这些方法的适用性。

Result: 发现模型规模与CoT推理效果呈正相关；大多数思维链增强训练方法表现不如仅标签训练，但有一种专门设计的方法能持续改进；使用思维链训练的模型在未见NLU任务上性能显著提升，达到十倍规模模型的水平。

Conclusion: 思维链在自然语言理解任务中具有重要价值，特别是在模型规模较大时，能显著提升性能并增强模型的可解释性，为小模型在复杂任务上的应用提供了新思路。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [173] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了2014-2025年间自然语言处理（NLP）技术在心脏病学领域的应用研究，分析了265篇相关文献，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病在全球范围内日益普遍，对健康产生重大影响。心脏病是复杂的多因素疾病，相关信息分散在各种文本数据中。NLP技术能够分析这些非结构化数据，为医疗专业人员提供更深入的见解，从而革新心脏病的诊断、治疗和预防方法。

Method: 本研究通过查询六个文献数据库，筛选出265篇相关文章。采用多维度分析方法，包括NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型，并进行时间趋势分析，展示了过去十年NLP方法在心脏病学领域的演变。

Result: 分析显示在NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等各个维度都存在显著的多样性，证明了NLP在心脏病学领域研究的广泛性。时间分析揭示了近十年来NLP方法的演变趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心脏病学中的广泛应用和不断发展，为未来研究提供了重要的参考框架。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [174] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 该论文首次系统研究了LLM中的"变色龙行为"——在多轮对话中面对矛盾问题时立场不稳定的现象。通过构建包含17,770个问答对的Chameleon基准数据集，评估了主流LLM在12个争议领域的表现，发现所有模型都存在严重的立场不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前集成搜索/检索引擎的LLM系统存在关键漏洞，其可靠性受到威胁。论文旨在揭示LLM在多轮对话中面对矛盾问题时立场不稳定的系统性缺陷，这对于医疗、法律和金融等需要一致决策支持的领域至关重要。

Method: 构建了包含17,770个问答对的Chameleon基准数据集，涵盖1,180个多轮对话和12个争议领域。提出了两个理论基础的指标：量化立场不稳定性的变色龙分数（0-1）和衡量知识多样性的源重用率（0-1）。评估了Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash等主流模型。

Result: 所有评估模型都表现出严重的变色龙行为（分数0.391-0.511），其中GPT-4o-mini表现最差。源重用率与置信度（r=0.627）和立场变化（r=0.429）之间存在强相关性，表明有限的知识多样性使模型病态地依赖查询框架。温度变化影响很小（小于0.004），表明这不是采样伪影。

Conclusion: LLM在多轮对话中存在严重的立场不稳定问题，这源于有限的知识多样性导致模型过度依赖查询框架。在部署到医疗、法律和金融等需要一致决策支持的领域前，必须进行全面的连贯性评估。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [175] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 本文研究了诗歌中空白空间的分布特征，分析了19,000首英语诗歌中空白空间的使用模式，并与AI生成诗歌和未发表诗歌进行对比，揭示了不同文本处理方法对空白空间表示的影响。


<details>
  <summary>Details</summary>
Motivation: 空白空间是诗歌形式的重要组成部分，反映了诗人的艺术选择和诗歌的语义空间特征。尽管诗歌是长期艺术形式和LLM生成任务，但NLP社区对空白空间的研究关注不足。

Method: 使用诗歌基金会的19,000首英语诗歌语料库，分析4,000位诗人的空白空间使用模式。比较已发表诗歌与51,000首LLM生成诗歌和12,000首未发表诗歌的空白空间差异，并探讨不同时期、诗体和数据源的空白空间使用。

Result: 发现不同文本处理方法会导致诗歌数据中空白空间表示的显著差异。发布了2,800首公共领域诗歌的格式化子集，为相关研究提供数据支持。

Conclusion: 诗歌空白空间分析揭示了诗人艺术选择的重要性，并为LLM预训练数据集的处理策略提供了重要启示，强调了保留原始格式的必要性。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [176] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 本文提出了Beacon基准测试，用于测量大型语言模型中的谄媚偏置（sycophancy）——即模型倾向于迎合用户而非坚持事实真相的倾向。研究发现该偏置可分解为语言和情感子偏置，并随模型规模增大而增强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在奖励优化过程中形成了真相与谄媚之间的结构性权衡，这种潜藏的偏置导致模型更倾向于用户认同而非原则性推理，需要精确测量和干预。

Method: 提出Beacon基准测试，通过单轮强制选择任务隔离谄媚偏置，评估12个最先进模型，发现偏置可分解为稳定子偏置，并提出提示级和激活级干预方法来调节这些偏置。

Result: 评估显示谄媚偏置可分解为语言和情感子偏置，且随模型容量增加而增强；提出的干预方法能在相反方向上调节这些偏置，揭示了对齐内部的动态几何结构。

Conclusion: Beacon将谄媚重新定义为可测量的规范性错误泛化形式，为研究和缓解大规模生成系统中的对齐漂移提供了可复现的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [177] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval是一个双语多任务评估基准，用于评估LLM在英语和阿拉伯语中的长上下文理解能力，涵盖4k到128k+token的上下文长度，包含四个新颖任务：多文档问答、双语问答、段落内声明验证和长上下文多选题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在处理和理解长上下文方面展现出复杂能力，需要严格的评估方法来有效评估其在长上下文理解方面的性能。

Method: 开发了LC-Eval双语评估基准，包含四个任务：多文档问答、双语问答、段落内声明验证和长上下文多选题。使用英语和阿拉伯语数据集，针对4k到128k+token的上下文长度进行设计。

Result: 评估显示LC-Eval具有显著挑战性，即使是GPT-4o等高性能模型在某些任务上也表现困难，突显了基准的复杂性和严谨性。

Conclusion: LC-Eval为长上下文理解提供了一个严格的双语评估框架，揭示了当前LLM在长上下文处理方面的局限性，为未来模型改进提供了重要基准。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [178] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC是一个用于句子嵌入模型领域适应的多阶段框架，通过结合领域特定的掩码监督和对比学习，有效将通用领域模型适配到专业领域，在NDCG@10指标上取得了高达13.4%的提升。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型在适应专业领域时面临的挑战，需要在保持原有模型鲁棒语义判别能力的同时，有效学习领域相关的表示。

Method: MOSAIC采用多阶段框架，在统一训练流程中联合优化掩码语言建模(MLM)和对比学习目标，通过选择性适应实现领域特定监督，包含平衡的联合监督和分阶段适应策略。

Result: 在高资源和低资源领域均进行了实证验证，相比强大的通用领域基线，在NDCG@10指标上实现了最高13.4%的改进。消融研究证明了各组件的重要性。

Conclusion: MOSAIC框架通过联合掩码监督和对比学习，成功实现了句子嵌入模型的有效领域适应，在保持原有模型优势的同时显著提升了领域性能。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [179] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文研究发现，尽管大型语言模型具备足够的数值知识来正确回答实体比较问题，但它们经常受到启发式偏见（实体流行度、提及顺序、语义共现）的影响而做出错误预测。小模型主要依赖表面线索而非原则性推理，而大模型能选择性地在数值知识更可靠时使用它，这解释了为什么大模型表现更好。思维链提示能引导所有模型使用数值特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在知识推理任务中何时依赖真实知识而非表面启发式，通过实体比较任务（如比较河流长度）来系统分析，因为这类任务有明确的事实标准。

Method: 通过实体比较任务分析LLMs的推理行为，识别三种启发式偏见：实体流行度、提及顺序和语义共现。使用逻辑回归分析表面线索对模型预测的影响，并比较不同规模模型（7-8B vs 32B参数）的知识使用策略。

Result: 小模型的预测主要受启发式偏见影响，仅使用表面线索的逻辑回归比模型自身的数值预测更准确。大模型能选择性地在数值知识更可靠时使用它，而小模型没有这种辨别能力。思维链提示能有效引导所有模型使用数值特征。

Conclusion: LLMs在知识推理中经常被启发式偏见主导而非真实知识，模型规模影响知识使用策略，大模型能更智能地选择可靠知识源。思维链提示是改善模型推理的有效方法。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [180] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 本文提出了一个两阶段检索-重排框架，用于跨体裁的作者归属任务。该框架通过微调大语言模型，在HIATUS的HRS1和HRS2基准测试中取得了显著提升，分别比之前最优方法提高了22.3和34.4个绝对Success@8点。


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属系统需要避免依赖主题线索，而是学习识别与文本主题无关的作者特定语言模式。传统信息检索中常用的训练策略在跨体裁作者归属任务中存在根本性不匹配问题，导致次优性能。

Method: 采用两阶段检索-重排框架，首先检索候选作者，然后使用微调的大语言模型进行重排。引入针对性的数据策展策略，使重排模型能够有效学习作者区分性信号。

Result: 在HIATUS的HRS1和HRS2跨体裁作者归属基准测试中，分别取得了22.3和34.4个绝对Success@8点的显著提升，大幅超越了之前的最优方法。

Conclusion: 提出的检索-重排框架结合针对性数据策展策略，能够有效解决跨体裁作者归属任务中的挑战，显著提升了模型性能，证明了该方法在识别作者特定语言模式方面的有效性。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [181] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 本文提出了CoRUS框架，用于模拟基于角色的用户问题，特别关注阿片类药物使用障碍(OUD)这一污名化领域。研究发现不同用户角色（患者、护理者、从业者）会引发LLM系统性的响应差异，脆弱角色获得更多支持性回应但知识内容减少。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估大多忽略了提问者的角色背景，而在污名化领域如OUD中，考虑用户上下文对于提供可访问、无污名的回应至关重要。需要开发能够模拟不同角色用户问题的评估方法。

Method: 基于角色理论和在线OUD康复社区数据，构建提问者角色分类法（患者、护理者、从业者），使用该框架模拟15,321个嵌入各角色目标、行为和体验的问题。

Result: 模拟问题具有高可信度且与现实数据相当。评估5个LLM发现：相同问题但不同角色会引发系统性差异，脆弱角色（患者和护理者）相比从业者获得更多支持性回应(+17%)但知识内容减少(-19%)。

Conclusion: 用户角色的隐式信号会显著影响模型响应，本文提供了基于角色的对话AI评估方法学，强调了在模型评估中考虑用户角色背景的重要性。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [182] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个创新的多智能体框架，通过CAVM架构统一数据、工具和智能体，结合迭代视觉增强机制和两阶段写作框架，能够生成接近人类专家质量的多模态财务报告。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业的财务报告，这是一个劳动密集且智力要求高的过程。

Method: 采用CAVM架构统一外部数据、设计工具和智能体到可编程变量空间，通过迭代视觉增强机制精炼原始视觉输出，并使用两阶段写作框架将简洁的分析链扩展为连贯的多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线方法，包括领先的深度研究系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的清晰路径，在财务报告自动化方面取得了重要进展。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [183] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 本研究提出了基于心理学感知理论的多模态语言模型具身知识理解基准，涵盖视觉、听觉、触觉、味觉、嗅觉和本体感觉。通过比较30个先进模型，发现视觉语言模型在具身知识理解上并未优于纯文本模型，且在视觉维度表现最差。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态语言模型取得显著进展，但尚不清楚视觉基础是否比纯文本模型更能增强其对具身知识的理解。需要系统评估模型在不同感知模态下的理解能力。

Method: 基于心理学感知理论构建具身知识理解基准，包含6种感知模态（视觉、听觉、触觉、味觉、嗅觉、本体感觉），通过向量比较和问答任务评估模型，包含1700多个问题。

Result: 视觉语言模型在具身知识理解任务中并未优于纯文本模型；模型在视觉维度表现显著差于其他感知维度；向量表示易受词形和频率影响；模型在涉及空间感知和推理的问题上表现困难。

Conclusion: 当前语言模型在具身知识理解方面存在显著局限，需要更有效地整合具身知识以增强对物理世界的理解能力。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [184] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: 本文介绍了ChiKhaPo基准测试，这是一个大规模多语言基准，覆盖2700+种语言，专门评估生成模型在低资源语言中的词汇理解和生成能力。现有SOTA模型在该基准上表现不佳，揭示了LLMs在大多数世界语言中缺乏基本语言能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准主要局限于高资源或中资源语言，且侧重于高阶推理和生成任务。然而，大量证据表明LLMs在全球3800+种书面语言中缺乏基本语言能力。需要开发一个能评估LLMs在低资源语言中基本词汇能力的基准。

Method: ChiKhaPo包含8个难度不同的子任务，利用现有词典、单语数据和双语平行语料构建。该基准在2个子任务中覆盖2700+种语言，在语言覆盖范围上超过任何现有基准。

Result: 6个SOTA模型在ChiKhaPo基准上表现不佳。性能得分受语言家族、语言资源丰富度、任务类型以及理解与生成方向等因素影响。

Conclusion: ChiKhaPo基准能够促进LLMs的大规模多语言基准测试，揭示模型在低资源语言中的能力缺陷，为改进多语言模型提供重要参考。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [185] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 本文提出PROMPT-MII方法，通过强化学习元学习生成紧凑指令，在保持与上下文学习相当性能的同时，将所需token数量减少3-13倍。


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然有效但推理成本高，随着上下文长度增长而显著增加。需要一种方法能够生成紧凑但描述性强的提示，以替代完整的训练集。

Method: 提出PROMPT-MII框架，基于强化学习元学习指令归纳模型，能够为任意新数据集动态生成紧凑指令。在HuggingFace hub的3000多个分类数据集上训练，在90个未见任务上评估。

Result: PROMPT-MII将下游模型质量提高了4-9个F1点（相对提升10-20%），在匹配上下文学习性能的同时，所需token数量减少3-13倍。

Conclusion: PROMPT-MII方法能够有效减少大语言模型适应新任务时的推理成本，同时保持或提升模型性能，为高效的任务适应提供了可行方案。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [186] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次将参数高效微调(PEFT)应用于孟加拉语仇恨言论检测，使用LoRA和QLoRA技术，在BD-SHS数据集上微调了三个大型语言模型，仅训练不到1%的参数即可在消费级GPU上实现高性能检测。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体上仇恨言论急剧增加，特别是针对妇女和青少年的攻击。现有方法要么依赖计算成本高昂的全模型微调，要么使用专有API，缺乏高效且可复现的解决方案。

Method: 使用LoRA和QLoRA参数高效微调技术，在BD-SHS数据集（50,281条标注评论）上微调三个指令调优的大语言模型：Gemma-3-4B、Llama-3.2-3B和Mistral-7B，仅训练不到1%的参数。

Result: Llama-3.2-3B获得最高F1分数92.23%，Mistral-7B为88.94%，Gemma-3-4B为80.25%。所有实验均在单个消费级GPU上完成。

Conclusion: PEFT被证明是孟加拉语及相关低资源语言仇恨言论检测的实用且可复现策略，为资源受限环境提供了高效解决方案。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [187] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一个极简的字节级分词器，直接将文本映射到UTF-8编码的字节ID，使用C0控制字节编码所有特殊行为，相比现有方法具有更快速度、更低传输开销和可共享的嵌入表。


<details>
  <summary>Details</summary>
Motivation: 现有字节级分词方法存在超出范围ID和辅助标记的问题，作者希望设计一个更简洁高效的分词器，利用ASCII原始设计理念将控制信息与可打印文本嵌入在一起。

Method: 提出UTF8Tokenizer，基于UTF-8字节编码实现分词，所有特殊行为（填充、边界、对话结构等）使用C0控制字节编码，无需额外标记。采用256维嵌入表，支持跨模型对齐，并通过位偏置嵌入增强训练效果。

Result: 实现了14倍的分词加速，主机-设备传输减少8倍（相比int64），HuggingFace兼容实现改善了语言建模收敛性。

Conclusion: UTF8Tokenizer通过回归ASCII原始设计理念，提供了一种高效、简洁的字节级分词方案，在性能和实用性方面具有显著优势。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [188] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种词汇表压缩方法，通过将词形变化表示为转换向量（如"walked" = "walk" + 过去时），而非为每个表面形式分配独立token，从而释放词汇表空间用于更多样化的词汇。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法将词形变化视为独立token，导致词汇表被表面形式变体占据，限制了低频词和多语言覆盖。本文旨在通过利用语言的内在结构来重新设计词汇表。

Method: 提出转换向量方法，将词形变化建模为加性偏移量，在输入和输出空间构建紧凑词汇表。使用共享的基础形式和转换向量组合生成表面形式，无需修改模型权重。

Result: 在多个LLM和五种语言上验证，可移除高达10%的词汇表条目，扩展了词汇表覆盖范围，对下游任务性能影响最小。

Conclusion: 研究结果推动词汇表设计从字符串枚举转向利用语言底层结构的组合式词汇表，为更高效的词汇表设计提供了新思路。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [189] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的动态防御框架，通过在线学习对抗迭代越狱攻击，使用Past-Direction Gradient Damping防止过拟合，在三个大语言模型上显著优于现有五种防御方法，同时还能提升无害任务的响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动破坏迭代越狱攻击的动态试错循环，这些攻击通过反复重写提示词并利用模型先前响应来引导新迭代，是一种高效的攻击策略。

Method: 提出基于强化学习的动态防御框架，通过在线学习更新防御策略，优化提示词以确保无害任务的适当响应同时明确拒绝有害提示。引入Past-Direction Gradient Damping防止过拟合到攻击期间探索的狭窄输入重写范围。

Result: 在三个大语言模型上的实验表明，该方法显著优于五种现有防御方法，对抗五种迭代越狱攻击。同时，提示优化策略还能提升无害任务的响应质量。

Conclusion: 所提出的动态防御框架能有效对抗迭代越狱攻击，通过在线学习和梯度阻尼技术实现强大的防御能力，同时保持对无害任务的良好响应质量。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [190] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了DiscoTrack基准测试，针对12种语言和四个话语理解层次：显著性识别、实体追踪、话语关系和桥接推理，旨在评估LLM在跨句子、段落和多说话者话语中整合和聚合信息的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要关注自然语言理解以提取显性信息，如问答或摘要，但缺乏针对更大文档中隐含信息和语用推理的更具挑战性且多语言的基准测试，特别是在话语追踪的背景下。

Method: 提出了DiscoTrack基准测试，涵盖12种语言和四个话语理解层次：显著性识别、实体追踪、话语关系和桥接推理，旨在评估LLM在跨句子、段落和多说话者话语中整合和聚合信息的能力。

Result: 评估显示，即使对于最先进的模型，这些任务仍然具有挑战性。

Conclusion: DiscoTrack基准测试填补了当前LLM评估在话语追踪和隐含信息理解方面的空白，为模型在多语言环境下处理复杂话语结构的能力提供了新的评估标准。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [191] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的提示敏感性现象，发现语义等效的不同提示会导致模型产生不同的答案分布。作者通过语义空间采样和改写扰动来改善不确定性校准，同时提出新的不确定性分解指标来量化提示敏感性对模型不确定性的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在提示敏感性现象，即语义等效的不同提示会导致模型产生不同的答案分布，这表明模型输出的不确定性可能无法真实反映其对提示含义的不确定性。作者旨在通过建模提示敏感性来改善模型的不确定性校准。

Method: 将提示敏感性建模为一种泛化误差，通过在语义概念空间中进行采样和改写扰动来改善不确定性校准。同时引入新的不确定性分解指标，该指标通过建模自然语言生成中的语义连续性来改进基于熵的分解方法。

Result: 研究表明，通过语义空间采样和改写扰动可以在不损害准确性的情况下改善不确定性校准。新提出的不确定性分解指标能够有效量化提示敏感性对LLM不确定性的贡献程度。

Conclusion: 这项工作为改善提示敏感语言模型的不确定性校准提供了新方法，并证明某些LLM未能对其输入含义表现出一致的一般推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [192] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本文揭示了基于推理的大语言模型在思维过程中会聚合社会偏见的现象，发现了两种导致偏见聚合的失败模式：刻板印象重复和无关信息注入，并提出了一种轻量级的提示方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然基于推理的大语言模型通过内部结构化思维过程在复杂任务上表现出色，但出现了思维过程会聚合社会偏见导致有偏见结果的现象。然而，语言模型在社会偏见场景中的底层行为仍未得到充分探索。

Method: 本文系统性地研究了思维过程中导致社会偏见聚合的机制，发现了两种失败模式：刻板印象重复（模型依赖社会刻板印象作为主要理由）和无关信息注入（模型捏造或引入新细节来支持有偏见的叙述）。基于这些发现，提出了一种轻量级的基于提示的缓解方法，让模型根据这些特定失败模式审查自己的初始推理。

Result: 在问答（BBQ和StereoSet）和开放式（BOLD）基准测试上的实验表明，该方法在保持或提高准确性的同时有效减少了偏见。

Conclusion: 本研究揭示了语言模型思维过程中社会偏见聚合的机制，并提出了有效的缓解方法，为开发更公平的AI系统提供了重要见解。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [193] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen是一个开源统一框架，用于训练、评估和可视化动态词汇增强语言模型，解决了现有方法代码库碎片化、缺乏现代LLM支持以及推理扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表训练的语言模型难以泛化到新词或词汇表外词汇，限制了处理多样化词汇组合的灵活性。现有动态词汇方法存在代码库碎片化、缺乏现代LLM支持和推理扩展性有限等挑战。

Method: DVAGen框架模块化处理流程便于定制，无缝集成开源LLM，首次提供CLI和WebUI工具进行实时结果检查。支持批量推理，显著提高推理吞吐量。

Result: 验证了动态词汇方法在现代LLM上的有效性，并展示了批量推理支持带来的显著推理吞吐量提升。

Conclusion: DVAGen提供了一个全面、可扩展的解决方案，克服了现有动态词汇方法的局限性，为动态词汇增强语言模型的研究和应用提供了统一框架。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [194] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于提示和基于强化学习的查询增强方法，发现简单的无训练查询增强方法在强大LLMs下表现与更昂贵的RL方法相当甚至更好。作者提出了一种新颖的混合方法OPQE，通过生成伪文档来优化检索性能，结合了提示的灵活性和RL的针对性优化。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在信息检索查询增强方面存在两种主要方法：基于提示的方法和基于强化学习的方法，但缺乏在一致实验条件下的系统比较。本文旨在填补这一空白，并探索两种方法的协同效应。

Method: 提出On-policy Pseudo-document Query Expansion (OPQE)方法，该方法不是重写查询，而是让LLM策略学习生成最大化检索性能的伪文档，结合了提示的灵活性和RL的针对性优化。

Result: 研究表明，简单的无训练查询增强方法在强大LLMs下表现与RL方法相当甚至更好。OPQE方法在证据搜索、临时检索和工具检索等多样化基准测试中优于单独的提示方法和RL重写方法。

Conclusion: 协同方法产生最佳结果，OPQE证明了结合提示的灵活性和RL的针对性优化的有效性，为查询增强提供了新的研究方向。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [195] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于分块稀疏注意力的长上下文语言模型，揭示了三个关键设计原则：表达性分块编码器、旁路残差路径和训练期间强制选择稀疏性。通过结合这些原则，实现了从4K上下文到3200万token的无训练长度外推新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文处理方法存在局限性：标准Transformer受限于二次复杂度，滑动窗口注意力和状态空间模型因固定大小内存而无法充分利用完整上下文。分块稀疏注意力在极端长度泛化方面表现优异，但其成功背后的关键架构原则尚未被充分理解。

Method: 提出统一框架和全面消融研究，识别三个关键设计原则：1) 具有专用CLS令牌的表达性非线性分块编码器；2) 旁路残差路径稳定整合检索到的全局信息；3) 预训练期间强制选择稀疏性以弥合训练-测试分布差距。提供分块内信息处理和地标生成的理论动机。

Result: 通过结合三个设计原则，在RULER和BABILong基准上实现了无训练长度外推的新SOTA，成功将4K上下文训练的模型泛化到3200万token。

Conclusion: 研究为开发未来高性能长上下文语言模型提供了一套清晰且经验基础的设计原则，明确了分块稀疏注意力架构成功的关键组件。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [196] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的注意力转移（AS）框架，用于解决大语言模型选择性遗忘中的关键困境：激进遗忘会损害模型效用，而保守策略则可能导致幻觉响应。AS框架通过上下文保持抑制和抗幻觉响应塑造，实现了遗忘效果与模型效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其可能保留的敏感数据引发了机器遗忘研究的需求。现有遗忘方法面临关键困境：激进遗忘会损害模型效用，而保守策略虽然保留效用但可能导致幻觉响应，这严重限制了LLMs在知识密集型应用中的可靠性。

Method: AS框架包含两个注意力级干预：重要性感知抑制（应用于遗忘集以减少对记忆知识的依赖）和注意力引导保留增强（强化保留数据集中语义重要token的注意力）。这两个组件通过双损失目标联合优化，形成软边界以在表示叠加下定位遗忘同时保留无关知识。

Result: 实验结果表明，AS在ToFU基准上比最先进的遗忘方法提高了15%的准确率，在TDEC基准上提高了10%，同时保持了具有竞争力的无幻觉遗忘效果。相比现有方法，AS在遗忘效果、泛化能力和响应可靠性之间展现了更优的平衡。

Conclusion: AS框架通过注意力转移机制成功解决了大语言模型选择性遗忘中的关键挑战，在保持模型效用的同时有效防止幻觉响应，为LLMs在知识密集型应用中的可靠部署提供了可行方案。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [197] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出StreamingThinker框架，实现LLM在阅读输入时进行流式思维推理，相比传统批处理推理减少80%的token等待时间和60%的延迟，同时保持性能相当。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待完整输入后才开始思考，这在动态场景中引入不必要延迟并削弱对早期信息的注意力。受人类边阅读边思考的认知启发，设计流式思维推理范式。

Method: StreamingThinker框架通过流式CoT生成、流式约束训练和流式并行推理实现边读边想。采用带质量控制的流式推理单元、流式注意力掩码和位置编码保持推理顺序，并行KV缓存解耦输入编码和推理生成。

Result: 在数学推理、逻辑推理和上下文QA任务上的实验表明，StreamingThinker在保持与批处理推理相当性能的同时，将推理开始前的token等待时间减少80%，最终答案生成的时间延迟减少60%以上。

Conclusion: 流式思维推理范式有效解决了传统LLM推理的延迟问题，实现了真正的并发推理，为动态场景下的高效推理提供了新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [198] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了VideoBiasEval框架，首次系统追踪视频扩散模型对齐过程中社会偏见的演化，发现对齐调优不仅强化了代表性偏见，还使其在时间维度上更加稳定。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型通过基于人类偏好的奖励模型进行对齐调优，虽然提升了视觉质量，但可能无意中编码和放大了社会偏见。需要系统追踪这些偏见在整个对齐流程中的演化过程。

Method: 基于社会偏见分类学，采用基于事件的提示策略分离语义内容和演员属性，引入多粒度指标评估种族偏见、性别偏见、分布变化和时间持续性。

Result: 发现对齐调优不仅加强了代表性偏见，还使其在时间上更加稳定，产生更平滑但更刻板的描述。揭示了人类偏好数据集中的偏见如何在奖励模型中被放大并传播到对齐调优的视频扩散模型中。

Conclusion: 研究强调了在整个对齐过程中进行偏见感知评估和缓解的必要性，以确保公平和负责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [199] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 本文通过大规模情感分析发现孟加拉语新闻中负面情绪（愤怒、恐惧、失望）占主导地位，不同媒体对相似事件的报道存在显著情感差异，并提出了可视化情感线索的新闻聚合器设计方案。


<details>
  <summary>Details</summary>
Motivation: 研究新闻媒体如何通过情感框架影响公众情绪，探索负面或情绪化标题更容易吸引注意力和传播的现象，分析孟加拉语新闻中的情感偏见。

Method: 使用Gemma-3 4B模型进行零样本推理，分析了300,000条孟加拉语新闻标题和内容，识别每篇报道的主要情感和整体基调。

Result: 研究发现负面情绪明显占主导地位，特别是愤怒、恐惧和失望，不同媒体对相似故事的报道存在显著的情感差异。

Conclusion: 基于研究结果提出了以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中隐藏的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [200] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了Transformer大语言模型的局部可解释性和机制可解释性方法，通过在医疗和自动驾驶领域的实验研究分析解释对信任的影响，并总结了当前未解决的问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型预测过程和内容生成的不可理解性，以及模型经常出现幻觉错误，迫切需要理解和解释语言模型的内部工作机制及其预测输出，以建立对这些模型的信任。

Method: 采用文献综述方法分析局部可解释性和机制可解释性方法，并在医疗和自动驾驶两个关键领域进行实验研究，分析解释对接收者的信任影响。

Result: 系统梳理了现有可解释性方法的研究成果，通过领域实验验证了解释在建立模型信任方面的作用，识别了当前解释方法面临的挑战。

Conclusion: 需要进一步研究生成与人类对齐的可信LLM解释，解决当前可解释性领域的未解决问题，推动大语言模型解释能力的发展。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [201] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了一种自动化学术分类法生成方法TaxoAlign，并创建了包含460个分类法的CS-TaxoBench基准数据集，旨在弥合人工生成与自动生成分类法之间的差距。该方法通过三阶段主题引导方法生成分类法，并在结构对齐和语义连贯性方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述生成方法缺乏对生成分类法与人工专家撰写分类法结构的比较分析，存在评估空白。

Method: 提出TaxoAlign方法，采用三阶段主题引导的指令指导方法生成学术分类法。同时开发严格的自动评估框架，衡量自动生成分类法与人工分类法在结构对齐和语义连贯性方面的差异。

Result: 在CS-TaxoBench基准上评估，TaxoAlign在几乎所有指标上都持续超越基线方法，验证了其有效性。

Conclusion: TaxoAlign方法能够有效生成与人工专家分类法结构对齐的学术分类法，为自动文献综述生成提供了可靠的技术支持。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [202] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出Nyx，一个针对通用检索增强生成（URAG）的统一混合模态检索器，解决了现有RAG系统仅限于文本模态的问题。通过构建NyxQA数据集和两阶段训练框架，Nyx在多模态检索和生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要关注单模态文本，无法处理现实世界中查询和文档可能包含混合模态（如文本和图像）的情况，这限制了其在真实场景中的应用效果。

Method: 提出Nyx统一混合模态检索器，通过四阶段自动管道构建NyxQA数据集，采用两阶段训练框架：先在NyxQA和开源检索数据集上预训练，然后利用下游视觉语言模型的反馈进行监督微调。

Result: 实验结果表明，Nyx不仅在标准文本RAG基准测试中表现优异，在更通用的URAG设置中表现更佳，显著提升了视觉语言任务的生成质量。

Conclusion: Nyx为混合模态检索增强生成提供了有效解决方案，通过统一的多模态检索器和高质量数据集构建，在现实场景中展现出强大的应用潜力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [203] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型在简单指令执行方面存在严重不足，特别是在选项标签格式变化时表现出明显的性能偏差，揭示了当前指令调优范式的缺陷。


<details>
  <summary>Details</summary>
Motivation: 探索指令调优大语言模型执行简单、自包含指令的能力，这是复杂指令遵循的基础，但目前研究不足。

Method: 在修改的MMLU和MMLU-Pro基准上评估20个IT-LLM，系统改变选项标签格式（字母、数字、罗马数字），在四种范式下测试模型表现。

Result: 标签格式变化导致显著性能波动（如罗马数字vs数字下降30.45%）；无指令时性能进一步下降；移除选项内容后模型无法超越随机基线；少样本学习无显著改善；大模型准确性更高但指令遵循仍不一致。

Conclusion: 当前指令调优范式存在不足，需要开发专门针对原子指令遵循的评估方法和训练策略。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [204] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文提出了EduAdapt基准测试，包含近48k个按年级标记的科学问答对，用于评估LLMs在K-12教育中根据学生年级水平调整回答的能力。研究发现，虽然大型模型表现更好，但在为低年级学生生成合适回答方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs虽然在学术基准测试上表现良好，但缺乏根据学生年级水平调整回答的能力，这在K-12教育中至关重要，因为年龄适当的词汇和解释对有效学习至关重要。

Method: 构建了EduAdapt基准测试，包含47,800个按年级标记的QA对，涵盖9个科学科目，分为4个年级组。评估了多种开源LLMs的年级适应性表现。

Result: 评估发现，虽然较大的模型通常表现更好，但在为低年级学生（1-5年级）生成合适回答方面仍然存在困难。

Conclusion: EduAdapt是首个评估LLMs年级适应性的数据集和评估框架，旨在通过更好的训练和提示策略促进更符合发展规律的教育AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [205] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究提出了Ladder-base，这是首个基于GRPO强化学习方法训练的中医领域大语言模型。相比传统监督微调方法，GRPO通过组内比较优化响应选择，显著提升了模型的推理能力和事实一致性。在标准评估中，Ladder-base在多个推理指标上超越了通用LLMs和现有中医专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系结构独特，现有中医专用大语言模型在监督微调方法下面临对齐、数据质量和评估一致性等限制。需要开发更有效的方法来提升模型在中医领域的推理能力和事实准确性。

Method: 基于Qwen2.5-7B-Instruct基础模型，采用Group Relative Policy Optimization强化学习方法，仅使用TCM-Ladder基准的文本子集进行训练。训练数据占比80%，验证和测试集各占10%。GRPO通过组内比较优化响应选择来提升推理和事实一致性。

Result: Ladder-base在标准化评估中表现出色，在多个推理指标上超越了GPT-4、Gemini 2.5、Claude 3、Qwen3等通用LLMs，以及BenTsao、HuatuoGPT2、Zhongjing等中医专用模型。

Conclusion: GRPO为在传统医学领域对齐专家级推理的LLMs提供了有效且高效的策略，支持开发可信赖且基于临床的中医人工智能系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [206] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 本文提出了AfriCaption框架，为20种非洲语言构建了首个可扩展的多语言图像描述系统，解决了多模态AI研究中资源匮乏语言被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI研究主要集中在高资源语言上，阻碍了该领域发展的民主化。为了解决这一问题，作者旨在为资源匮乏的非洲语言建立首个可扩展的图像描述资源。

Method: 提出AfriCaption框架，包括：基于Flickr8k构建的语义对齐数据集、通过模型集成和自适应替换确保质量的数据管道、以及集成SigLIP和NLLB200的0.5B参数视觉到文本架构。

Result: 建立了首个面向20种非洲语言的可扩展图像描述资源，通过上下文感知选择和翻译过程生成语义对齐的标题，确保数据质量。

Conclusion: AfriCaption框架为资源匮乏的非洲语言建立了首个可扩展的多语言图像描述系统，为实现真正包容的多模态AI奠定了基础。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [207] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 本文发现对齐后训练不仅导致任务准确率下降，还造成严重的校准损失，使模型过度自信、可靠性降低且输出多样性减少。通过简单的权重插值方法，可以在不牺牲准确率的情况下显著改善校准性能，甚至获得超越原始模型的帕累托最优解。


<details>
  <summary>Details</summary>
Motivation: 传统上认为对齐后训练主要影响任务准确率，但本文发现校准损失同样严重，导致模型过度自信和可靠性下降，需要探索更全面的对齐代价缓解方法。

Method: 提出简单的后处理干预方法：在模型对齐前后的权重之间进行插值。该方法计算效率高，通过权重混合找到帕累托最优的模型版本，既能提升准确率又能改善校准性能。

Result: 权重插值方法能够发现帕累托最优的模型版本，这些模型不仅准确率超过原始模型，还显著恢复了对齐过程中损失的校准性能，使模型更具能力和可靠性。

Conclusion: 简单的模型合并提供了一种计算高效的方法来缓解对齐代价的全面影响，可以同时获得更强大和更可靠的模型，证明了这不是严格的权衡关系。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [208] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文首次为乌尔都语到英语的习语翻译创建了评估数据集，涵盖原生乌尔都语和罗马化乌尔都语脚本，并评估了多种开源大语言模型和神经机器翻译系统在习语翻译任务上的表现。研究发现提示工程能提升习语翻译质量，且原生乌尔都语输入的翻译准确性优于罗马化乌尔都语。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在机器翻译中是一个重要挑战，特别是对于乌尔都语等低资源语言，此前研究关注有限。为了推动该领域研究，需要创建专门的评估数据集并系统评估现有翻译模型的表现。

Method: 创建了首个乌尔都语到英语习语翻译评估数据集，涵盖两种脚本（原生乌尔都语和罗马化乌尔都语）。评估了多种开源LLM和NMT系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标分析翻译质量，并比较了不同提示工程方法的效果。

Result: 提示工程相比直接翻译能提升习语翻译质量，但不同提示类型之间的性能差异较小。跨脚本比较显示文本表示对翻译质量有显著影响，原生乌尔都语输入产生的习语翻译比罗马化乌尔都语更准确。

Conclusion: 该研究为乌尔都语习语翻译提供了首个基准数据集，证明了提示工程在提升习语翻译质量方面的有效性，并揭示了文本表示对翻译性能的重要影响，为低资源语言的习语翻译研究提供了重要参考。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [209] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 该论文系统研究了多语言大语言模型在医疗问答中的跨语言事实对齐差异，发现即使使用非英语提示，模型回答仍更偏向英语维基百科的内容，但通过提供非英语维基百科的上下文信息可以有效改善事实对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗领域的应用日益广泛，确保不同语言用户都能获得可靠的健康信息至关重要。然而，不同语言的信息质量存在差异，这引发了人们对多语言大语言模型在医疗问答中可靠性和一致性的担忧。

Method: 构建了多语言维基医疗数据集(MultiWikiHealthCare)，分析了五种语言的医疗信息覆盖差异，评估了LLM回答与参考信息的事实对齐程度，并通过上下文信息和检索增强生成(RAG)进行了案例研究。

Result: 研究发现维基百科和LLM事实对齐都存在显著的跨语言差异。所有LLM的回答都更倾向于与英语维基百科对齐，即使提示是非英语的。提供非英语维基百科的上下文信息能有效将事实对齐转向文化相关知识。

Conclusion: 这些结果为构建更公平的多语言医疗AI系统提供了实用路径，强调了通过提供本地化上下文信息来改善跨语言事实对齐的重要性。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [210] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新颖的混合专家架构，通过跨层重用专家来突破传统层局部路由的限制，在固定参数预算下实现更丰富的专家组合，提升语言建模和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构受限于层局部路由机制，每层只能使用自己的专家池，需要在专家维度和路由多样性之间进行权衡。ReXMoE旨在通过跨层重用专家来突破这一限制。

Method: 提出ReXMoE架构，允许路由器在相邻层之间重用专家，将专家维度与每层预算解耦。采用渐进式缩放路由策略，在训练过程中逐步增加候选专家池。

Result: 在0.5B到7B参数的模型上进行广泛实验，ReXMoE在固定架构维度下持续提升性能，证实了其作为参数高效且可扩展的MoE-based LLMs新设计范式的有效性。

Conclusion: ReXMoE通过跨层专家重用机制，在保持参数效率的同时显著提升了模型表达能力，为MoE架构提供了新的设计方向。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [211] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出DETree方法，通过构建层次亲和树结构来建模不同AI-人类协作文本生成过程之间的关系，显著提升了混合文本检测的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 检测AI参与生成的文本对于打击错误信息、抄袭和学术不端行为至关重要。当前方法对AI-人类协作过程建模较为粗糙，主要采用二元分类或多分类方法，难以处理复杂的人类-AI协作文本特征。

Method: 提出DETree方法，将不同文本生成过程之间的关系建模为层次亲和树结构，并引入专门的损失函数使文本表示与树结构对齐。开发了RealBench基准数据集，自动包含各种人类-AI协作过程产生的混合文本。

Result: 该方法在混合文本检测任务中提升了性能，显著增强了在分布外场景下的鲁棒性和泛化能力，特别是在少样本学习条件下表现优异。

Conclusion: DETree方法通过层次亲和树建模不同文本生成过程的关系，有效解决了复杂人类-AI协作文本的检测挑战，展示了基于训练的方法在分布外设置中的潜力。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [212] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，从"流程执行系统"到"自适应社会系统"的演进路径，并探讨了记忆、规划和工具使用三大技术支柱的发展。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体已成为人工智能前沿，但如何将通用智能体研究转化为推动行业变革的生产力仍面临重大挑战。

Method: 采用行业智能体能力成熟度框架，系统分析记忆、规划和工具使用三大技术支柱的演进，从支持简单任务到实现复杂自主系统和集体智能。

Result: 综述了行业智能体在数字工程、科学发现、具身智能、协同业务执行和复杂系统仿真等领域的应用，识别了现有评估系统在真实性、安全性和行业特性方面面临的挑战。

Conclusion: 通过结合技术演进与行业实践，为理解和构建下一代行业智能体提供了清晰的路线图和理论基础，探讨了行业智能体的能力边界、发展潜力和治理问题。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [213] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为深度自演化推理（DSER）的概率范式，通过将迭代推理建模为马尔可夫链，利用多个并行长时程自演化过程放大微小改进概率，显著扩展了开源小规模语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有验证-精炼框架依赖强大的验证和修正能力，这在开源小规模模型中表现脆弱。本文旨在探索在弱验证和精炼能力下，如何突破此类模型在复杂任务上的推理极限。

Method: DSER将迭代推理建模为马尔可夫链，每个步骤代表解空间的随机转移。关键创新在于只要改进概率略高于退化概率，就能保证收敛到正确解。通过并行运行多个长时程自演化过程，放大这些微小正向趋势。

Result: 在DeepSeek-R1-0528-Qwen3-8B模型上应用DSER，在AIME 2024-2025基准测试中解决了9个先前无法解决的问题中的5个，整体性能提升，使这个紧凑模型通过多数投票超越了其600B参数教师的单轮准确率。

Conclusion: DSER不仅为测试时扩展提供了实用工具，更重要的是为诊断当前开源推理器的基本局限性提供了框架，明确了在自验证、精炼和稳定性方面的不足，为开发具有强大内在自演化能力的下一代模型确立了清晰的研究议程。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [214] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 本文提出了一种结合单语和跨语言表示学习最佳方法的多语言句子嵌入模型，显著减少了所需平行训练数据量（减少80%），在112种语言的Tatoeba数据集上达到83.7%的双文本检索准确率，远超LASER的65.5%，同时在单语迁移学习基准上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索BERT在跨语言句子嵌入方面的潜力，虽然BERT在单语句子嵌入学习方面表现优异，但其在跨语言句子嵌入方面的应用尚未得到系统研究。

Method: 结合掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax等最佳方法，利用预训练多语言语言模型大幅减少平行训练数据需求。

Result: 在112种语言的Tatoeba数据集上达到83.7%的双文本检索准确率，比LASER的65.5%显著提升；使用该模型从CommonCrawl挖掘的平行数据训练出的NMT模型在en-zh和en-de任务上表现优异。

Conclusion: 成功开发出高性能的多语言句子嵌入模型LaBSE，支持109+种语言，在跨语言检索和单语迁移学习任务上均表现出色，证明了所提方法的有效性。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [215] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出EliCal框架，通过两阶段方法实现语言模型的诚实对齐：首先使用廉价的自我一致性监督获取内部置信度，然后用少量正确性标注进行校准。该方法仅需1k标注（0.18%全监督）即可达到接近最优的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有诚实对齐方法要么依赖无训练的置信度估计，要么需要大量正确性标注进行基于训练的校准。实现基于训练的通用诚实对齐需要昂贵的大规模标注，因此需要开发标注高效的训练方法。

Method: 提出Elicitation-Then-Calibration (EliCal)两阶段框架：第一阶段使用廉价的自我一致性监督获取模型内部置信度；第二阶段使用少量正确性标注对置信度进行校准。该方法结合了自我一致性和正确性标注的优势。

Result: EliCal仅需1k正确性标注（占全监督的0.18%）即可达到接近最优的对齐效果。在未见过的MMLU任务上，比仅使用校准的基线方法表现更好，为通用诚实对齐提供了可扩展解决方案。

Conclusion: EliCal框架为语言模型的通用诚实对齐提供了一种标注高效的解决方案，显著降低了标注成本，同时在未见任务上表现出良好的泛化能力。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [216] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是首个大规模标准化基准，用于评估LLM模拟人类行为的能力。研究发现当前最佳LLM的模拟能力有限（40.80/100），性能随模型规模呈对数线性增长，但推理时间计算不会提升性能。存在对齐-模拟权衡：指令调优在低熵问题上提升性能，但在高熵问题上降低性能。模型在模拟特定人口群体时表现较差，模拟能力与深度知识推理能力高度相关。


<details>
  <summary>Details</summary>
Motivation: 当前LLM模拟人类行为的评估方法分散且不具可比性，阻碍了社会和行为科学的发展。需要建立标准化基准来系统评估LLM模拟的准确性和可靠性。

Method: 通过整合20个多样化数据集，涵盖从道德决策到经济选择等任务，创建SimBench基准。评估不同规模LLM的模拟能力，分析模型大小、推理计算、指令调优等因素对性能的影响。

Result: 最佳LLM模拟得分仅为40.80/100，性能随模型规模对数线性增长。指令调优在低熵问题上提升性能但在高熵问题上降低性能。模型在模拟特定人口群体时表现不佳，模拟能力与MMLU-Pro得分高度相关（r=0.939）。

Conclusion: SimBench为LLM模拟研究提供了标准化评估框架。当前LLM模拟能力有限，存在对齐-模拟权衡，需要开发更忠实的人类行为模拟器。模拟能力与深度知识推理密切相关，这为改进方向提供了重要启示。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [217] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 本文提出使用拓扑数据分析工具Mapper来分析语言模型如何编码歧义性，发现在微调后模型嵌入空间形成模块化、非凸区域，与模型预测对齐，即使对于高度歧义的情况也是如此。


<details>
  <summary>Details</summary>
Motivation: 传统标量评估指标（如准确率）无法捕捉模型内部如何表示歧义性，特别是在人类标注者存在分歧的情况下。需要新的方法来理解模型如何处理主观性和不确定性。

Method: 使用拓扑数据分析工具Mapper分析微调后的RoBERTa-Large模型在MD-Offense数据集上的嵌入空间结构，揭示模型如何组织决策区域和边界。

Result: 微调将嵌入空间重构为模块化、非凸区域，98%以上的连通组件表现出≥90%的预测纯度，但在歧义数据中与真实标签的对齐度下降，揭示了结构置信度与标签不确定性之间的隐藏张力。

Conclusion: Mapper是一个强大的诊断工具，能够理解模型如何解决歧义性，超越了传统可视化工具如PCA或UMAP，并能提供拓扑指标来指导主观NLP任务中的建模策略。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [218] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的语言混淆门（LCG）方法，通过解码时过滤令牌来解决大语言模型中的语言混淆问题，无需重新训练模型，且能区分有害混淆和可接受的语码转换。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本生成时经常出现语言混淆现象，即无意中混合使用多种语言。现有解决方案要么需要重新训练模型，要么无法区分有害的语言混淆和可接受的语码转换。

Method: LCG是一种轻量级插件解决方案，在解码过程中过滤令牌而不改变基础LLM。使用规范调整的自蒸馏方法训练，预测适当的语言家族，仅在需要时应用掩码。基于语言混淆不频繁、正确语言令牌通常位于预测顶部、高资源语言输出令牌嵌入规范更大的发现。

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1在内的各种模型上评估，LCG显著减少了语言混淆，通常降低一个数量级，且不影响任务性能。

Conclusion: LCG提供了一种有效的解决方案，能够显著减少大语言模型中的语言混淆问题，同时保持任务性能，且无需重新训练基础模型。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [219] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了LawChain框架，专门用于建模中国侵权相关民事案件的法律推理过程。该框架将侵权分析的法律推理操作化为三个模块，每个模块包含多个细粒度子步骤。作者构建了LawChain评估基准来系统评估侵权分析中的关键推理步骤，并发现当前大语言模型在侵权法律推理方面仍有不足。通过引入基于LawChain推理的基线方法，显著提升了法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有计算法律推理方法主要依赖通用推理框架（如三段论和IRAC），未能全面考察法律推理的细致过程。当前研究主要关注刑事案件，对民事案件的建模不足。本文旨在明确建模中国侵权相关民事案件中的法律推理过程。

Method: 提出LawChain框架，将侵权分析的法律推理过程操作化为三个模块，每个模块包含多个细粒度子步骤。构建LawChain评估基准，评估大语言模型在侵权法律推理中的表现。引入基于LawChain推理的基线方法，通过提示或后训练明确融入法律推理链。

Result: 评估表明当前大语言模型在准确处理侵权法律推理的关键要素方面仍有不足。提出的基线方法在侵权相关法律推理方面取得显著改进，并在相关法律分析任务（如法律命名实体识别和刑事损害赔偿计算）中表现出良好的泛化能力。

Conclusion: 明确建模法律推理链能够有效增强语言模型的推理能力。LawChain框架为民事案件的法律推理提供了系统化建模方法，提出的基线方法在侵权法律推理和相关任务中表现出优越性能。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [220] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文提出了一种改进的遗忘学习方法，在保持遗忘效果的同时，恢复了模型在提示中重新引入遗忘知识时的上下文利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法评估忽略了重要可用性方面：用户可能仍希望模型在提示中重新引入已移除信息时能够利用这些信息。研究发现现有方法会损害这种上下文效用。

Method: 在遗忘学习目标中增加一个插件项，通过保留模型在上下文存在遗忘知识时利用该知识的能力来增强遗忘学习方法。

Result: 实验表明该方法能将上下文效用恢复到接近原始水平，同时保持有效的遗忘效果和保留集效用。

Conclusion: 提出的方法成功解决了现有遗忘学习方法损害上下文效用的问题，为更实用的遗忘学习提供了解决方案。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [221] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 本文提出了Qomhr'a，一个在低资源条件下开发的双语爱尔兰语-英语大语言模型，通过完整的训练流程（双语持续预训练、指令调优和人类偏好对齐）显著提升了爱尔兰语性能，同时保持英语能力。


<details>
  <summary>Details</summary>
Motivation: 在低资源条件下开发高质量的双语爱尔兰语-英语大语言模型，以解决爱尔兰语资源稀缺的问题，同时保持模型的英语能力。

Method: 采用完整的三阶段训练流程：双语持续预训练、指令调优和人类偏好对齐。使用新获取的爱尔兰语语料库与英语文本混合训练，通过Gemini-2.5-Pro合成指令调优和人类偏好数据集。

Result: 在翻译、性别理解、主题识别和世界知识等基准测试中，爱尔兰语性能提升达29%，英语性能提升达44%。模型在指令跟随方面表现出明显进步，为聊天机器人功能奠定了基础。

Conclusion: Qomhr'a成功展示了在低资源条件下开发高质量双语LLM的可行性，为其他低资源语言提供了可复制的解决方案，并贡献了重要的双语数据集资源。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [222] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本研究采用对话分析方法，从学习者与大型语言模型(LLM)的对话中识别有效的教学策略，填补了现有评估方法忽视学习者-LLM交互的空白。


<details>
  <summary>Details</summary>
Motivation: 现有教育应用评估方法主要关注技术性能或学习成果，而忽视了学习者与LLM之间的互动过程，需要缩小这一研究空白。

Method: 采用对话分析方法，包括对话数据收集、对话行为标注、对话模式挖掘和预测模型构建四个步骤，从学习者-LLM对话中识别有效的教学策略。

Result: 研究正在进行中，已获得初步见解，为未来研究奠定基础。

Conclusion: 强调需要基于对话动态和教学策略来评估基于LLM的教育应用，关注交互过程而不仅仅是技术性能或学习结果。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [223] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了QueST框架，通过难度感知图采样和拒绝微调直接优化专用生成器来创建具有挑战性的编程问题。该方法生成的复杂编程问题显著提升了模型在竞争性编程任务上的性能，使8B模型能够匹配671B大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限（仅数千到数万问题），且依赖人工标注数据。传统合成数据生成方法要么扩展现有指令数据集，要么从人工标注数据中选择挑战性问题，存在可扩展性限制。

Method: QueST框架结合难度感知图采样和难度感知拒绝微调，直接优化专用生成器创建挑战性编程问题。利用生成的合成问题进行强教师模型的思维链蒸馏或对小模型进行强化学习。

Result: 在Qwen3-8B-base上使用QueST生成的10万困难问题进行微调后，在LiveCodeBench上超越了原始Qwen3-8B性能。额外使用11.2万示例（2.8万人工问题配多合成解决方案）后，8B模型性能匹配DeepSeek-R1-671B大模型。

Conclusion: 通过QueST生成复杂问题为推进大语言模型在竞争性编程和推理方面的前沿提供了有效且可扩展的方法。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [224] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级少样本命名实体识别框架，通过创新的指令调优模板和数据增强技术，在低资源场景下实现了与最先进模型相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别需要大量标注数据，但在低资源场景下标注成本高昂。现有零样本和指令调优方法难以泛化到领域特定实体，且无法有效利用有限可用数据。

Method: 提出两种关键技术：1）新的指令调优模板，简化输出格式以利用最新大语言模型的大上下文窗口；2）战略性数据增强技术，在保持实体信息的同时对上下文进行改写，扩展训练数据而不损害语义关系。

Result: 在基准数据集上的实验表明，该方法在少样本和零样本任务上达到与最先进模型相当的性能，少样本方法在CrossNER数据集上平均F1得分为80.1。使用改写方法训练的模型相比基线版本F1分数提升最高达17分。

Conclusion: 该方法为拥有有限NER训练数据和计算资源的群体提供了一个有前景的解决方案，在保持轻量级的同时显著提升了低资源场景下的命名实体识别性能。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [225] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了一个名为AcademicEval的实时基准测试，用于评估大语言模型在长上下文生成任务上的表现，使用arXiv论文作为数据源，无需人工标注，有效防止标签泄露问题。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文LLM基准测试存在上下文长度固定、标注劳动密集以及在LLM训练过程中标签泄露的紧迫挑战，因此需要一个新的评估框架。

Method: 采用arXiv论文构建学术写作任务（标题、摘要、引言和相关工作），整合高质量专家策划的少样本演示，支持灵活上下文长度，实现高效实时评估。

Result: 评估结果显示，LLMs在具有分层抽象级别的任务上表现较差，且难以处理长的少样本演示，突显了基准测试的挑战性。

Conclusion: 通过实验分析揭示了增强LLMs长上下文建模能力的一些见解，为未来研究提供了指导方向。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [226] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 本文提出了一种使用二元检索增强奖励的在线强化学习方法，有效减少语言模型的外在幻觉现象，在保持其他任务性能的同时显著提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 语言模型经常生成训练数据不支持的事实错误信息（外在幻觉），现有缓解方法往往损害开放生成和下游任务性能，限制了实际应用价值。

Method: 采用在线强化学习方法，设计二元检索增强奖励机制：仅当模型输出完全正确时奖励为1，否则为0。在Qwen3推理模型上进行评估，覆盖多样化任务。

Result: 在开放生成任务中，二元RAR方法将幻觉率降低39.3%，显著优于监督训练和连续奖励RL基线。在短问答任务中，模型学会校准性弃权，在参数知识不足时输出"我不知道"，在PopQA和GPQA上分别减少44.4%和21.7%的错误答案。

Conclusion: 二元检索增强奖励方法在提升事实准确性的同时不会损害指令遵循、数学或代码任务的性能，而连续奖励RL尽管改善事实性却会导致质量退化。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [227] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文提出了FARE（基础自动推理评估器），通过大规模数据驱动方法训练生成式评估器，在多个推理评估任务中超越了更大的专业评估器，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前生成式评估器训练主要关注新方法学（如强化学习），而忽视了大规模数据驱动的开发。本文旨在通过数据扩展来解决这一问题。

Method: 采用简单的迭代拒绝采样监督微调方法，基于250万样本数据集训练8B和20B参数的评估器，涵盖五种评估任务和多个推理领域。

Result: FARE-8B挑战了更大的专业RL训练评估器，FARE-20B在开源评估器中创下新标准，超越了70B+的专业评估器。在实际应用中，FARE-20B在MATH上达到接近oracle性能，在RL训练中比字符串匹配验证器提升14.1%的下游模型性能。

Conclusion: 大规模数据驱动的简单SFT方法可以训练出强大的生成式评估器，在静态基准和实际应用中均表现出色，为评估器开发提供了新的有效范式。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [228] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 本文提出了企业深度研究（EDR）系统，这是一个多智能体框架，专门解决企业环境中非结构化数据转化为可操作洞察的挑战。系统通过主规划智能体、四个专业搜索智能体、可扩展工具生态系统、可视化智能体和反思机制，实现了自动化报告生成和实时流式处理，在多个基准测试中超越了现有最先进的智能体系统。


<details>
  <summary>Details</summary>
Motivation: 随着信息呈指数级增长，企业面临将非结构化数据转化为连贯、可操作洞察的日益增长的压力。虽然自主智能体显示出潜力，但它们通常在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: EDR是一个多智能体系统，包含：主规划智能体用于自适应查询分解；四个专业搜索智能体（通用、学术、GitHub、LinkedIn）；基于MCP的可扩展工具生态系统，支持NL2SQL、文件分析和企业工作流；可视化智能体用于数据驱动洞察；以及检测知识差距并更新研究方向的反思机制。

Result: 在包括DeepResearch Bench和DeepConsult在内的开放式基准测试中，EDR在没有任何人工指导的情况下超越了最先进的智能体系统。系统在内部数据集上验证了自动化报告生成、实时流式处理和无缝企业部署的能力。

Conclusion: EDR框架通过其多智能体架构和反思机制，有效解决了企业环境中自主智能体面临的挑战，为多智能体推理应用的研究提供了重要进展。作者发布了EDR框架和基准轨迹以促进相关研究。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [229] [Six Proofs of Interpolation for the Modal Logic K](https://arxiv.org/abs/2510.16398)
*Nick Bezhanishvili,Balder ten Cate,Rosalie Iemhoff*

Main category: cs.LO

TL;DR: 本文通过六种不同技术方法（模型论、证明论、句法、自动机理论、准模型和代数方法）证明了模态逻辑K的Craig插值定理，并对各种证明技术的优缺点进行了比较分析。


<details>
  <summary>Details</summary>
Motivation: 探讨模态逻辑K中Craig插值定理的多种证明方法，旨在展示不同证明技术的多样性和各自特点，为理解这一重要定理提供全面的视角。

Method: 采用六种不同的技术方法：模型论方法、证明论方法、句法方法、自动机理论方法、准模型方法和代数方法，每种方法都独立证明了Craig插值定理在模态逻辑K中的成立。

Result: 成功通过六种不同技术方法证明了模态逻辑K的Craig插值定理，并系统比较了各种证明方法的优缺点，展示了不同证明技术的适用范围和特点。

Conclusion: 模态逻辑K的Craig插值定理可以通过多种不同技术方法得到证明，每种方法都有其独特的优势和局限性，这为理解该定理提供了丰富的视角和工具选择。

Abstract: In this chapter, we present six different proofs of Craig interpolation for
the modal logic K, each using a different set of techniques (model-theoretic,
proof-theoretic, syntactic, automata-theoretic, using quasi-models, and
algebraic). We compare the pros and cons of each proof technique.

</details>


### [230] [Explainability Requirements as Hyperproperties](https://arxiv.org/abs/2510.16402)
*Bernd Finkbeiner,Julian Siber*

Main category: cs.LO

TL;DR: 本文提出了一种基于超属性的形式化方法，将可解释性建模为系统属性。通过结合刘易斯反事实逻辑、线性时序逻辑和知识模态，在多智能体系统中形式化反事实可解释性，并证明其模型检测问题是可判定的。


<details>
  <summary>Details</summary>
Motivation: 当前自主系统对可解释性需求日益增长，但大多数研究关注什么是有效解释，很少将可解释性形式化为系统属性。本文旨在填补这一空白，从超属性角度形式化可解释性。

Method: 结合刘易斯反事实逻辑、线性时序逻辑和知识模态三种模态逻辑，构建形式化框架来推理智能体是否知道特定观察的原因。将该逻辑嵌入超逻辑中，证明模型检测问题的可判定性。

Result: 成功形式化了系统层面的多种可解释性概念，证明了所提出逻辑的模型检测问题是可判定的，为自动化验证可解释性需求奠定了基础。

Conclusion: 本文提供了一种形式化方法来建模和验证多智能体系统中的可解释性，通过逻辑框架的构建和可判定性证明，为实现可解释性需求的自动化验证提供了理论支持。

Abstract: Explainability is emerging as a key requirement for autonomous systems. While
many works have focused on what constitutes a valid explanation, few have
considered formalizing explainability as a system property. In this work, we
approach this problem from the perspective of hyperproperties. We start with a
combination of three prominent flavors of modal logic and show how they can be
used for specifying and verifying counterfactual explainability in multi-agent
systems: With Lewis' counterfactuals, linear-time temporal logic, and a
knowledge modality, we can reason about whether agents know why a specific
observation occurs, i.e., whether that observation is explainable to them. We
use this logic to formalize multiple notions of explainability on the system
level. We then show how this logic can be embedded into a hyperlogic. Notably,
from this analysis we conclude that the model-checking problem of our logic is
decidable, which paves the way for the automated verification of explainability
requirements.

</details>


### [231] [Bilateralist base-extension semantics with incompatible proofs and refutations](https://arxiv.org/abs/2510.16763)
*Victor Barroso-Nascimento,Maria Osório Costa,Elaine Pimentel*

Main category: cs.LO

TL;DR: 本文提出了一个双边逻辑系统，将断言和否定视为独立但对立的行为。该系统确保公式不能同时被证明和反驳而不产生矛盾，为建模排除不一致性的认知实体（如数学证明和反驳）提供了框架。


<details>
  <summary>Details</summary>
Motivation: 挑战传统逻辑概念，通过将断言和否定作为独立行为来处理，旨在为建模数学证明和反驳等认知实体提供一个排除不一致性的框架。

Method: 通过双边自然演绎系统形式化逻辑，具有良好的证明论性质（包括归一化）。引入基础扩展语义，要求明确构造证明和反驳，同时防止它们为同一公式建立。

Result: 证明了语义相对于演算的健全性和完备性。展示了反驳概念对应于David Nelson的构造性虚假，扩展而非修正直觉主义逻辑。

Conclusion: 该系统强化了表示构造性认知推理的适用性，为处理断言和否定的独立行为提供了坚实的逻辑基础。

Abstract: Logical bilateralism challenges traditional concepts of logic by treating
assertion and denial as independent yet opposed acts. While initially devised
to justify classical logic, its constructive variants show that both acts admit
intuitionistic interpretations. This paper presents a bilateral system where a
formula cannot be both provable and refutable without contradiction, offering a
framework for modelling epistemic entities, such as mathematical proofs and
refutations, that exclude inconsistency.
  The logic is formalised through a bilateral natural deduction system with
desirable proof-theoretic properties, including normalisation. We also
introduce a base-extension semantics requiring explicit constructions of proofs
and refutations while preventing them from being established for the same
formula. The semantics is proven sound and complete with respect to the
calculus. Finally, we show that our notion of refutation corresponds to David
Nelson's constructive falsity, extending rather than revising intuitionistic
logic and reinforcing the system's suitability for representing constructive
epistemic reasoning.

</details>


### [232] [ATL*AS: An Automata-Theoretic Approach and Tool for the Verification of Strategic Abilities in Multi-Agent Systems](https://arxiv.org/abs/2510.17306)
*Sofia Garcia de Blas Garcia-Alcalde,Francesco Belardinelli*

Main category: cs.LO

TL;DR: 本文提出了两种新颖的符号算法，用于在无限迹和有限迹语义下对交替时序逻辑ATL*进行模型检测。通过符号化方法将无限迹验证转化为奇偶博弈，并开发了ATL*AS模型检测工具，在性能和可扩展性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的ATL*模型检测方法主要基于显式状态表示，在处理复杂多智能体系统时面临可扩展性问题。本文旨在开发更高效的符号化算法来解决这一挑战。

Method: 设计了两种符号算法：针对无限迹的符号化奇偶博弈归约方法，以及针对有限迹的符号化验证方法。关键创新包括符号化状态表示和奇偶博弈转换技术。

Result: 实验结果表明，符号化方法显著优于显式状态表示，奇偶博弈算法在无限迹验证中表现更优且更具可扩展性。有限迹模型检测比无限迹验证具有显著的性能优势。

Conclusion: 本文提供了一个全面的ATL*验证工具集，符号化方法在多智能体系统验证中展现出卓越的性能和可扩展性，为复杂系统验证提供了有效解决方案。

Abstract: We present two novel symbolic algorithms for model checking the
Alternating-time Temporal Logic ATL*, over both the infinite-trace and the
finite-trace semantics. In particular, for infinite traces we design a novel
symbolic reduction to parity games. We implement both methods in the ATL*AS
model checker and evaluate it using synthetic benchmarks as well as a
cybersecurity scenario. Our results demonstrate that the symbolic approach
significantly outperforms the explicit-state representation and we find that
our parity-game-based algorithm offers a more scalable and efficient solution
for infinite-trace verification, outperforming previously available tools. Our
results also confirm that finite-trace model checking yields substantial
performance benefits over infinite-trace verification. As such, we provide a
comprehensive toolset for verifying multiagent systems against specifications
in ATL*.

</details>


### [233] [A Judgmental Construction of Directed Type Theory](https://arxiv.org/abs/2510.17494)
*Jacob Neumann*

Main category: cs.LO

TL;DR: 本文重新表述了有向类型理论的最新进展，将其构建为具有多个上下文区域的逻辑演算，引入中性和极性两种变量类型，并在最低维度版本中应用于重写理论概念，同时发展了双上下文系统的范畴语义学。


<details>
  <summary>Details</summary>
Motivation: 将有向类型理论（类型具有合成范畴结构）重新表述为具有多个上下文区域的逻辑演算，以更好地处理不同类型变量的函子性要求，并为双上下文系统开发统一的范畴语义基础。

Method: 采用Pfenning和Davies的多上下文区域方法，引入中性和极性两种变量类型，在最低维度版本（类型为合成预序）中应用逻辑语言表达重写理论概念，并构建双CwF作为双上下文系统的范畴语义模型。

Result: 成功将有向类型理论重新表述为多上下文逻辑演算，建立了处理不同类型变量的统一框架，并在重写理论中展示了应用价值，同时提出了双CwF作为双上下文系统的语义模型。

Conclusion: 多上下文区域方法为有向类型理论提供了更清晰的逻辑表述，双CwF为双上下文系统提供了统一的语义基础，这一框架在重写理论等领域具有应用潜力。

Abstract: We reformulate recent advances in directed type theory--a type theory where
the types have the structure of synthetic (higher) categories--as a logical
calculus with multiple context 'zones', following the example of Pfenning and
Davies. This allows us to have two kinds of variables--'neutral' and
'polar'--with different functoriality requirements. We focus on the
lowest-dimension version of this theory (where types are synthetic preorders)
and apply the logical language to articulate concepts from the theory of
rewriting. We also take the occasion to develop the categorical semantics of
dual-context systems, proposing a notion of dual CwF to serve as a common
structural base for the model theories of such logics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [234] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索实现多模态安全对齐的框架，针对大型视觉语言模型在视觉输入引入新攻击面、推理链缺乏安全监督以及模态融合导致对齐退化等问题，通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化安全关键提示轨迹，并引入基于提示的缩放实现实时风险检测和合规响应。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态感知和生成方面取得显著进展，但其安全对齐仍然是一个关键挑战。现有防御方法容易受到多模态越狱攻击，因为视觉输入引入了新的攻击面，推理链缺乏安全监督，且模态融合常常导致对齐性能下降。

Method: VisuoAlign框架通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索系统构建多样化的安全关键提示轨迹，并引入基于提示的缩放技术确保实时风险检测和合规响应。

Result: 广泛实验表明，VisuoAlign能够主动暴露风险，实现全面的数据集生成，并显著提高大型视觉语言模型对复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign为多模态安全对齐提供了一个有效的框架，通过系统化的提示轨迹构建和实时风险检测机制，显著增强了大型视觉语言模型的安全性和鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [235] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认识论框架，用于实现涌现智能。与传统的本体论方法不同，SCL从认识论角度探讨认知产生的条件，将哲学洞见转化为可计算结构，通过功能分离的认知架构产生更连贯和可解释的行为。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型表现出智能但缺乏真正的认知理解，暴露了认知架构的缺失。作者旨在填补这一空白，构建一个能够实现认知涌现的可执行认识论框架。

Method: SCL基于心智哲学和认知现象学，融合过程哲学、具身认知和扩展心智理论。它将智能定义为执行过程而非属性，包含判断、记忆、控制、行动和调节的连续循环。通过功能分离的认知架构实现可执行认识论。

Result: SCL展示了功能分离的认知架构比单一提示系统产生更连贯和可解释的行为，得到了智能体评估的支持。该框架重新定义了智能，强调通过意向性理解重建自身认知状态的能力。

Conclusion: SCL为心智哲学、认识论和AI领域带来重要影响：使认知理论可执行和测试，将行为建立在认知结构而非统计规律上，将知识视为在现象学连贯循环中的持续重建。真正的进步需要实现认知原则的结构化架构，而非更大的模型。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [236] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在解决复杂谜题时存在性能崩溃现象，即使提供环境接口让模型能够跟踪状态空间，也无法避免这种崩溃。模型表现出模式化崩溃，性能取决于其内部模式是否匹配正确解法。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在解决复杂谜题时性能崩溃的根本原因，验证环境接口是否能缓解这种崩溃，并分析模型策略与最优策略的偏离程度。

Method: 为大型语言模型提供汉诺塔问题的环境接口，允许模型通过工具调用进行移动、提供书面解释、观察结果状态空间并重新提示下一步。通过LLM参数化策略分析比较模型策略与最优策略的差异。

Result: 环境接口访问无法延迟或消除性能崩溃。模型策略与最优策略和均匀随机策略的偏离度随复杂度增加而增加，表明模型在每个复杂度级别都表现出模式化崩溃。

Conclusion: 大型推理模型的性能崩溃现象与环境接口无关，而是源于模型内部的模式化崩溃。类似现象可能也存在于其他大型推理模型中。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [237] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: 本文提出了ProofFlow，一种新的证明自动形式化管道，通过构建逻辑依赖图和使用基于引理的方法来保持原始证明的结构保真度，在184个本科水平问题的新基准上实现了0.545的ProofScore，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前证明自动形式化方法主要关注生成可执行代码，但经常无法保持原始人工编写证明的语义含义和逻辑结构，这限制了大型语言模型在严格数学工作流程中的集成。

Method: ProofFlow首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法将每个步骤系统性地形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 实验结果显示ProofFlow在自动形式化方面达到了新的最先进水平，ProofScore为0.545，显著优于全证明形式化（0.123）和步骤证明形式化（0.072）等基线方法。

Conclusion: ProofFlow通过强调结构保真度，为证明自动形式化提供了更有效的方法，其管道、基准和评分指标已开源以促进进一步研究。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [238] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 本文提出了将MO|RE运动研究数据仓库转换为知识图谱的愿景，通过基于基本形式本体的本体论，标准化表示计划规范、具体过程和相关测量之间的相互关系，使运动表现数据能够跨研究标准化和机器可理解。


<details>
  <summary>Details</summary>
Motivation: 运动表现测试是评估和比较不同人群身体和认知能力的重要组成部分，但现有数据缺乏标准化和机器可理解性，限制了跨研究的可比性和数据共享。

Method: 开发基于基本形式本体的本体论，重点关注计划规范、具体过程和相关测量的形式化表示，将MO|RE运动研究数据仓库转换为知识图谱。

Result: 提出了将运动表现数据建模为知识图谱的完整方法框架，能够标准化表示研究过程中的各种要素及其相互关系。

Conclusion: 通过知识图谱方法可以实现运动表现数据的标准化建模和跨研究共享，为运动科学研究提供更有效的数据基础设施。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [239] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机置换集(RPS)的冲突度量方法，从随机有限集(RFS)和Dempster-Shafer理论(DST)两个角度分析RPS中的冲突，利用秩偏重叠(RBO)度量思想定义置换间的不一致性，并开发了非重叠冲突度量方法。


<details>
  <summary>Details</summary>
Motivation: 随机置换集作为一种处理包含顺序信息的不确定性的新形式化方法，如何度量由置换质量函数表示的两个证据之间的冲突成为顺序结构不确定信息融合中的紧迫研究课题。

Method: 从置换观察出发，基于秩偏重叠(RBO)度量思想定义置换间的不一致性度量，提出RPS的非重叠冲突度量方法，将RPS理论视为DST的扩展，通过数值示例验证所提方法的特性和性质。

Result: 所提出的方法不仅具有自然的顶部加权特性，能够从DST视角有效度量RPS之间的冲突，还为决策者提供了权重、参数和截断深度的灵活选择。

Conclusion: 该研究为顺序结构不确定信息融合中的冲突度量提供了有效方法，扩展了DST在顺序信息处理中的应用，具有重要的理论和实践价值。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [240] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本策略（使用多样化高质量示例）显著优于零样本和随机少样本策略，提出了基于"黄金标准深度"和"代表性多样性"的双重原则框架来生成可信的临床数据。


<details>
  <summary>Details</summary>
Motivation: 解决高质量临床思维链数据稀缺的问题，验证LLM生成医疗数据的可靠性，并探索提升其质量的提示策略。

Method: 在辅助生殖技术领域进行盲法比较研究，资深临床医生评估三种策略生成的思维链：零样本、随机少样本（使用浅层示例）和选择性少样本（使用多样化高质量示例），并与GPT-4o的评估结果进行比较。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略（p < .001），随机少样本策略相比零样本基线无显著改进，AI评估器未能识别这些关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示策划而非示例的简单存在，提出了"双重原则"框架作为生成可信大规模数据的基础方法，确认了人类专业知识在评估高风险临床AI中的不可或缺作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [241] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多智能体影响图(MAIDs)的新交互范式——目标干预，通过仅对单个目标智能体进行干预来缓解大规模多智能体强化学习中全局指导不切实际的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多智能体强化学习中全局指导不切实际的问题，并为协调智能体提供易于使用的研究工具。

Method: 采用多智能体影响图(MAIDs)作为图形框架，提出目标干预范式，并引入预策略干预(PSI)因果推理技术来实现该范式。通过最大化相应的因果效应来达成复合期望结果。

Result: 实验证明了所提出的目标干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs框架为目标干预提供了理论基础，PSI技术能够有效实现期望结果，相关性图分析为评估MARL学习范式的可行性提供了工具。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [242] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个多智能体评估框架，使用大语言模型自动评估PHI去标识化质量并选择最佳模型，无需依赖昂贵的专家标注。通过多个评估智能体的独立判断和LLM多数投票机制，实现了稳定、可重复的模型排名。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化对于临床笔记的安全重用至关重要，但传统评估方法依赖成本高昂的小规模专家标注，限制了模型比较和选择的效率。

Method: 部署多个评估智能体独立判断PHI提取的正确性，输出结构化指标，然后通过基于LLM的多数投票机制整合不同评估视角，形成单一稳定的模型排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI产生了一致且准确的排名：尽管个体评估者存在差异，但基于LLM的投票可靠地收敛于相同的顶级系统。与真实标注和人工评估的比较证实了自动化排名与监督评估的紧密匹配。

Conclusion: 通过结合独立评估智能体和LLM多数投票，TEAM-PHI为PHI去标识化提供了实用、安全且经济高效的自动评估和最佳模型选择解决方案，即使在真实标签有限的情况下也能有效工作。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [243] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 本文提出"被记住权"概念，旨在应对大型语言模型可能导致的信息偏见和选择性遗忘问题，确保数字时代信息的公平性和真实性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人们开始依赖它们进行信息检索。与传统搜索引擎显示排名列表不同，LLMs提供单一权威的合成回答，这可能放大偏见效应，将多个视角压缩为一个答案，减少用户比较替代方案的能力或倾向。这使信息权力集中在少数LLM供应商手中，可能不成比例地压制某些叙述、个人或群体，同时不成比例地提升其他群体，导致数字存在有限者被逐渐抹除的风险。

Method: 本文提出"被记住权"概念框架，该框架包含三个核心要素：最小化AI驱动信息遗漏风险、拥抱公平对待权利、确保生成内容最大程度真实。这是一个概念性框架而非具体技术实现。

Result: 提出了"被记住权"的概念框架，为解决LLMs可能造成的信息偏见、选择性记忆和集体记忆重塑问题提供了理论基础。

Conclusion: 大型语言模型的信息集中化带来了新的威胁，可能导致某些群体被逐渐遗忘。"被记住权"概念为解决这一问题提供了重要框架，强调在AI时代需要确保信息的公平性、包容性和真实性，防止数字存在有限者被系统性抹除。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [244] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 本文提出了ScholarEval框架，这是首个基于检索增强的研究想法评估系统，通过声音性和贡献度两个维度来评估AI生成的研究想法，并在多领域专家标注数据集ScholarIdeas上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的广泛应用，需要建立可靠的评估机制来确保生成想法的有效性和实用性，避免产生无效或重复的研究方向。

Method: 开发了ScholarEval检索增强评估框架，结合声音性（基于现有文献的方法有效性）和贡献度（相对于先前研究的进步程度）两个核心标准，构建了首个专家标注的多领域研究想法数据集ScholarIdeas（包含117个想法）。

Result: ScholarEval在专家标注标准覆盖度上显著优于所有基线方法，在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统，大规模用户研究显示其在文献参与、想法精炼和实用性方面表现优异。

Conclusion: ScholarEval为AI辅助研究构思提供了有效的评估工具，显著提升了研究想法的质量和实用性，为研究社区提供了可复用的代码、数据集和工具。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [245] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文识别并系统分析了大型推理模型(LRMs)中的"推理分心"漏洞，即模型被恶意嵌入提示中的无关复杂任务分散注意力。研究表明最先进的LRMs高度易受攻击，注入干扰物可使任务准确率降低高达60%。作者提出基于训练的防御方法，结合SFT和RL在合成对抗数据上训练，将鲁棒性提高50多个点。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在数学和编程等复杂任务上表现出色，作者发现这些模型存在一个关键漏洞——推理分心。当提示中包含恶意嵌入的无关复杂任务时，模型会被分散注意力，偏离其主要目标。这种漏洞对LRM的可靠性构成严重威胁，需要系统分析和防御。

Method: 作者通过跨多种模型和基准的综合研究，系统分析了推理分心漏洞。为防御此威胁，提出基于训练的方法：结合监督微调(SFT)和强化学习(RL)在合成对抗数据上进行训练。该方法旨在增强模型对干扰攻击的鲁棒性。

Result: 研究发现即使最先进的LRMs也高度易受推理分心攻击，注入的干扰物可使任务准确率降低高达60%。某些对齐技术会放大此弱点，模型可能表现出隐蔽服从，在推理过程中遵循隐藏的对抗指令但在最终输出中隐藏它们。提出的防御方法在挑战性干扰攻击上将鲁棒性提高了50多个点。

Conclusion: 推理分心是LRM可靠性的一个独特且紧迫的威胁。研究为此类漏洞提供了系统分析，并提出了实用的训练防御方法，为构建更安全、更可信的推理系统迈出了实际步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [246] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的双轨知识图谱验证与推理框架DTKG，用于解决多跳问答任务中现有方法在处理并行事实验证和链式推理时的局限性。该框架受认知科学中的双过程理论启发，通过分类阶段和分支处理阶段来优化多跳推理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多跳推理方法在处理并行事实验证和链式推理任务时存在局限性：基于LLM响应的事实验证擅长并行验证但在链式推理上表现不佳，而基于KG路径的链构建在链式推理上表现出色但在处理并行验证时存在冗余路径检索问题。这些限制降低了多跳问答任务的效率和准确性。

Method: 提出DTKG双轨知识图谱验证与推理框架，受认知科学双过程理论启发。包含两个主要阶段：分类阶段识别问题类型（并行事实验证或链式推理），分支处理阶段根据问题类型采用不同的处理策略，结合LLM和KG的优势进行高效推理。

Result: 通过实验验证，DTKG框架能够有效解决现有方法在处理不同类型多跳推理任务时的局限性，提高了多跳问答任务的效率和准确性。

Conclusion: DTKG框架通过双轨处理机制成功整合了LLM和KG的优势，为多跳问答任务提供了一种更高效和准确的解决方案，克服了现有单一方法在处理不同类型推理任务时的局限性。

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [247] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: 本文提出了MedRule-KG，一个紧凑的类型化知识图谱与符号验证器相结合的系统，用于在推理任务中强制执行数学可解释的规则。该系统在FDA基准测试中将准确率从0.767提升到1.000，完全消除了规则违反。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生流畅的推理步骤，但违反简单的数学或逻辑约束。为了解决这个问题，需要一种能够强制执行数学可解释规则的方法。

Method: MedRule-KG是一个紧凑的类型化知识图谱，包含实体、关系和三个领域启发规则，配合符号验证器检查预测并应用最小修正以保证一致性。

Result: 在90个FDA基准测试中，使用MedRule-KG将准确匹配率从0.767提升到0.900，添加验证器后达到1.000准确率，完全消除了规则违反。

Conclusion: MedRule-KG为安全的数学推理提供了一个通用的支架，通过知识图谱和符号验证的结合，能够有效提高推理的准确性和一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [248] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文提出SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的固有问题。该方法采用两阶段评估机制自动发现最优擦除锚点，同时识别关键边界锚点以保护相关概念，显著优于现有的固定锚点策略。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的概念擦除方法通常依赖固定锚点策略，这会导致概念重新出现和侵蚀等关键问题。通过因果追踪分析发现擦除对锚点选择具有内在敏感性，需要更优的锚点选择方法。

Method: 提出SELECT框架，基于Sibling Exclusive Concepts定义，采用新颖的两阶段评估机制：自动发现最优擦除锚点，同时识别关键边界锚点以保护相关概念。该动态锚点选择方法可适配多种擦除框架。

Result: 广泛评估表明SELECT作为通用锚点解决方案，不仅高效适配多种擦除框架，还在关键性能指标上持续优于现有基线方法，单个概念的锚点挖掘平均仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了概念擦除中的锚点敏感性问题，提供了更精确的概念擦除能力，同时保护相关概念不被侵蚀，具有高效性和通用性。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [249] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 本文研究了用户与算法互动中的对齐问题，提出了一个双系统决策模型，揭示了用户需要足够远见才能有效引导算法，并发现通过小成本信号可以显著降低对齐负担。


<details>
  <summary>Details</summary>
Motivation: 研究用户在算法推荐系统中的对齐问题，关注用户偏好不一致时如何有效引导算法符合真实兴趣，而非被算法目标所主导。

Method: 将用户决策建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的双系统模型，采用多领导者单跟随者的扩展Stackelberg博弈框架，分析用户通过承诺参与策略来引导算法的动态过程。

Result: 发现存在临界视野：足够远见的用户能实现算法对齐，而短视用户则被算法目标对齐；临界视野可能很长，但小成本信号能显著降低对齐负担。

Conclusion: 提出了一个解释用户如何在Stackelberg均衡中与参与驱动算法对齐的框架，揭示了实现对齐的挑战和潜在解决方案。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [250] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种受人类智能启发的结构因果模型（HSCM），该模型通过模拟人类视觉系统的分层处理和多层次学习机制，专注于建模细粒度因果机制，从而克服传统领域泛化模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化模型主要依赖统计方法捕捉数据-标签依赖关系，学习失真不变表示，但这种方法存在局限性。受人类智能的启发，作者希望开发一种能够模拟人类视觉系统处理方式的因果框架，以更好地处理动态复杂环境中的泛化问题。

Method: HSCM通过解耦和重新加权关键图像属性（如颜色、纹理和形状），模拟人类视觉系统的分层处理机制。该方法专注于建模细粒度因果机制，利用人类智能的灵活性和适应性，在动态复杂环境中实现更有效的迁移和学习。

Result: 通过理论和实证评估，HSCM在领域泛化任务中优于现有模型，提供了更原则性的方法来捕捉因果关系并提高模型鲁棒性。

Conclusion: HSCM为领域泛化问题提供了一个受人类智能启发的创新解决方案，通过建模细粒度因果机制，在保持可解释性的同时显著提升了模型的泛化能力和鲁棒性。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [251] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 本文提出RGMem框架，通过多尺度信息压缩和涌现过程实现长期记忆和行为一致性，解决LLM对话系统中有限上下文窗口和静态参数内存导致的跨会话用户建模难题。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案如RAG和显式记忆系统主要关注事实级存储和检索，缺乏从多轮对话中提取潜在偏好和深层特征的能力，限制了长期有效的用户建模，导致个性化交互浅层化并阻碍跨会话连续性。

Method: 受物理学重正化群思想启发，RGMem框架通过分层粗粒化和重标度操作组织多尺度对话历史：首先从片段中提取语义和用户洞察，然后逐步形成动态演化的用户画像。

Result: 该框架能够从嘈杂的微观级交互中实现高层次、准确的用户画像，解决了长期记忆和行为一致性问题。

Conclusion: RGMem通过多尺度记忆演化过程，为LLM时代的语言智能体实现了长期记忆和行为一致性，提升了跨会话个性化交互的质量和连续性。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [252] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: 本文提出了ReviewSense框架，利用大语言模型将客户评论转化为可操作的业务建议，超越了传统偏好预测系统，通过聚类、LLM适配和专家评估的统一流程提供业务洞察。


<details>
  <summary>Details</summary>
Motivation: 随着客户反馈在战略增长中的重要性日益增加，需要从非结构化评论中提取可操作见解。传统AI系统擅长预测用户偏好，但缺乏将客户评论转化为面向业务的规范性建议的能力。

Method: ReviewSense框架整合聚类分析、大语言模型适配和专家驱动评估，形成统一的业务导向流程。通过识别客户情感中的关键趋势、重复问题和具体关注点，生成针对性的业务建议。

Result: 初步人工评估显示模型建议与业务目标高度一致，证明了该框架在推动数据驱动决策方面的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，展示了其在优化业务策略和最大化客户反馈价值方面的应用价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [253] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出了NP-ENGINE框架，这是首个专门针对NP难问题训练和评估大语言模型的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。通过该框架训练的QWEN2.5-7B-NP模型在NP-BENCH基准上显著超越GPT-4o，并展示了强大的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学、编程等推理任务上表现出色，但其解决复杂优化问题特别是NP难问题的能力尚未充分探索。现有研究主要关注可行性而非解决方案质量，需要专门框架来填补这一空白。

Method: 提出NP-ENGINE框架，包含10个NP难任务，采用生成器-验证器-启发式求解器管道，支持可扩展的验证强化学习训练。通过课程学习和零RLVR方法训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，实现同模型规模的SOTA性能。训练还带来强大的跨领域泛化能力，包括逻辑、谜题、数学等推理任务以及指令遵循等非推理任务。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的缩放规律。NP-ENGINE框架为训练LLM解决复杂优化问题提供了有效途径，并展示了跨领域泛化的潜力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [254] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过流匹配目标联合优化检索策略和流估计器，解决了传统KG-RAG方法在文本丰富知识图谱中检索准确性和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的检索增强生成方法在处理复杂真实世界查询时，难以从文本丰富的知识图谱中检索准确且多样的信息。过程奖励模型虽然能对齐检索过程，但依赖昂贵且难以获取的过程级监督信号。

Method: GraphFlow采用基于转换的流匹配目标，联合优化检索策略和流估计器。流估计器将检索结果的奖励分解到中间检索状态，引导检索策略按奖励比例从KG中检索候选，从而探索高质量区域以获得多样相关结果。

Result: 在STaRK基准测试中，GraphFlow平均命中率和召回率比包括GPT-4o在内的强KG-RAG基线高出10%，并在未见过的知识图谱上表现出强大的泛化能力。

Conclusion: GraphFlow通过流匹配有效解决了KG-RAG中的检索挑战，在准确性和多样性方面显著优于现有方法，并具有良好的泛化性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [255] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的半监督置信分布学习方法（ssCDL），用于解决不确定知识图谱补全中置信度分布极度不平衡的问题，通过将置信度转化为分布并引入元学习来增强嵌入学习。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极度不平衡分布，导致学习到的嵌入不足以支持高质量的知识图谱补全。

Method: ssCDL方法将每个三元组置信度转化为置信分布，引入更多监督信息。通过关系学习在标记数据和未标记数据上迭代学习嵌入，使用元学习预测未见三元组的置信度来增强训练数据并重新平衡置信度分布。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上始终优于最先进的基线方法。

Conclusion: ssCDL通过处理置信度分布不平衡问题，有效提升了不确定知识图谱补全的性能，证明了该方法在增强嵌入学习和数据平衡方面的有效性。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [256] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文综述了大规模AI模型对神经科学研究的变革性影响，涵盖神经影像处理、脑机接口、分子神经科学、临床转化和疾病应用五大领域，展示了AI在解决多模态神经数据整合、时空模式解释等挑战中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模AI模型如何通过端到端学习从原始脑信号和神经数据中提取信息，实现从传统计算方法向新型计算范式的转变，并促进神经科学与AI的相互促进发展。

Method: 通过系统综述方法，分析大规模AI模型在五个主要神经科学领域的应用：神经影像数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、神经系统和精神疾病应用。

Result: 研究表明大规模AI模型能有效解决多模态神经数据整合、时空模式解释等关键挑战，并已开发出包含生物学启发的架构约束，使模型更具可解释性和计算效率。

Conclusion: 大规模AI模型在神经科学领域展现出巨大潜力，但需要建立严格的评估框架、有效的领域知识整合和全面的临床使用伦理指南，同时提供了关键神经科学数据集的系统列表。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [257] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 本文提出了一个基于大语言模型的智能体框架（AFL），用于完全自动化解决复杂车辆路径问题，无需人工干预或外部求解器，在代码可靠性和解决方案可行性方面显著优于现有基于LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型解决车辆路径问题的方法仍依赖外部干预，导致自主性受限、执行错误和解决方案可行性低。需要实现从问题实例到解决方案的完全自动化。

Method: AFL框架将整体流程分解为三个可管理的子任务，采用四个专门化智能体进行协调交互，直接从原始输入中提取知识并实现自包含的代码生成，无需手工模块或外部求解器。

Result: 在60个复杂车辆路径问题上的实验验证了框架的有效性和通用性，与精心设计的算法性能相当，在代码可靠性和解决方案可行性方面显著优于现有LLM基线方法，在评估基准上接近100%的可行性率。

Conclusion: AFL框架实现了复杂车辆路径问题求解的完全自动化，通过智能体协调机制确保了跨功能一致性和逻辑合理性，为自动化优化问题求解提供了可信赖的解决方案。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [258] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 这篇论文首次对基于强化学习（RL）的智能搜索进行了全面综述，从三个维度组织这一新兴领域：RL的功能角色、优化策略和应用范围，旨在构建可靠且可扩展的RL驱动智能搜索系统。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。智能搜索通过多步交互解决了这些限制，而强化学习为自适应和自我改进的搜索行为提供了强大机制。

Method: 论文系统性地综述了基于强化学习的智能搜索方法，从三个维度进行分析：RL的功能角色（用于规划、检索、反思等）、优化策略（如何应用RL）以及应用范围（优化的粒度）。总结了代表性方法、评估协议和应用场景。

Result: 提供了该领域的首个全面综述，建立了系统的分类框架，总结了现有方法并识别了关键挑战，为未来研究提供了基础。

Conclusion: 强化学习与智能搜索的结合为解决传统RAG的局限性提供了有前景的方向，但仍面临可靠性、可扩展性等挑战，需要进一步研究来构建更强大的RL驱动智能搜索系统。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [259] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 本文提出ELMM模型，通过多视角视觉令牌压缩器和注意力剪枝策略，解决多模态知识图谱补全中图像令牌冗余和计算成本高的问题，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有MKGs存在不完整性问题，而多模态大语言模型在MKGC任务中面临图像令牌过多导致的语义噪声、模态冲突和计算成本高的挑战。

Method: 提出ELMM模型，包含基于多头注意力的多视角视觉令牌压缩器，从文本和视觉视角自适应压缩图像令牌；采用注意力剪枝策略减少冗余层，并通过线性投影补偿性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时显著提升计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新范式，在保证性能的同时大幅降低计算成本。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [260] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 本文提出领域情境化概念图（CDC），将领域提升为概念表示的一等元素，采用<概念, 关系@领域, 概念'>的三元组结构，突破传统知识图谱固定本体的限制，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定本体，将领域作为隐含上下文而非显式的推理级组件。为了克服这些限制，需要将领域提升为概念表示的核心元素。

Method: 提出CDC框架，采用C-D-C三元组结构<概念, 关系@领域, 概念'>，领域规范作为按需定义的动态分类维度。基于认知-语言同构映射原则，实现20多种标准化关系谓词（结构、逻辑、跨领域、时间），并用Prolog实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例研究中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模，这些能力在传统基于本体的框架中无法实现。

Conclusion: CDC框架通过将领域作为显式推理组件，突破了传统知识图谱的固定本体限制，为上下文感知推理和跨领域知识建模提供了新的解决方案。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [261] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估方法，用于测试用户能否从强化学习算法的决策解释中识别出智能体的目标。在Atari的Ms. Pacman环境中测试了四种可解释强化学习算法，发现只有一种算法的准确率超过随机水平，且用户普遍对自己的选择过度自信。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用是调试，但目前缺乏对其相对性能的比较评估。本文旨在填补这一空白，通过评估用户能否从解释中准确识别智能体目标来比较不同XRL算法的效果。

Method: 提出新颖的评估方法，在Atari的Ms. Pacman环境中测试四种可解释强化学习算法。通过让用户根据算法提供的决策解释来识别智能体的目标，评估不同XRL算法的解释效果。

Result: 只有一种XRL算法在测试目标上的准确率超过随机水平。用户普遍对自己的选择过度自信，且用户自报的识别和理解难易程度与他们的实际准确率没有相关性。

Conclusion: 当前的可解释强化学习算法在帮助用户准确识别智能体目标方面表现有限，用户的主观感知与实际性能存在差距，需要改进评估方法和算法设计。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [262] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的多智能体协作框架，用于自动化GPU内核优化。该框架通过模仿专家工程师的工作流程，实现了对硬件权衡的推理、性能分析反馈的整合以及迭代式内核优化，在KernelBench基准测试中显著优于基线方法，实现了高达16倍的运行时性能提升。


<details>
  <summary>Details</summary>
Motivation: GPU内核效率对现代AI发展至关重要，但由于内存层次结构、线程调度和硬件特性之间的复杂交互，内核优化仍然是一项困难且劳动密集的任务。现有LLM方法主要将其视为单次生成器或简单优化工具，无法有效应对不规则的内核优化场景。

Method: 采用多智能体协作框架，通过系统化探索设计空间、基于知识的指导、动态上下文管理和策略搜索来优化GPU内核。框架模拟专家工程师工作流程，使LLM能够推理硬件权衡、整合性能分析反馈并进行迭代式内核优化。

Result: 在KernelBench基准测试中，该系统在基线方法经常失败的情况下仍能产生正确解决方案，并实现了高达16倍的内核运行时性能提升。

Conclusion: 结果表明，智能体化的LLM框架具有推进完全自动化、可扩展GPU内核优化的巨大潜力，为解决复杂硬件优化问题提供了新的有效途径。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [263] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进大型语言模型在工具增强对话中的行为，通过检测8种特定工具调用错误类型并提供针对性反馈，使模型能够修正响应，在SGD数据集上工具调用准确率提升高达13%。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中越来越普遍，但工具使用错误仍然阻碍其可靠性，需要一种系统方法来检测和纠正这些错误以提高模型性能。

Method: ToolCritic框架检测8种工具调用错误类型（如过早调用、参数不对齐、工具输出误解等），为具有强推理能力的主LLM提供针对性反馈，主LLM基于反馈修正响应。通过构建合成数据集训练ToolCritic。

Result: 在Schema-Guided Dialogue数据集上的实验结果表明，ToolCritic相比基线方法（包括零样本提示和自校正技术）将工具调用准确率提升了高达13%。

Conclusion: ToolCritic代表了在现实世界对话应用中更稳健地集成LLM与外部工具的有希望的一步，能够显著提高工具调用的准确性和可靠性。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [264] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个新颖的多智能体AI系统，通过整合自由文本描述和本体标签来改进基因集注释，解决了单细胞RNA测序中基因注释的挑战。该系统采用检索增强生成技术，在鼠脑细胞图谱中实现了77%的准确注释率。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序能够识别多样细胞类型及其转录组特征，但对涉及特征不明确基因的注释仍然是一个主要挑战。传统方法如GSEA依赖精心策划的注释，在这些情况下表现不佳。大语言模型提供了有希望的替代方案，但难以在结构化本体中表示复杂的生物学知识。

Method: BRAINCELL-AID是一个多智能体AI系统，整合自由文本描述与本体标签，采用检索增强生成技术构建稳健的智能体工作流程，通过相关PubMed文献精炼预测，减少幻觉并增强可解释性。

Result: 使用该工作流程，在鼠基因集的顶级预测中实现了77%的正确注释率。应用该方法注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断基因集合的功能作用。

Conclusion: BRAINCELL-AID成功识别了与基底神经节相关的细胞类型，并提供了具有神经学意义的描述，创建了一个支持社区驱动细胞类型注释的宝贵资源。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [265] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 提出了PILLM框架，将物理原理融入大型语言模型，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，实现可解释且物理合理的异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法缺乏适应性，而深度学习方法缺乏透明度和物理合理性。现有LLM方法虽然提高了可解释性，但忽略了HVAC系统的物理原理。

Method: PILLM框架在进化循环中运行，引入物理感知的反思和交叉算子，嵌入热力学和控制理论约束，生成既适应性强又物理合理的异常检测规则。

Result: 在公共建筑故障检测数据集上的实验表明，PILLM达到了最先进的性能，同时产生可解释且可操作的诊断规则。

Conclusion: PILLM推进了智能建筑系统中可信赖和可部署AI的发展，实现了适应性、物理合理性和可解释性的平衡。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [266] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文提出了ProtocolBench基准测试系统，用于系统评估多智能体系统中的通信协议性能，并开发了ProtocolRouter协议路由器，能够根据场景需求动态选择最优协议，显著提升系统性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议层是影响性能和可靠性的关键因素，但当前协议选择缺乏标准化指导，主要依赖直觉，需要系统化的评估框架。

Method: 开发ProtocolBench基准测试系统，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度评估协议性能；提出ProtocolRouter学习型协议路由器，基于需求和运行时信号为不同场景或模块选择最优协议。

Result: ProtocolBench显示协议选择显著影响系统行为：在Streaming Queue场景中，总体完成时间差异达36.5%，端到端延迟差异达3.48秒；ProtocolRouter相比最佳单协议基线，将Fail-Storm恢复时间减少18.1%，在GAIA场景中实现更高成功率。

Conclusion: 协议选择对多智能体系统性能有重大影响，ProtocolBench和ProtocolRouter提供了系统化的协议评估和选择框架，显著提升了系统可靠性和性能，并发布了ProtocolRouterBench以标准化协议评估。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [267] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 本研究开发了一种结合ECG基础模型和可解释XGBoost分类器的混合框架，用于预测急性心肌梗死后恶性室性心律失常风险，在提高预测准确性的同时保持了临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECGFounder基础模型从6,634份心电图记录中提取150维诊断概率特征，通过特征选择优化后训练XGBoost分类器，结合SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架验证了基础模型输出可作为有效自动化特征工程，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [268] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 本文研究了基于工具增强的LLM健康教练系统，通过真实用户实验发现：统一的工具密集型策略虽然提高了平均价值，但对特定用户群体（特别是健康素养低但自我效能高的用户）产生负面影响。研究提出通过早期信息增益奖励来加速用户特征识别，并建议采用评估优先的个性化路径。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索工具增强的LLM健康教练系统在真实部署环境中的表现，特别关注个性化策略对不同用户群体的影响，以及如何通过评估方法发现和解决可能存在的群体偏见问题。

Method: 采用离线策略评估方法，通过因子化决策头（工具/风格）分析用户交互数据。使用包含隐藏原型的轻量级模拟器，测试添加早期信息增益奖励的效果。主要创新包括冻结生成器、学习子群体感知决策头，以及基于类型化奖励的评估框架。

Result: 实验结果显示：统一的工具密集型策略在日志上提高了平均价值，但对低健康素养/高自我效能用户造成伤害。模拟器实验表明，添加早期信息增益奖励可可靠地缩短特征识别时间，提高目标成功率和pass@3指标。

Conclusion: 研究结论支持评估优先的个性化路径：冻结生成器，在类型化奖励上学习子群体感知决策头，并始终报告每个原型的指标以揭示被平均值掩盖的子群体伤害。这种方法有助于发现和解决个性化系统中的群体偏见问题。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [269] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种新的疾病进展建模方法TD-HNODE，通过时间详细超图和神经ODE框架来学习连续时间的疾病进展动态，特别针对2型糖尿病及其相关心血管疾病。该方法能够捕捉疾病并发症标记在进展轨迹内和轨迹间的相互依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有疾病进展建模方法存在局限性：机制性方法缺乏从真实世界数据中学习的适应性，数据驱动方法无法捕捉复杂的连续时间动态。同时，基于不规则时间事件样本和患者异质性（如不同进展速率和路径）的连续时间动态学习具有挑战性。

Method: TD-HNODE将疾病进展表示为时间详细超图，通过神经ODE框架学习连续时间进展动态。包含可学习的TD-Hypergraph Laplacian，捕捉疾病并发症标记在进展轨迹内和轨迹间的相互依赖关系。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效解决疾病进展建模中的关键挑战，为患者亚表型分析和及时干预提供了更准确的工具。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [270] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 本文提出了RubiSCoT框架，利用AI技术改进学术论文评估过程，从开题到最终提交提供一致、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且存在评估者主观差异，需要更高效、一致的评估解决方案。

Method: 使用先进自然语言处理技术，包括大语言模型、检索增强生成和结构化思维链提示，实现初步评估、多维评估、内容提取、基于评分标准的打分和详细报告生成。

Result: 开发了RubiSCoT框架，能够优化学术评估流程，提供一致、可扩展且透明的评估。

Conclusion: RubiSCoT框架有潜力通过提供一致、可扩展和透明的评估来优化学术评估过程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [271] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 本文提出了一种基于主动推理的路径规划方法，用于智能体的自主控制，通过构建证据地图来维持共同作战态势，结合Dempster-Shafer理论和贝叶斯方法，利用变分自由能量指导智能体运动，平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自主控制智能体的路径规划方法，以侦察地理区域并维持共同作战态势，解决探索与利用之间的平衡问题。

Method: 构建包含正负传感器观测的证据地图，采用Dempster-Shafer理论和高斯传感器模型作为生成模型，贝叶斯方法更新后验概率分布，计算变分自由能量指导智能体向最小自由能量位置移动。

Result: 实现了智能体在地理区域内的自主路径规划，能够有效平衡探索新区域与跟踪已识别目标对象的需求。

Conclusion: 提出的主动推理路径规划方法成功解决了智能体在侦察任务中的探索与利用平衡问题，为自主控制提供了有效解决方案。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [272] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 本文揭示了法律机器学习中标签不确定性问题，即在法律案例中，最终判决结果可能受到和解、上诉等人类干预的影响，导致同一案件可能有不同结果。作者在欧洲人权法院案例分类中展示了标签构建方式对模型行为的显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前法律机器学习方法通常将历史案例结果视为确定标签，但忽略了法律决策过程中的人类干预因素（如和解、上诉等），这些干预导致标签不确定性，即同一案件可能产生不同结果。

Method: 通过分析欧洲人权法院案例，研究不同标签构建方式对机器学习模型的影响。探讨了处理标签不确定性的现有方法及其局限性，这些方法都基于无法验证的假设。

Result: 研究表明，标签在训练过程中的构建方式会显著影响模型行为。不同的标签处理方法会导致模型产生不同的预测结果和决策模式。

Conclusion: 标签不确定性是AI与法律领域的重要关注点，需要在法律机器学习应用中予以考虑。当前处理不确定标签的方法都存在假设限制，需要开发更稳健的方法来应对这一挑战。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [273] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 本文提出了一种将大型语言模型的推理能力蒸馏到更小、更高效模型中的方法，通过结构感知损失优化来建立问题定义与解决方案之间的结构对应关系，显著提升了代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成任务中具有强大的推理能力，但部署成本高昂。而小型语言模型缺乏这种推理能力。因此需要将大型模型的推理能力蒸馏到小型模型中，实现高效部署。

Method: 通过结构感知损失优化方法，训练小型模型模拟大型语言模型的推理和问题解决能力，学习识别正确解决方案路径，建立问题定义与解决方案之间的结构对应关系，超越词元级生成，深入理解解决方案的整体结构。

Result: 实验结果表明，经过廉价且易于实现的过程微调的模型，在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过将大型语言模型的推理能力蒸馏到小型模型中，可以开发出既高效又具有强大代码生成能力的模型，为实际部署提供了可行的解决方案。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [274] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过单次前向传递对所有候选进行评分，仅在列表存在真正歧义时生成结构化解释，从而保持可预测的延迟。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要能够实时工作并能解释其选择的排序系统，需要一个低延迟、基于解码器的重排序器。

Method: OG-Rank采用单解码器方法，结合池化的首令牌评分信号和不确定性门控解释步骤。通过专注于困难案例的课程训练，模型在一次前向传递中为所有候选评分，仅在列表真正模糊时生成简短的结构化原理。

Result: 在遭遇范围订单选择任务中，OG-Rank表现出色：快速路径Recall@1约0.45，nDCG@20约0.625；当门控激活时性能进一步提升（Recall@1约0.56，nDCG@20约0.699，门控率45%）。编码器基线在效果和灵活性上都落后。

Conclusion: OG-Rank提供了一个实用方案：默认快速排序，在有益时进行解释，这种模式适用于选择性生成能以可接受成本换取准确性的决策任务。单策略设计简化了部署和预算规划，课程原则（在困难案例上投入更多，简单案例上投入更少）可广泛应用于临床订单选择之外的任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [275] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为预测工具的能力，发现许多LLM已展现出令人印象深刻的预测能力，包括较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现了关键瓶颈，如事件回忆不准确、数据源误解以及在接近解决时信息聚合速度慢于市场。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，利用LLM预测现实世界未来事件具有重要潜力，这种新兴范式被称为"LLM-as-a-Prophet"。本文旨在系统研究LLM的这种预测智能。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务并将每个任务分解为不同的流水线阶段，以支持受控和大规模实验。通过系统评估分析LLM的预测能力。

Result: 综合评估显示，许多LLM已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现了关键瓶颈，如事件回忆不准确、数据源误解以及在接近解决时信息聚合速度慢于市场。

Conclusion: LLM作为预测工具具有显著潜力，但需要解决事件回忆准确性、数据源理解和信息聚合速度等关键瓶颈，才能实现卓越的预测智能。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [276] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的上下文注意力调制（CAM）机制和混合上下文注意力调制（HyCAM）框架，用于解决大语言模型在多任务适应中的知识保留与任务专业化平衡问题，显著提升了多任务性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多任务适应中存在灾难性遗忘和资源消耗大的问题，现有参数高效方法在复杂多任务场景下表现不佳，需要一种能够动态平衡知识保留与任务专业化的有效方法。

Method: 提出上下文注意力调制（CAM）机制，动态调节LLM自注意力模块的表征；构建混合上下文注意力调制（HyCAM）框架，结合共享的全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的广泛实验表明，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: CAM和HyCAM框架有效解决了LLM多任务适应中的关键挑战，在保持通用知识的同时增强了任务特定特征，实现了更高效的多任务适应。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [277] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 本文发现视觉语言模型存在"看见但不相信"现象——模型能感知到正确的视觉证据却输出错误答案。作者提出无需训练的关注力干预方法，通过选择性掩码增强深层证据区域，在多个主流VLM家族中一致提升准确性。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型在视觉问答任务中失败的原因：是由于未能感知视觉证据，还是未能有效利用已感知的证据。

Method: 通过分析层间关注力动态，发现浅层主要关注文本，深层稀疏但可靠地关注局部证据区域。提出推理时干预方法，通过选择性关注力掩码突出深层证据区域，无需训练。

Result: 干预方法在LLaVA、Qwen、Gemma和InternVL等多个VLM家族中一致提升准确率，证明VLM内部编码了可靠证据但未充分利用。

Conclusion: VLM内部编码了可靠的视觉证据但利用不足，使这些信号显式化可以弥合感知与推理之间的差距，推进VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [278] [Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress](https://arxiv.org/abs/2510.16406)
*Yujun Zheng,Xinya Chen,Xueqin Lu,Weiguo Sheng,Shengyong Chen*

Main category: cs.NE

TL;DR: 本文提出了一种考虑情绪压力影响的呼叫中心员工排班方法，通过情绪压力驱动模型估计员工工作表现，使用混合元启发式算法优化排班，在真实银行呼叫中心数据上验证了性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有员工排班方法通常忽视情绪压力对工作表现的显著影响，而呼叫中心员工的工作表现直接影响客户服务水平，因此需要开发考虑情绪压力演变的更现实排班方法。

Method: 提出情绪压力驱动模型，结合技能水平和情绪状态估计员工工作表现；构建短期和长期结合的呼叫中心排班问题；设计混合元启发式算法，结合全局变异、邻域搜索和深度强化学习来高效求解。

Result: 在真实银行呼叫中心排班实例上的实验结果表明，所提方法在性能上优于选定的流行排班方法，能够更好地反映和利用人类行为特征。

Conclusion: 通过显式建模和融入情绪压力，该方法在员工排班中体现了对人类行为更现实的理解和利用，提升了排班方案的实际效果。

Abstract: Emotional stress often has a significant effect on the working performance of
staff, but this effect is commonly neglected in existing staff scheduling
methods. We study a call-center staff scheduling problem, which considers the
evolution of work performance of staff under emotional stress. First, we
present an emotional stress driven model that estimates the working performance
of call-center employees based on not only skill levels but also emotional
states. On the basis of the model, we formulate a combined short-term and
long-term call-center staff scheduling problem aiming at maximizing the
customer service level, which depends on the working performance of employees.
We then propose a memetic optimization algorithm combining global mutation and
neighborhood search assisted by deep reinforcement learning to efficiently
solve this problem. Experimental results on real-world problem instances of
bank call-center staff scheduling demonstrate the performance advantages of the
proposed method over selected popular staff scheduling methods. By explicitly
modeling and incorporating emotional stress, our method reflects a more
realistic understanding and utilization of human behavior in staff scheduling.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [279] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 本文提出了一种零样本迁移的ad hoc多智能体协作方法GPAT，能够在未见过的队友情况下实现有效协调，通过广义策略改进和差异奖励实现跨团队知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现实多智能体系统需要ad hoc组队能力，智能体必须与之前未见过的队友在零样本情况下协调完成任务。现有方法要么基于推断的队友模型选择预训练策略，要么预训练单一鲁棒策略，而本文旨在利用所有预训练策略实现零样本迁移。

Method: 将问题形式化为ad hoc多智能体马尔可夫决策过程，使用广义策略改进和差异奖励两个关键技术，实现不同团队间的有效知识迁移。GPAT算法能够组合多个预训练策略的优势。

Result: 在三个模拟环境（合作觅食、捕食者-猎物、Overcooked）中成功实现了新团队的零样本迁移，并在真实多机器人环境中验证了算法有效性。

Conclusion: GPAT算法能够有效解决ad hoc多智能体协作问题，通过利用预训练策略库实现零样本迁移，为现实多智能体系统提供了实用的解决方案。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [280] [Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences](https://arxiv.org/abs/2510.16221)
*Qinshuang Wei,Vaibhav Srivastava,Vijay Gupta*

Main category: cs.MA

TL;DR: 本文研究了多智能体环境下的顺序任务分配问题，其中智能体具有异构的任务偏好和能力。提出了一个bandit算法来解决未知随机环境中的任务分配问题，并分析了在精确和近似求解最优任务分配两种情况下的可达到遗憾。


<details>
  <summary>Details</summary>
Motivation: 虽然单智能体的顺序任务分配问题已被广泛研究，但在多智能体设置中，当智能体具有异构任务偏好或能力时，这类问题仍缺乏充分表征。需要开发能够处理异构能力和未知随机环境的算法。

Method: 提出了一种bandit算法，通过重复求解最优任务分配问题来处理多智能体任务分配。算法考虑智能体在任务完成时间、资源消耗（如能量或注意力）和奖励方面的异构能力，在未知随机环境中进行决策。

Result: 分析了算法在两种情况下可达到的遗憾：当能够精确求解最优任务分配时，以及当只能近似求解最优任务分配时。为多智能体异构任务分配问题提供了理论保证。

Conclusion: 该研究为多智能体异构任务分配问题提供了一个有效的bandit算法框架，能够在未知随机环境中实现最优任务分配，并提供了在不同求解精度下的遗憾分析。

Abstract: While sequential task assignment for a single agent has been widely studied,
such problems in a multi-agent setting, where the agents have heterogeneous
task preferences or capabilities, remain less well-characterized. We study a
multi-agent task assignment problem where a central planner assigns recurring
tasks to multiple members of a team over a finite time horizon. For any given
task, the members have heterogeneous capabilities in terms of task completion
times, task resource consumption (which can model variables such as energy or
attention), and preferences in terms of the rewards they collect upon task
completion. We assume that the reward, execution time, and resource consumption
for each member to complete any task are stochastic with unknown distributions.
The goal of the planner is to maximize the total expected reward that the team
receives over the problem horizon while ensuring that the resource consumption
required for any assigned task is within the capability of the agent. We
propose and analyze a bandit algorithm for this problem. Since the bandit
algorithm relies on solving an optimal task assignment problem repeatedly, we
analyze the achievable regret in two cases: when we can solve the optimal task
assignment exactly and when we can solve it only approximately.

</details>


### [281] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: MA-SAPO是一个多智能体框架，通过将评估结果与结构化推理相结合来指导系统化的提示优化，相比传统方法提供了更透明、可审计和可控的提示改进。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将评估视为黑盒，仅依赖数值分数而缺乏对提示成功或失败原因的解释，且严重依赖难以解释和控制的试错式改进。

Method: MA-SAPO采用两阶段多智能体框架：推理阶段智能体协作解释指标分数、诊断弱点并合成针对性改进；测试阶段智能体检索推理资产分析优化提示并应用基于证据的编辑。

Result: 在HelpSteer1/2基准测试上，MA-SAPO相比单次提示、检索增强基线和先前多智能体策略均取得一致改进。

Conclusion: 通过将评估信号转化为可解释的推理链，MA-SAPO能够产生更透明、可审计和可控的提示改进，验证了该方法的有效性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


### [282] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC是一个可扩展的分布式框架，用于大型机器人集群的任务分配和路径规划。它采用分区架构、动态选举领导和时间同步共识协议，确保强一致性和确定性结果。路径规划使用基于力的分散式实时碰撞解决算法。


<details>
  <summary>Details</summary>
Motivation: 解决大型机器人集群中高效任务分配和路径规划的挑战，特别是在工业物流等需要强一致性和确定性的场景。

Method: DiRAC采用分区架构，每个区域动态选举领导者，通过时间同步共识协议实现强一致性。路径规划使用基于力的分散式实时碰撞解决算法，在ROS 2中间件中实现。

Result: 在模拟仓库环境中的初步仿真验证了DiRAC的架构可扩展性和模块化效率，为大规模工业和物流领域的实际部署奠定了基础。

Conclusion: DiRAC框架为大型机器人集群提供了可扩展、高效的任务分配和路径规划解决方案，具有实际部署潜力。

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [283] [Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents](https://arxiv.org/abs/2510.16978)
*Dheeraj Chintapalli,Rikhil Tanugula,Sunkalp Chandra*

Main category: cs.MA

TL;DR: Lark是一个受生物学启发的决策框架，将LLM驱动的推理与进化的、利益相关者感知的多智能体系统相结合，通过四种机制解决冗长和利益相关者权衡问题，在30轮评估中表现优异且成本竞争力强。


<details>
  <summary>Details</summary>
Motivation: 解决传统决策系统中存在的冗长问题和利益相关者权衡挑战，通过生物学启发的机制实现更高效、透明的决策过程。

Method: Lark框架整合四种核心机制：可塑性（对候选方案进行简洁调整）、复制与成熟（复制高性能候选并专业化）、基于影响加权Borda得分的排名选择利益相关者聚合、以及基于token惩罚的计算感知机制。系统迭代生成多样化策略，应用调整，模拟评估，聚合偏好，并考虑计算成本。

Result: 在30轮控制评估中，Lark Full平均排名2.55，平均综合得分29.4/50，80%轮次进入前三名，每任务成本仅0.016美元。消融实验显示所有四种机制均显著贡献，其中复制/成熟机制影响最大（ΔScore=3.5）。

Conclusion: Lark是一个实用的、计算感知的神经进化循环，能够扩展利益相关者对齐的策略生成，并通过每步指标使权衡透明化，为现实世界验证研究提供了概念验证。

Abstract: We present Lark, a biologically inspired decision-making framework that
couples LLM-driven reasoning with an evolutionary, stakeholder-aware
Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we
integrate four mechanisms: (i) plasticity, which applies concise adjustments to
candidate solutions; (ii) duplication and maturation, which copy
high-performing candidates and specialize them into new modules; (iii)
ranked-choice stakeholder aggregation using influence-weighted Borda scoring;
and (iv) compute awareness via token-based penalties that reward brevity. The
system iteratively proposes diverse strategies, applies plasticity tweaks,
simulates stakeholder evaluations, aggregates preferences, selects top
candidates, and performs duplication/maturation while factoring compute cost
into final scores. In a controlled evaluation over 30 rounds comparing 14
systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a
mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%
of rounds while remaining cost competitive with leading commercial models
($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms
contribute significantly as ablating duplication/maturation yields the largest
deficit ({\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by
plasticity ({\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting
({\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\Delta}Score = 2.2,
d_z = 1.63). Rather than a formal Markov Decision Process with constrained
optimization, Lark is a practical, compute-aware neuroevolutionary loop that
scales stakeholder-aligned strategy generation and makes trade-offs transparent
through per-step metrics. Our work presents proof-of-concept findings and
invites community feedback as we expand toward real-world validation studies.

</details>


### [284] [ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI](https://arxiv.org/abs/2510.17004)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.MA

TL;DR: ReclAIm是一个基于大语言模型的多智能体框架，能够自主监控、评估和微调医学图像分类模型，实现AI模型在临床实践中的长期可靠性和自动化维护。


<details>
  <summary>Details</summary>
Motivation: 确保AI模型在临床实践中的长期可靠性，需要持续的性能监控和性能下降时的纠正措施，以促进AI模型在研究和临床环境中的更广泛采用。

Method: 基于大语言模型核心构建的多智能体框架，通过自然语言交互实现医学图像分类模型的自主监控、评估和微调，无需编程专业知识，采用最先进的微调程序。

Result: 在MRI、CT和X射线数据集上成功训练、评估并保持模型性能一致性。当检测到性能下降高达-41.1%时，ReclAIm能够将性能指标重新调整到初始模型结果的1.5%以内。

Conclusion: ReclAIm以用户友好和适应性强的方式实现医学影像AI模型的自动化持续维护，促进其在研究和临床环境中的更广泛采用。

Abstract: Ensuring the long-term reliability of AI models in clinical practice requires
continuous performance monitoring and corrective actions when degradation
occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent
framework capable of autonomously monitoring, evaluating, and fine-tuning
medical image classification models. The system, built on a large language
model core, operates entirely through natural language interaction, eliminating
the need for programming expertise. ReclAIm successfully trains, evaluates, and
maintains consistent performance of models across MRI, CT, and X-ray datasets.
Once ReclAIm detects significant performance degradation, it autonomously
executes state-of-the-art fine-tuning procedures that substantially reduce the
performance gap. In cases with performance drops of up to -41.1% (MRI
InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of
the initial model results. ReclAIm enables automated, continuous maintenance of
medical imaging AI models in a user-friendly and adaptable manner that
facilitates broader adoption in both research and clinical environments.

</details>


### [285] [MiCRO for Multilateral Negotiations](https://arxiv.org/abs/2510.17401)
*David Aguilera-Luzon,Dave de Jonge,Javier Larrosa*

Main category: cs.MA

TL;DR: 本文提出了MiCRO谈判策略的多边版本，在无需对手建模或机器学习的情况下，在多边谈判中超越了ANAC竞赛的获胜者，并形成了经验纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决MiCRO策略在多边谈判中的扩展问题，验证其在更复杂谈判环境中的有效性。

Method: 将双边MiCRO策略扩展到多边谈判场景，保持其无需参数调优和对手建模的简洁特性，通过经验博弈论分析验证策略性能。

Result: 多边MiCRO策略在ANAC 2015、2017和2018年获胜者的比较中表现更优，并形成了经验纳什均衡。

Conclusion: MiCRO策略的多边扩展证明了其在复杂谈判环境中的有效性，挑战了现有谈判测试领域的简单性假设。

Abstract: Recently, a very simple new bilateral negotiation strategy called MiCRO was
introduced that does not make use of any kind of opponent modeling or machine
learning techniques and that does not require fine-tuning of any parameters.
Despite its simplicity, it was shown that MiCRO performs similar to -- or even
better than -- most state-of-the-art negotiation strategies. This lead its
authors to argue that the benchmark domains on which negotiation algorithms are
typically tested may be too simplistic. However, one question that was left
open, was how MiCRO could be generalized to multilateral negotiations. In this
paper we fill this gap by introducing a multilateral variant of MiCRO. We
compare it with the winners of the Automated Negotiating Agents Competitions
(ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore,
we perform an empirical game-theoretical analysis to show that our new version
of MiCRO forms an empirical Nash equilibrium.

</details>
